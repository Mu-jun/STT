{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLCZvETJuFaO"
      },
      "source": [
        "## Create Wav2Vec2CTCTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-722dfaf1004ac490\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-722dfaf1004ac490\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d718d6cf3dae486f893074a0c85c3302",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7e27d6db4274dde933c50f58ee350cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-722dfaf1004ac490\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "all_data = load_dataset('csv',data_files='./order_speech_ko1000.csv',split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G34Hj6BgnK3L"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fe6c717cc1e4a20ba477069e6ca5edb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "remove_spectial_char_data = all_data.map(remove_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>script1_g_0044-6001-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>보고 있는 영상 정지시켜 줘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>script1_g_0044-6002-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>다음 주까지 날씨가 어때</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>script1_g_0044-6003-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>나 대신 점등해 줘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>script1_g_0044-6004-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>이번 주 대체로 흐린지 궁금해</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>script1_g_0044-6005-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>지금 당장 취침 등 꺼 줘</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        src               text\n",
              "0  script1_g_0044-6001-01-01-KSM-F-05-A.wav   보고 있는 영상 정지시켜 줘 \n",
              "1  script1_g_0044-6002-01-01-KSM-F-05-A.wav     다음 주까지 날씨가 어때 \n",
              "2  script1_g_0044-6003-01-01-KSM-F-05-A.wav        나 대신 점등해 줘 \n",
              "3  script1_g_0044-6004-01-01-KSM-F-05-A.wav  이번 주 대체로 흐린지 궁금해 \n",
              "4  script1_g_0044-6005-01-01-KSM-F-05-A.wav    지금 당장 취침 등 꺼 줘 "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(remove_spectial_char_data)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZsimiwGSnWDY"
      },
      "outputs": [],
      "source": [
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"text\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e11b9b3826d4bbb860fbb0fa64c54f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "char_vocab = remove_spectial_char_data.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=remove_spectial_char_data.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['vocab', 'all_text'],\n",
              "    num_rows: 1\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_list = list(set(char_vocab[\"vocab\"][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'미': 0,\n",
              " '량': 1,\n",
              " '소': 2,\n",
              " '핵': 3,\n",
              " '말': 4,\n",
              " '야': 5,\n",
              " '인': 6,\n",
              " '줬': 7,\n",
              " '풀': 8,\n",
              " '뒤': 9,\n",
              " '곱': 10,\n",
              " '너': 11,\n",
              " '은': 12,\n",
              " '랑': 13,\n",
              " '류': 14,\n",
              " '약': 15,\n",
              " '놨': 16,\n",
              " '겨': 17,\n",
              " '존': 18,\n",
              " '택': 19,\n",
              " '즘': 20,\n",
              " '베': 21,\n",
              " '른': 22,\n",
              " '빛': 23,\n",
              " '큼': 24,\n",
              " '력': 25,\n",
              " '엘': 26,\n",
              " '감': 27,\n",
              " '와': 28,\n",
              " '복': 29,\n",
              " '괜': 30,\n",
              " '면': 31,\n",
              " '봐': 32,\n",
              " '록': 33,\n",
              " '났': 34,\n",
              " '개': 35,\n",
              " '마': 36,\n",
              " '작': 37,\n",
              " '튼': 38,\n",
              " '추': 39,\n",
              " '역': 40,\n",
              " '기': 41,\n",
              " '환': 42,\n",
              " '목': 43,\n",
              " '못': 44,\n",
              " '료': 45,\n",
              " '혹': 46,\n",
              " '토': 47,\n",
              " '되': 48,\n",
              " '회': 49,\n",
              " '찾': 50,\n",
              " '낭': 51,\n",
              " '먼': 52,\n",
              " '듣': 53,\n",
              " '예': 54,\n",
              " '밤': 55,\n",
              " '염': 56,\n",
              " '늘': 57,\n",
              " '둡': 58,\n",
              " '농': 59,\n",
              " '큰': 60,\n",
              " '졸': 61,\n",
              " '셔': 62,\n",
              " '확': 63,\n",
              " '픈': 64,\n",
              " '닫': 65,\n",
              " '빙': 66,\n",
              " '니': 67,\n",
              " '단': 68,\n",
              " '나': 69,\n",
              " '알': 70,\n",
              " '란': 71,\n",
              " '연': 72,\n",
              " '취': 73,\n",
              " '어': 74,\n",
              " '맞': 75,\n",
              " '집': 76,\n",
              " '떤': 77,\n",
              " '랐': 78,\n",
              " '시': 79,\n",
              " '절': 80,\n",
              " '찮': 81,\n",
              " '느': 82,\n",
              " '며': 83,\n",
              " '케': 84,\n",
              " '열': 85,\n",
              " '망': 86,\n",
              " '곳': 87,\n",
              " '롤': 88,\n",
              " '얘': 89,\n",
              " '적': 90,\n",
              " '낮': 91,\n",
              " '먹': 92,\n",
              " '돼': 93,\n",
              " '꿔': 94,\n",
              " '빠': 95,\n",
              " '황': 96,\n",
              " '혀': 97,\n",
              " '의': 98,\n",
              " '풍': 99,\n",
              " '남': 100,\n",
              " '순': 101,\n",
              " '줘': 102,\n",
              " '굽': 103,\n",
              " '키': 104,\n",
              " '즐': 105,\n",
              " '최': 106,\n",
              " '볼': 107,\n",
              " '될': 108,\n",
              " '고': 109,\n",
              " '뤄': 110,\n",
              " '초': 111,\n",
              " '자': 112,\n",
              " '안': 113,\n",
              " '두': 114,\n",
              " '짧': 115,\n",
              " '잘': 116,\n",
              " '룸': 117,\n",
              " '않': 118,\n",
              " '떻': 119,\n",
              " '따': 120,\n",
              " '명': 121,\n",
              " '꿀': 122,\n",
              " '블': 123,\n",
              " '효': 124,\n",
              " '졌': 125,\n",
              " '했': 126,\n",
              " '표': 127,\n",
              " '엌': 128,\n",
              " '십': 129,\n",
              " '합': 130,\n",
              " '배': 131,\n",
              " '잔': 132,\n",
              " '중': 133,\n",
              " '타': 134,\n",
              " '메': 135,\n",
              " '왔': 136,\n",
              " '식': 137,\n",
              " '끼': 138,\n",
              " '걷': 139,\n",
              " '읊': 140,\n",
              " '없': 141,\n",
              " '된': 142,\n",
              " '드': 143,\n",
              " '요': 144,\n",
              " '재': 145,\n",
              " '센': 146,\n",
              " '꺼': 147,\n",
              " '맥': 148,\n",
              " '루': 149,\n",
              " '정': 150,\n",
              " '쉬': 151,\n",
              " '태': 152,\n",
              " '라': 153,\n",
              " '룹': 154,\n",
              " '불': 155,\n",
              " '투': 156,\n",
              " '모': 157,\n",
              " '뱅': 158,\n",
              " '밝': 159,\n",
              " '가': 160,\n",
              " '호': 161,\n",
              " '관': 162,\n",
              " '싶': 163,\n",
              " '칠': 164,\n",
              " '체': 165,\n",
              " '챙': 166,\n",
              " '휴': 167,\n",
              " '저': 168,\n",
              " '항': 169,\n",
              " '법': 170,\n",
              " '경': 171,\n",
              " '컨': 172,\n",
              " '깐': 173,\n",
              " '펼': 174,\n",
              " '락': 175,\n",
              " '층': 176,\n",
              " '지': 177,\n",
              " '생': 178,\n",
              " '능': 179,\n",
              " '략': 180,\n",
              " '도': 181,\n",
              " '람': 182,\n",
              " '증': 183,\n",
              " '턴': 184,\n",
              " '무': 185,\n",
              " '채': 186,\n",
              " '해': 187,\n",
              " '았': 188,\n",
              " '실': 189,\n",
              " '진': 190,\n",
              " '상': 191,\n",
              " '홉': 192,\n",
              " '머': 193,\n",
              " '레': 194,\n",
              " '행': 195,\n",
              " '활': 196,\n",
              " '치': 197,\n",
              " '픽': 198,\n",
              " '광': 199,\n",
              " '한': 200,\n",
              " '즌': 201,\n",
              " '끓': 202,\n",
              " '필': 203,\n",
              " '뭐': 204,\n",
              " '몇': 205,\n",
              " '칼': 206,\n",
              " '슨': 207,\n",
              " '창': 208,\n",
              " '랜': 209,\n",
              " '임': 210,\n",
              " '잡': 211,\n",
              " '렴': 212,\n",
              " '울': 213,\n",
              " '런': 214,\n",
              " '같': 215,\n",
              " '포': 216,\n",
              " '꾸': 217,\n",
              " '뭔': 218,\n",
              " '계': 219,\n",
              " '놔': 220,\n",
              " '탐': 221,\n",
              " '국': 222,\n",
              " '샷': 223,\n",
              " '준': 224,\n",
              " '톱': 225,\n",
              " '멍': 226,\n",
              " ' ': 227,\n",
              " '서': 228,\n",
              " '독': 229,\n",
              " '었': 230,\n",
              " '근': 231,\n",
              " '켤': 232,\n",
              " '길': 233,\n",
              " '올': 234,\n",
              " '닝': 235,\n",
              " '뽑': 236,\n",
              " '권': 237,\n",
              " '새': 238,\n",
              " '구': 239,\n",
              " '닥': 240,\n",
              " '럼': 241,\n",
              " '종': 242,\n",
              " '세': 243,\n",
              " '씨': 244,\n",
              " '입': 245,\n",
              " '네': 246,\n",
              " '운': 247,\n",
              " '액': 248,\n",
              " '신': 249,\n",
              " '월': 250,\n",
              " '떴': 251,\n",
              " '팝': 252,\n",
              " '분': 253,\n",
              " '뉴': 254,\n",
              " '천': 255,\n",
              " '갔': 256,\n",
              " '융': 257,\n",
              " '뀌': 258,\n",
              " '게': 259,\n",
              " '답': 260,\n",
              " '총': 261,\n",
              " '릴': 262,\n",
              " '얼': 263,\n",
              " '혼': 264,\n",
              " '있': 265,\n",
              " '아': 266,\n",
              " '여': 267,\n",
              " '싫': 268,\n",
              " '탑': 269,\n",
              " '려': 270,\n",
              " '봅': 271,\n",
              " '플': 272,\n",
              " '처': 273,\n",
              " '폭': 274,\n",
              " '높': 275,\n",
              " '사': 276,\n",
              " '줄': 277,\n",
              " '테': 278,\n",
              " '곡': 279,\n",
              " '히': 280,\n",
              " '갈': 281,\n",
              " '책': 282,\n",
              " '균': 283,\n",
              " '착': 284,\n",
              " '펴': 285,\n",
              " '유': 286,\n",
              " '달': 287,\n",
              " '켜': 288,\n",
              " '름': 289,\n",
              " '캐': 290,\n",
              " '넘': 291,\n",
              " '산': 292,\n",
              " '즉': 293,\n",
              " '터': 294,\n",
              " '쌀': 295,\n",
              " '짝': 296,\n",
              " '탁': 297,\n",
              " '텔': 298,\n",
              " '냄': 299,\n",
              " '뜻': 300,\n",
              " '흐': 301,\n",
              " '비': 302,\n",
              " '는': 303,\n",
              " '점': 304,\n",
              " '겠': 305,\n",
              " '만': 306,\n",
              " '바': 307,\n",
              " '제': 308,\n",
              " '방': 309,\n",
              " '씻': 310,\n",
              " '간': 311,\n",
              " '틀': 312,\n",
              " '로': 313,\n",
              " '율': 314,\n",
              " '슬': 315,\n",
              " '엇': 316,\n",
              " '대': 317,\n",
              " '덤': 318,\n",
              " '덜': 319,\n",
              " '속': 320,\n",
              " '쯤': 321,\n",
              " '뜨': 322,\n",
              " '많': 323,\n",
              " '맑': 324,\n",
              " '워': 325,\n",
              " '껴': 326,\n",
              " '돌': 327,\n",
              " '이': 328,\n",
              " '림': 329,\n",
              " '장': 330,\n",
              " '그': 331,\n",
              " '엠': 332,\n",
              " '설': 333,\n",
              " '귀': 334,\n",
              " '검': 335,\n",
              " '르': 336,\n",
              " '셋': 337,\n",
              " '규': 338,\n",
              " '커': 339,\n",
              " '좋': 340,\n",
              " '녹': 341,\n",
              " '횐': 342,\n",
              " '련': 343,\n",
              " '현': 344,\n",
              " '봤': 345,\n",
              " '공': 346,\n",
              " '슈': 347,\n",
              " '률': 348,\n",
              " '우': 349,\n",
              " '오': 350,\n",
              " '딱': 351,\n",
              " '몸': 352,\n",
              " '급': 353,\n",
              " '번': 354,\n",
              " '쉼': 355,\n",
              " '문': 356,\n",
              " '넷': 357,\n",
              " '크': 358,\n",
              " '끝': 359,\n",
              " '하': 360,\n",
              " '널': 361,\n",
              " '후': 362,\n",
              " '냐': 363,\n",
              " '놓': 364,\n",
              " '앞': 365,\n",
              " '를': 366,\n",
              " '륙': 367,\n",
              " '건': 368,\n",
              " '든': 369,\n",
              " '됐': 370,\n",
              " '본': 371,\n",
              " '핫': 372,\n",
              " '통': 373,\n",
              " '주': 374,\n",
              " '용': 375,\n",
              " '청': 376,\n",
              " '져': 377,\n",
              " '할': 378,\n",
              " '쪽': 379,\n",
              " '늦': 380,\n",
              " '색': 381,\n",
              " '손': 382,\n",
              " '츠': 383,\n",
              " '습': 384,\n",
              " '팀': 385,\n",
              " '햇': 386,\n",
              " '춰': 387,\n",
              " '텐': 388,\n",
              " '래': 389,\n",
              " '음': 390,\n",
              " '수': 391,\n",
              " '민': 392,\n",
              " '측': 393,\n",
              " '티': 394,\n",
              " '성': 395,\n",
              " '엔': 396,\n",
              " '퍼': 397,\n",
              " '깨': 398,\n",
              " '곧': 399,\n",
              " '심': 400,\n",
              " '난': 401,\n",
              " '외': 402,\n",
              " '딘': 403,\n",
              " '괄': 404,\n",
              " '탕': 405,\n",
              " '핑': 406,\n",
              " '차': 407,\n",
              " '넣': 408,\n",
              " '흥': 409,\n",
              " '힐': 410,\n",
              " '거': 411,\n",
              " '벽': 412,\n",
              " '결': 413,\n",
              " '조': 414,\n",
              " '러': 415,\n",
              " '교': 416,\n",
              " '던': 417,\n",
              " '막': 418,\n",
              " '춤': 419,\n",
              " '쓸': 420,\n",
              " '전': 421,\n",
              " '콘': 422,\n",
              " '발': 423,\n",
              " '잠': 424,\n",
              " '였': 425,\n",
              " '프': 426,\n",
              " '승': 427,\n",
              " '날': 428,\n",
              " '파': 429,\n",
              " '짜': 430,\n",
              " '특': 431,\n",
              " '금': 432,\n",
              " '피': 433,\n",
              " '언': 434,\n",
              " '쇼': 435,\n",
              " '출': 436,\n",
              " '위': 437,\n",
              " '린': 438,\n",
              " '떨': 439,\n",
              " '브': 440,\n",
              " '궁': 441,\n",
              " '흘': 442,\n",
              " '침': 443,\n",
              " '깥': 444,\n",
              " '노': 445,\n",
              " '팔': 446,\n",
              " '즈': 447,\n",
              " '북': 448,\n",
              " '학': 449,\n",
              " '반': 450,\n",
              " '악': 451,\n",
              " '각': 452,\n",
              " '보': 453,\n",
              " '다': 454,\n",
              " '멈': 455,\n",
              " '둬': 456,\n",
              " '부': 457,\n",
              " '까': 458,\n",
              " '물': 459,\n",
              " '트': 460,\n",
              " '편': 461,\n",
              " '디': 462,\n",
              " '또': 463,\n",
              " '더': 464,\n",
              " '영': 465,\n",
              " '축': 466,\n",
              " '내': 467,\n",
              " '쳐': 468,\n",
              " '변': 469,\n",
              " '매': 470,\n",
              " '형': 471,\n",
              " '으': 472,\n",
              " '빨': 473,\n",
              " '일': 474,\n",
              " '삼': 475,\n",
              " '평': 476,\n",
              " '스': 477,\n",
              " '녁': 478,\n",
              " '카': 479,\n",
              " '코': 480,\n",
              " '직': 481,\n",
              " '좀': 482,\n",
              " '화': 483,\n",
              " '눈': 484,\n",
              " '퇴': 485,\n",
              " '뀐': 486,\n",
              " '낌': 487,\n",
              " '당': 488,\n",
              " '떼': 489,\n",
              " '품': 490,\n",
              " '등': 491,\n",
              " '석': 492,\n",
              " '걸': 493,\n",
              " '들': 494,\n",
              " '뜰': 495,\n",
              " '리': 496,\n",
              " '꼭': 497,\n",
              " '맙': 498,\n",
              " '밖': 499,\n",
              " '받': 500,\n",
              " '애': 501,\n",
              " '깜': 502,\n",
              " '읽': 503,\n",
              " '온': 504,\n",
              " '때': 505,\n",
              " '것': 506,\n",
              " '살': 507,\n",
              " '둠': 508,\n",
              " '별': 509,\n",
              " '팅': 510,\n",
              " '원': 511,\n",
              " '동': 512,\n",
              " '데': 513,\n",
              " '맛': 514,\n",
              " '을': 515,\n",
              " '삭': 516,\n",
              " '떠': 517,\n",
              " '선': 518,\n",
              " '틴': 519,\n",
              " '송': 520,\n",
              " '웠': 521,\n",
              " '업': 522,\n",
              " '강': 523,\n",
              " '에': 524,\n",
              " '헤': 525,\n",
              " '빵': 526,\n",
              " '쓰': 527,\n",
              " '과': 528}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "vocab_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_hi_HbR_rCKb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "531"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "del vocab_dict[\" \"]\n",
        "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "len(vocab_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rMWugphlrsZ1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YJUUl9lnryU-"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\",\n",
        "                                 unk_token=\"[UNK]\",\n",
        "                                 pad_token=\"[PAD]\",\n",
        "                                 word_delimiter_token=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCtrdRsuuDix"
      },
      "source": [
        "## Create XLSR-Wav2Vec2 Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O0vdrkhKr8D1"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1,\n",
        "                                             sampling_rate=16000,\n",
        "                                             padding_value=0.0,\n",
        "                                             do_normalize=True,\n",
        "                                             return_attention_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fY_Qm1dpsE78"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor,\n",
        "                              tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KTUahP1PsL3g"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9tMSA3xKsVma"
      },
      "outputs": [],
      "source": [
        "# processor.save_pretrained(\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## add audio array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_audio(batch):\n",
        "    batch['array'],_ = librosa.load('./dataset/audio/'+batch['src'],sr=16000)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dd4b4e5b7dc47f9a43cca1e4993ec8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "audio_data = remove_spectial_char_data.map(load_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZiv4zqyt2cY"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "AscOxzgUsb50"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "\n",
        "    # batched output is \"un-batched\"\n",
        "    batch[\"input_values\"] = processor(batch[\"array\"], sampling_rate=16000).input_values[0]\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['src', 'text', 'array'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "audio_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fe6fe77ce034c8ba40cdc12a8103535",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "order_voice = audio_data.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=remove_spectial_char_data.column_names,\n",
        "    # num_proc=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['array', 'input_values', 'labels'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "order_voice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StCtCQxDtuY3"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUTqiRY-tpXZ"
      },
      "source": [
        "## Set-up Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nTlewiOzsyqR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.cuda.HalfTensor]]]) -> Dict[str, torch.cuda.HalfTensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WmfIPs18tGQb"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "vJjfNy9-tJM6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c5fce1568194122a6403fa84dbccb08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/1.90k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset, load_metric, Audio\n",
        "\n",
        "wer_metric = load_metric(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "03mNNUfNthSZ"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vgRoeQHvTqk"
      },
      "source": [
        "## Import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Iyv4qaclue6n"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_q.bias', 'project_q.weight', 'project_hid.weight', 'quantizer.weight_proj.weight', 'project_hid.bias', 'quantizer.weight_proj.bias', 'quantizer.codevectors']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\", \n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaYmsSutuv26"
      },
      "source": [
        "XLSR-Wav2Vec2의 첫 번째 구성 요소는 원시 음성 신호에서 음향적으로 의미가 있지만 문맥적으로 독립적인 기능을 추출하는 데 사용되는 CNN 계층 스택으로 구성됩니다.  \n",
        "모델의 이 부분은 사전 교육 중에 이미 충분히 훈련되었으며 논문에 명시된 바와 같이 더 이상 미세 조정할 필요가 없습니다. 따라서 특징 추출 부분의 모든 파라미터에 대해 require_grad를 False로 설정할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5VX8z6PpuhqG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1677: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "model.freeze_feature_extractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAfoHUk9vGxd"
      },
      "source": [
        "메모리를 절약하기 위해 그라데이션 체크포인팅을 활성화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "wZm6in_2vAIN"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-aJNQplvQgn"
      },
      "source": [
        "## TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-LuuwoeMvOnV"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-turkish-demo\",\n",
        "  output_dir=\"./wav2vec2-large-xlsr-ko-demo\",\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=16,\n",
        "  gradient_accumulation_steps=2,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=3,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=3e-4,\n",
        "  warmup_steps=500,\n",
        "  save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6vhFC0BdvcOu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=order_voice, # train_ds\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGNj1NSvsFk"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "a-rJ8fokvjJb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
            "c:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 93\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a15d0e282df04584a8375725b51833b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/93 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 52.6768, 'learning_rate': 4.8e-06, 'epoch': 0.32}\n",
            "{'loss': 50.3339, 'learning_rate': 1.0799999999999998e-05, 'epoch': 0.63}\n",
            "{'loss': 51.2774, 'learning_rate': 1.68e-05, 'epoch': 0.95}\n",
            "{'loss': 53.6257, 'learning_rate': 2.1599999999999996e-05, 'epoch': 1.29}\n",
            "{'loss': 48.8287, 'learning_rate': 2.7599999999999997e-05, 'epoch': 1.6}\n",
            "{'loss': 45.2693, 'learning_rate': 3.36e-05, 'epoch': 1.92}\n",
            "{'loss': 40.0247, 'learning_rate': 3.9e-05, 'epoch': 2.25}\n",
            "{'loss': 31.6594, 'learning_rate': 4.4999999999999996e-05, 'epoch': 2.57}\n",
            "{'loss': 25.5611, 'learning_rate': 5.1e-05, 'epoch': 2.89}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 277.6588, 'train_samples_per_second': 10.805, 'train_steps_per_second': 0.335, 'train_loss': 43.6303466468729, 'epoch': 2.98}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=93, training_loss=43.6303466468729, metrics={'train_runtime': 277.6588, 'train_samples_per_second': 10.805, 'train_steps_per_second': 0.335, 'train_loss': 43.6303466468729, 'epoch': 2.98})"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxpIR_X8w3Pq"
      },
      "source": [
        "CTC 손실을 사용하여 더 큰 데이터 세트에서 더 큰 모델을 미세 조정하려면 [여기서](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) 공식 음성 인식 예를 살펴봐야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shZn6QzW0T1h"
      },
      "source": [
        "## Model predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Indexing with integers is not available when using Python based feature extractors'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20420\\3291417311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morder_voice\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   1719\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1720\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1721\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1722\u001b[0m         )\n\u001b[0;32m   1723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1345\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1347\u001b[1;33m         \u001b[0mextract_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1348\u001b[0m         \u001b[0mextract_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_values)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# make sure hidden_states require grad for gradient_checkpointing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\feature_extraction_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Indexing with integers is not available when using Python based feature extractors\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Indexing with integers is not available when using Python based feature extractors'"
          ]
        }
      ],
      "source": [
        "\n",
        "model(data_collator([order_voice[0]]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "00.XLSR - Wav2Vec2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('STT')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "99a2429cd063090ad39acbbb390da3f5879d741ee9cb02f1744db7376a74b1ea"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
