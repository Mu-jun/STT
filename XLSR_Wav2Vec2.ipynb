{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "sr = 16000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-3a7ac772dc9fef4d\n",
            "Reusing dataset csv (C:\\Users\\Mu-jun\\.cache\\huggingface\\datasets\\csv\\default-3a7ac772dc9fef4d\\0.0.0\\bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30.802470922470093\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "all_data = load_dataset('csv',data_files='./order_speech_ko1000.csv',split='train')\n",
        "print(time.time()-start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G34Hj6BgnK3L"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parameter 'function'=<function remove_special_characters at 0x000001A179641EE8> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a02eee7c852844bd950e139ef02aaefc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "remove_spectial_char_data = all_data.map(remove_special_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLCZvETJuFaO"
      },
      "source": [
        "## Create Wav2Vec2CTCTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YJUUl9lnryU-"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\",\n",
        "                                 unk_token=\"[UNK]\",\n",
        "                                 pad_token=\"[PAD]\",\n",
        "                                 word_delimiter_token=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCtrdRsuuDix"
      },
      "source": [
        "## Create XLSR-Wav2Vec2 Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O0vdrkhKr8D1"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1,\n",
        "                                             sampling_rate=16000,\n",
        "                                             padding_value=0.0,\n",
        "                                             do_normalize=True,\n",
        "                                             return_attention_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fY_Qm1dpsE78"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor,\n",
        "                              tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KTUahP1PsL3g"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9tMSA3xKsVma"
      },
      "outputs": [],
      "source": [
        "# processor.save_pretrained(\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## add audio array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def voice_sep(sig):\n",
        "    sig = np.array(sig).flatten()\n",
        "    S_full, phase = librosa.magphase(librosa.stft(sig))\n",
        "    S_filter = librosa.decompose.nn_filter(S_full,\n",
        "                                       aggregate=np.median,\n",
        "                                       metric='cosine',\n",
        "                                       width=int(librosa.time_to_frames(2, sr=sr)))\n",
        "    S_filter = np.minimum(S_full, S_filter)\n",
        "    margin_v = 2\n",
        "    power = 2\n",
        "    mask_v = librosa.util.softmask(S_full - S_filter,\n",
        "                               margin_v * S_filter,\n",
        "                               power=power)\n",
        "    S_foreground = mask_v * S_full\n",
        "    y_foreground = librosa.istft(S_foreground * phase)\n",
        "    return y_foreground\n",
        "\n",
        "def load_audio(batch):\n",
        "    batch['array'],_ = librosa.load('./dataset/audio/'+batch['filename'],sr=16000)\n",
        "    batch['array'] = np.array(batch['array'])\n",
        "    \n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3583b2b0986b4ababd62b5be0b94109b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "audio_data = remove_spectial_char_data.map(load_audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(audio_data[0]['array'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(audio_data[0]['array'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data augmentation\n",
        "- RIR applied\n",
        "- stretching(fast,slow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4160])\n"
          ]
        }
      ],
      "source": [
        "rir_raw,_ = librosa.load('./room_component.wav',sr)\n",
        "rir = torch.from_numpy(rir_raw.reshape(1,-1))\n",
        "print(rir.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rir_applied(batch):\n",
        "\n",
        "    speech = torch.from_numpy(np.array(batch['array'],dtype=np.float32).reshape(1,-1))\n",
        "\n",
        "    speech_ = torch.nn.functional.pad(speech, (rir.shape[1] - 1, 0))\n",
        "    print(speech.dtype)\n",
        "    print(speech_.dtype)\n",
        "    print(rir.dtype)\n",
        "    augmented = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n",
        "    batch['array'] = augmented.reshape(-1)\n",
        "    return batch\n",
        "\n",
        "def fast_stretching(batch):\n",
        "    array = np.array(batch['array'],dtype=np.float32)\n",
        "    batch['array'] = librosa.effects.time_stretch(array,0.8)\n",
        "    return batch\n",
        "\n",
        "def slow_stretching(batch):\n",
        "    array = np.array(batch['array'],dtype=np.float32)\n",
        "    batch['array'] = librosa.effects.time_stretch(array,1.2)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rir_applied_audio_data = audio_data.map(rir_applied)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-f49502427ad3afb2\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-bd67c8f5c959c293.arrow\n"
          ]
        }
      ],
      "source": [
        "fast_stretching_data = audio_data.map(fast_stretching)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-f49502427ad3afb2\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-4226a44a075f52dd.arrow\n"
          ]
        }
      ],
      "source": [
        "slow_stretching_data = audio_data.map(slow_stretching)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZiv4zqyt2cY"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "AscOxzgUsb50"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "\n",
        "    # batched output is \"un-batched\"\n",
        "    batch[\"input_values\"] = processor(batch[\"array\"], sampling_rate=16000).input_values[0]\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]Loading cached processed dataset at C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-f49502427ad3afb2\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-2d4b16bd19083f38.arrow\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dead89d95fb482bb07891e1a211ca4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 2/4 [00:18<00:18,  9.15s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a39de93e73824796830a685285c31be9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 3/4 [00:41<00:14, 14.99s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79ef1f7748444164b2041e3243eb301d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:56<00:00, 14.23s/it]\n"
          ]
        }
      ],
      "source": [
        "ds_list = [audio_data, rir_applied_audio_data, fast_stretching_data, slow_stretching_data]\n",
        "prepare_ds_list = []\n",
        "for ds in ds_list:\n",
        "    prepare_ds_list.append(ds.map(\n",
        "        prepare_dataset,\n",
        "        remove_columns=ds.column_names,\n",
        "        # num_proc=2\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_data = torch.utils.data.ConcatDataset(prepare_ds_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StCtCQxDtuY3"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUTqiRY-tpXZ"
      },
      "source": [
        "## Set-up Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nTlewiOzsyqR"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.cuda.HalfTensor]]]) -> Dict[str, torch.cuda.HalfTensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WmfIPs18tGQb"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vJjfNy9-tJM6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcf95383f70841b3a01d6af094892620",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.95k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset, load_metric, Audio\n",
        "\n",
        "wer_metric = load_metric(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "03mNNUfNthSZ"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vgRoeQHvTqk"
      },
      "source": [
        "## Import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyv4qaclue6n"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    'facebook/wav2vec2-large-xlsr-53',\n",
        "    # \"./wav2vec2-large-xlsr-ko-demo/checkpoint-1900\", \n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaYmsSutuv26"
      },
      "source": [
        "XLSR-Wav2Vec2의 첫 번째 구성 요소는 원시 음성 신호에서 음향적으로 의미가 있지만 문맥적으로 독립적인 기능을 추출하는 데 사용되는 CNN 계층 스택으로 구성됩니다.  \n",
        "모델의 이 부분은 사전 교육 중에 이미 충분히 훈련되었으며 논문에 명시된 바와 같이 더 이상 미세 조정할 필요가 없습니다. 따라서 특징 추출 부분의 모든 파라미터에 대해 require_grad를 False로 설정할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "5VX8z6PpuhqG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1677: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "model.freeze_feature_extractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAfoHUk9vGxd"
      },
      "source": [
        "메모리를 절약하기 위해 그라데이션 체크포인팅을 활성화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "wZm6in_2vAIN"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-aJNQplvQgn"
      },
      "source": [
        "## TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-LuuwoeMvOnV"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "tb = transformers.integrations.TensorBoardCallback()\n",
        "class MyCallback(TrainerCallback):\n",
        "  def on_evaluate(self, arg, state, control, **kwargs):\n",
        "    if state.global_step == int(state.best_model_checkpint):\n",
        "          model.save(os.path.join(arg.output_dir,'best_model'))\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ko-demo\",\n",
        "  output_dir=\"./wav2vec2-large-xlsr-ko-demo\",\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=16,\n",
        "  gradient_accumulation_steps=2,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=88,\n",
        "  fp16=True,\n",
        "  # save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=3e-4,\n",
        "  warmup_steps=500,\n",
        "  save_total_limit=2,\n",
        "  callbacks=[tb,]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(augmented_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "6vhFC0BdvcOu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "ds_size = len(augmented_data)\n",
        "train_size = int(ds_size*0.8)\n",
        "val_size = ds_size - train_size\n",
        "train_ds, val_ds = random_split(augmented_data,[train_size,val_size])\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGNj1NSvsFk"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "a-rJ8fokvjJb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 3200\n",
            "  Num Epochs = 88\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 8800\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b81a92a61024c40bfc05bffc6eeffcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8800 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4571, 'learning_rate': 5.999999999999999e-06, 'epoch': 0.1}\n",
            "{'loss': 0.6694, 'learning_rate': 1.1999999999999999e-05, 'epoch': 0.2}\n",
            "{'loss': 0.4597, 'learning_rate': 1.7999999999999997e-05, 'epoch': 0.3}\n",
            "{'loss': 0.4644, 'learning_rate': 2.3999999999999997e-05, 'epoch': 0.4}\n",
            "{'loss': 0.4915, 'learning_rate': 2.9999999999999997e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4299, 'learning_rate': 3.5999999999999994e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4021, 'learning_rate': 4.2e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4114, 'learning_rate': 4.7999999999999994e-05, 'epoch': 0.8}\n",
            "{'loss': 0.3713, 'learning_rate': 5.399999999999999e-05, 'epoch': 0.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4654, 'learning_rate': 5.9999999999999995e-05, 'epoch': 1.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc5ab35896134e7db5f5f161b5a714a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.19907264411449432, 'eval_wer': 0.4514174945480979, 'eval_runtime': 26.2118, 'eval_samples_per_second': 30.521, 'eval_steps_per_second': 3.815, 'epoch': 1.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3345, 'learning_rate': 6.599999999999999e-05, 'epoch': 1.1}\n",
            "{'loss': 0.4252, 'learning_rate': 7.199999999999999e-05, 'epoch': 1.2}\n",
            "{'loss': 0.3824, 'learning_rate': 7.8e-05, 'epoch': 1.3}\n",
            "{'loss': 0.3448, 'learning_rate': 8.4e-05, 'epoch': 1.4}\n",
            "{'loss': 0.3284, 'learning_rate': 8.999999999999999e-05, 'epoch': 1.5}\n",
            "{'loss': 0.2864, 'learning_rate': 9.599999999999999e-05, 'epoch': 1.6}\n",
            "{'loss': 0.2833, 'learning_rate': 0.000102, 'epoch': 1.7}\n",
            "{'loss': 0.3454, 'learning_rate': 0.00010799999999999998, 'epoch': 1.8}\n",
            "{'loss': 0.2857, 'learning_rate': 0.00011399999999999999, 'epoch': 1.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2802, 'learning_rate': 0.00011999999999999999, 'epoch': 2.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "818ef48dd4344897b53dd3ffb2119076",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.13029611110687256, 'eval_wer': 0.4252483644293676, 'eval_runtime': 25.5688, 'eval_samples_per_second': 31.288, 'eval_steps_per_second': 3.911, 'epoch': 2.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2473, 'learning_rate': 0.00012599999999999997, 'epoch': 2.1}\n",
            "{'loss': 0.3059, 'learning_rate': 0.00013199999999999998, 'epoch': 2.2}\n",
            "{'loss': 0.264, 'learning_rate': 0.000138, 'epoch': 2.3}\n",
            "{'loss': 0.2276, 'learning_rate': 0.00014399999999999998, 'epoch': 2.4}\n",
            "{'loss': 0.2838, 'learning_rate': 0.00015, 'epoch': 2.5}\n",
            "{'loss': 0.2407, 'learning_rate': 0.000156, 'epoch': 2.6}\n",
            "{'loss': 0.2393, 'learning_rate': 0.000162, 'epoch': 2.7}\n",
            "{'loss': 0.25, 'learning_rate': 0.000168, 'epoch': 2.8}\n",
            "{'loss': 0.2186, 'learning_rate': 0.00017399999999999997, 'epoch': 2.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2479, 'learning_rate': 0.00017999999999999998, 'epoch': 3.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee5a177c22b9413c8ff9c02569890161",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0840657651424408, 'eval_wer': 0.43348679428156045, 'eval_runtime': 25.4562, 'eval_samples_per_second': 31.427, 'eval_steps_per_second': 3.928, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2198, 'learning_rate': 0.000186, 'epoch': 3.1}\n",
            "{'loss': 0.1805, 'learning_rate': 0.00019199999999999998, 'epoch': 3.2}\n",
            "{'loss': 0.2207, 'learning_rate': 0.000198, 'epoch': 3.3}\n",
            "{'loss': 0.1952, 'learning_rate': 0.000204, 'epoch': 3.4}\n",
            "{'loss': 0.2352, 'learning_rate': 0.00020999999999999998, 'epoch': 3.5}\n",
            "{'loss': 0.2157, 'learning_rate': 0.00021599999999999996, 'epoch': 3.6}\n",
            "{'loss': 0.2466, 'learning_rate': 0.00022199999999999998, 'epoch': 3.7}\n",
            "{'loss': 0.2702, 'learning_rate': 0.00022799999999999999, 'epoch': 3.8}\n",
            "{'loss': 0.2148, 'learning_rate': 0.000234, 'epoch': 3.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2755, 'learning_rate': 0.00023999999999999998, 'epoch': 4.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07c426cff5944a4c867c8128dd2453c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.07987350970506668, 'eval_wer': 0.42452144414829174, 'eval_runtime': 25.3946, 'eval_samples_per_second': 31.503, 'eval_steps_per_second': 3.938, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2107, 'learning_rate': 0.00024599999999999996, 'epoch': 4.1}\n",
            "{'loss': 0.2034, 'learning_rate': 0.0002514, 'epoch': 4.2}\n",
            "{'loss': 0.2399, 'learning_rate': 0.00025739999999999997, 'epoch': 4.3}\n",
            "{'loss': 0.2034, 'learning_rate': 0.00026339999999999995, 'epoch': 4.4}\n",
            "{'loss': 0.2674, 'learning_rate': 0.0002694, 'epoch': 4.5}\n",
            "{'loss': 0.2091, 'learning_rate': 0.00027539999999999997, 'epoch': 4.6}\n",
            "{'loss': 0.2186, 'learning_rate': 0.00028139999999999996, 'epoch': 4.7}\n",
            "{'loss': 0.2467, 'learning_rate': 0.00028739999999999994, 'epoch': 4.8}\n",
            "{'loss': 0.1947, 'learning_rate': 0.0002934, 'epoch': 4.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.225, 'learning_rate': 0.00029939999999999996, 'epoch': 5.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e0d8edf66ae45b4b1adc0417ef2f66d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.09395793825387955, 'eval_wer': 0.4761327841046765, 'eval_runtime': 25.8193, 'eval_samples_per_second': 30.985, 'eval_steps_per_second': 3.873, 'epoch': 5.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2163, 'learning_rate': 0.0002996746987951807, 'epoch': 5.1}\n",
            "{'loss': 0.2336, 'learning_rate': 0.00029931325301204816, 'epoch': 5.2}\n",
            "{'loss': 0.2316, 'learning_rate': 0.00029895180722891565, 'epoch': 5.3}\n",
            "{'loss': 0.1977, 'learning_rate': 0.0002985903614457831, 'epoch': 5.4}\n",
            "{'loss': 0.2602, 'learning_rate': 0.00029822891566265057, 'epoch': 5.5}\n",
            "{'loss': 0.2551, 'learning_rate': 0.00029786746987951805, 'epoch': 5.6}\n",
            "{'loss': 0.2606, 'learning_rate': 0.0002975060240963855, 'epoch': 5.7}\n",
            "{'loss': 0.2525, 'learning_rate': 0.000297144578313253, 'epoch': 5.8}\n",
            "{'loss': 0.2032, 'learning_rate': 0.00029678313253012046, 'epoch': 5.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2223, 'learning_rate': 0.00029642168674698794, 'epoch': 6.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e94cfb3622fd4dcaadee42d2708f862d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.07939021289348602, 'eval_wer': 0.4482675066634359, 'eval_runtime': 25.9959, 'eval_samples_per_second': 30.774, 'eval_steps_per_second': 3.847, 'epoch': 6.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1967, 'learning_rate': 0.00029606024096385543, 'epoch': 6.1}\n",
            "{'loss': 0.1851, 'learning_rate': 0.00029569879518072286, 'epoch': 6.2}\n",
            "{'loss': 0.1697, 'learning_rate': 0.00029533734939759035, 'epoch': 6.3}\n",
            "{'loss': 0.1923, 'learning_rate': 0.00029497590361445784, 'epoch': 6.4}\n",
            "{'loss': 0.2221, 'learning_rate': 0.00029461445783132527, 'epoch': 6.5}\n",
            "{'loss': 0.1714, 'learning_rate': 0.00029425301204819276, 'epoch': 6.6}\n",
            "{'loss': 0.1804, 'learning_rate': 0.00029389156626506024, 'epoch': 6.7}\n",
            "{'loss': 0.2004, 'learning_rate': 0.0002935301204819277, 'epoch': 6.8}\n",
            "{'loss': 0.1581, 'learning_rate': 0.00029316867469879516, 'epoch': 6.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2118, 'learning_rate': 0.00029280722891566265, 'epoch': 7.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a933b501cd3447e2a1f21b423b680940",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.060216568410396576, 'eval_wer': 0.3976253937484856, 'eval_runtime': 25.9043, 'eval_samples_per_second': 30.883, 'eval_steps_per_second': 3.86, 'epoch': 7.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1868, 'learning_rate': 0.0002924457831325301, 'epoch': 7.1}\n",
            "{'loss': 0.1813, 'learning_rate': 0.00029208433734939757, 'epoch': 7.2}\n",
            "{'loss': 0.1429, 'learning_rate': 0.00029172289156626505, 'epoch': 7.3}\n",
            "{'loss': 0.1463, 'learning_rate': 0.0002913614457831325, 'epoch': 7.4}\n",
            "{'loss': 0.175, 'learning_rate': 0.00029099999999999997, 'epoch': 7.5}\n",
            "{'loss': 0.1803, 'learning_rate': 0.00029063855421686746, 'epoch': 7.6}\n",
            "{'loss': 0.1834, 'learning_rate': 0.0002902771084337349, 'epoch': 7.7}\n",
            "{'loss': 0.1673, 'learning_rate': 0.0002899156626506024, 'epoch': 7.8}\n",
            "{'loss': 0.159, 'learning_rate': 0.00028955421686746986, 'epoch': 7.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1726, 'learning_rate': 0.00028919277108433735, 'epoch': 8.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a09f1fe285b433093e86348ff6a4d7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.04269620403647423, 'eval_wer': 0.3683062757450933, 'eval_runtime': 26.0007, 'eval_samples_per_second': 30.768, 'eval_steps_per_second': 3.846, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1421, 'learning_rate': 0.0002888313253012048, 'epoch': 8.1}\n",
            "{'loss': 0.1748, 'learning_rate': 0.00028846987951807227, 'epoch': 8.2}\n",
            "{'loss': 0.1911, 'learning_rate': 0.00028810843373493976, 'epoch': 8.3}\n",
            "{'loss': 0.1192, 'learning_rate': 0.0002877469879518072, 'epoch': 8.4}\n",
            "{'loss': 0.1567, 'learning_rate': 0.0002873855421686747, 'epoch': 8.5}\n",
            "{'loss': 0.1606, 'learning_rate': 0.00028702409638554216, 'epoch': 8.6}\n",
            "{'loss': 0.1466, 'learning_rate': 0.0002866626506024096, 'epoch': 8.7}\n",
            "{'loss': 0.1182, 'learning_rate': 0.0002863012048192771, 'epoch': 8.8}\n",
            "{'loss': 0.17, 'learning_rate': 0.00028593975903614457, 'epoch': 8.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1127, 'learning_rate': 0.000285578313253012, 'epoch': 9.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01aa9e4336fc46ef9d5a6538f464b14d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.08529697358608246, 'eval_wer': 0.48509813423794523, 'eval_runtime': 25.6631, 'eval_samples_per_second': 31.173, 'eval_steps_per_second': 3.897, 'epoch': 9.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.153, 'learning_rate': 0.0002852168674698795, 'epoch': 9.1}\n",
            "{'loss': 0.1341, 'learning_rate': 0.00028485542168674697, 'epoch': 9.2}\n",
            "{'loss': 0.1442, 'learning_rate': 0.0002844939759036144, 'epoch': 9.3}\n",
            "{'loss': 0.1618, 'learning_rate': 0.0002841325301204819, 'epoch': 9.4}\n",
            "{'loss': 0.1652, 'learning_rate': 0.0002837710843373494, 'epoch': 9.5}\n",
            "{'loss': 0.1186, 'learning_rate': 0.0002834096385542168, 'epoch': 9.6}\n",
            "{'loss': 0.119, 'learning_rate': 0.00028304819277108435, 'epoch': 9.7}\n",
            "{'loss': 0.143, 'learning_rate': 0.0002826867469879518, 'epoch': 9.8}\n",
            "{'loss': 0.1087, 'learning_rate': 0.00028232530120481927, 'epoch': 9.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1039, 'learning_rate': 0.00028196385542168676, 'epoch': 10.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "782dde9ee3824c24b1b943d316cf32e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.02579297311604023, 'eval_wer': 0.41991761570147806, 'eval_runtime': 25.6388, 'eval_samples_per_second': 31.203, 'eval_steps_per_second': 3.9, 'epoch': 10.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.123, 'learning_rate': 0.0002816024096385542, 'epoch': 10.1}\n",
            "{'loss': 0.1094, 'learning_rate': 0.0002812409638554217, 'epoch': 10.2}\n",
            "{'loss': 0.0981, 'learning_rate': 0.00028087951807228916, 'epoch': 10.3}\n",
            "{'loss': 0.1229, 'learning_rate': 0.0002805180722891566, 'epoch': 10.4}\n",
            "{'loss': 0.1116, 'learning_rate': 0.0002801566265060241, 'epoch': 10.5}\n",
            "{'loss': 0.1092, 'learning_rate': 0.00027979518072289157, 'epoch': 10.6}\n",
            "{'loss': 0.11, 'learning_rate': 0.000279433734939759, 'epoch': 10.7}\n",
            "{'loss': 0.151, 'learning_rate': 0.0002790722891566265, 'epoch': 10.8}\n",
            "{'loss': 0.1368, 'learning_rate': 0.000278710843373494, 'epoch': 10.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1479, 'learning_rate': 0.0002783493975903614, 'epoch': 11.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38e73e47233a4fbc92e430ff7a4879b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.031650666147470474, 'eval_wer': 0.4322752604797674, 'eval_runtime': 25.6235, 'eval_samples_per_second': 31.221, 'eval_steps_per_second': 3.903, 'epoch': 11.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1013, 'learning_rate': 0.0002779879518072289, 'epoch': 11.1}\n",
            "{'loss': 0.1268, 'learning_rate': 0.0002776265060240964, 'epoch': 11.2}\n",
            "{'loss': 0.1061, 'learning_rate': 0.0002772650602409638, 'epoch': 11.3}\n",
            "{'loss': 0.1147, 'learning_rate': 0.0002769036144578313, 'epoch': 11.4}\n",
            "{'loss': 0.1227, 'learning_rate': 0.0002765421686746988, 'epoch': 11.5}\n",
            "{'loss': 0.0999, 'learning_rate': 0.0002761807228915662, 'epoch': 11.6}\n",
            "{'loss': 0.1151, 'learning_rate': 0.0002758192771084337, 'epoch': 11.7}\n",
            "{'loss': 0.0949, 'learning_rate': 0.0002754578313253012, 'epoch': 11.8}\n",
            "{'loss': 0.111, 'learning_rate': 0.0002750963855421687, 'epoch': 11.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2007, 'learning_rate': 0.0002747349397590361, 'epoch': 12.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6514ed94d2b49e3ba97bff2033b2781",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.03761187940835953, 'eval_wer': 0.4150714804943058, 'eval_runtime': 25.6359, 'eval_samples_per_second': 31.206, 'eval_steps_per_second': 3.901, 'epoch': 12.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1381, 'learning_rate': 0.0002743734939759036, 'epoch': 12.1}\n",
            "{'loss': 0.1167, 'learning_rate': 0.0002740120481927711, 'epoch': 12.2}\n",
            "{'loss': 0.1075, 'learning_rate': 0.0002736506024096385, 'epoch': 12.3}\n",
            "{'loss': 0.1222, 'learning_rate': 0.000273289156626506, 'epoch': 12.4}\n",
            "{'loss': 0.1164, 'learning_rate': 0.0002729277108433735, 'epoch': 12.5}\n",
            "{'loss': 0.0846, 'learning_rate': 0.0002725662650602409, 'epoch': 12.6}\n",
            "{'loss': 0.1112, 'learning_rate': 0.0002722048192771084, 'epoch': 12.7}\n",
            "{'loss': 0.1331, 'learning_rate': 0.0002718433734939759, 'epoch': 12.8}\n",
            "{'loss': 0.0759, 'learning_rate': 0.0002714819277108433, 'epoch': 12.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1498, 'learning_rate': 0.0002711204819277108, 'epoch': 13.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a44ecce006d04197864e6a7ce5e0547c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.022112470120191574, 'eval_wer': 0.40247152895565785, 'eval_runtime': 25.5755, 'eval_samples_per_second': 31.28, 'eval_steps_per_second': 3.91, 'epoch': 13.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1096, 'learning_rate': 0.0002707590361445783, 'epoch': 13.1}\n",
            "{'loss': 0.0743, 'learning_rate': 0.00027039759036144573, 'epoch': 13.2}\n",
            "{'loss': 0.0993, 'learning_rate': 0.0002700361445783132, 'epoch': 13.3}\n",
            "{'loss': 0.0815, 'learning_rate': 0.0002696746987951807, 'epoch': 13.4}\n",
            "{'loss': 0.079, 'learning_rate': 0.00026931325301204814, 'epoch': 13.5}\n",
            "{'loss': 0.0959, 'learning_rate': 0.0002689518072289157, 'epoch': 13.6}\n",
            "{'loss': 0.0933, 'learning_rate': 0.0002685903614457831, 'epoch': 13.7}\n",
            "{'loss': 0.0567, 'learning_rate': 0.00026822891566265054, 'epoch': 13.8}\n",
            "{'loss': 0.1134, 'learning_rate': 0.0002678674698795181, 'epoch': 13.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0993, 'learning_rate': 0.0002675060240963855, 'epoch': 14.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6425b623b5be443ca6c43de6b23658e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0242829080671072, 'eval_wer': 0.3930215653016719, 'eval_runtime': 25.9057, 'eval_samples_per_second': 30.881, 'eval_steps_per_second': 3.86, 'epoch': 14.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0787, 'learning_rate': 0.000267144578313253, 'epoch': 14.1}\n",
            "{'loss': 0.1218, 'learning_rate': 0.0002667831325301205, 'epoch': 14.2}\n",
            "{'loss': 0.0848, 'learning_rate': 0.0002664216867469879, 'epoch': 14.3}\n",
            "{'loss': 0.1017, 'learning_rate': 0.0002660602409638554, 'epoch': 14.4}\n",
            "{'loss': 0.0836, 'learning_rate': 0.0002656987951807229, 'epoch': 14.5}\n",
            "{'loss': 0.0744, 'learning_rate': 0.0002653373493975903, 'epoch': 14.6}\n",
            "{'loss': 0.0972, 'learning_rate': 0.0002649759036144578, 'epoch': 14.7}\n",
            "{'loss': 0.0834, 'learning_rate': 0.0002646144578313253, 'epoch': 14.8}\n",
            "{'loss': 0.0801, 'learning_rate': 0.00026425301204819273, 'epoch': 14.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1126, 'learning_rate': 0.0002638915662650602, 'epoch': 15.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3376749e498b4c47a6505ff9811619ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.019084060564637184, 'eval_wer': 0.39350617882238914, 'eval_runtime': 25.8768, 'eval_samples_per_second': 30.916, 'eval_steps_per_second': 3.864, 'epoch': 15.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1003, 'learning_rate': 0.0002635301204819277, 'epoch': 15.1}\n",
            "{'loss': 0.1015, 'learning_rate': 0.00026316867469879514, 'epoch': 15.2}\n",
            "{'loss': 0.115, 'learning_rate': 0.0002628072289156626, 'epoch': 15.3}\n",
            "{'loss': 0.0901, 'learning_rate': 0.0002624457831325301, 'epoch': 15.4}\n",
            "{'loss': 0.0947, 'learning_rate': 0.00026208433734939754, 'epoch': 15.5}\n",
            "{'loss': 0.0765, 'learning_rate': 0.00026172289156626503, 'epoch': 15.6}\n",
            "{'loss': 0.0915, 'learning_rate': 0.0002613614457831325, 'epoch': 15.7}\n",
            "{'loss': 0.0791, 'learning_rate': 0.000261, 'epoch': 15.8}\n",
            "{'loss': 0.091, 'learning_rate': 0.00026063855421686743, 'epoch': 15.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1012, 'learning_rate': 0.0002602771084337349, 'epoch': 16.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9229f8c2610e43b49d1181d0fbd3e936",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.016041653230786324, 'eval_wer': 0.2866488975042404, 'eval_runtime': 25.9694, 'eval_samples_per_second': 30.806, 'eval_steps_per_second': 3.851, 'epoch': 16.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0725, 'learning_rate': 0.0002599156626506024, 'epoch': 16.1}\n",
            "{'loss': 0.0912, 'learning_rate': 0.00025955421686746984, 'epoch': 16.2}\n",
            "{'loss': 0.0996, 'learning_rate': 0.0002591927710843373, 'epoch': 16.3}\n",
            "{'loss': 0.0975, 'learning_rate': 0.0002588313253012048, 'epoch': 16.4}\n",
            "{'loss': 0.1112, 'learning_rate': 0.00025846987951807224, 'epoch': 16.5}\n",
            "{'loss': 0.0948, 'learning_rate': 0.00025810843373493973, 'epoch': 16.6}\n",
            "{'loss': 0.1016, 'learning_rate': 0.0002577469879518072, 'epoch': 16.7}\n",
            "{'loss': 0.1024, 'learning_rate': 0.00025738554216867465, 'epoch': 16.8}\n",
            "{'loss': 0.0885, 'learning_rate': 0.00025702409638554214, 'epoch': 16.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0922, 'learning_rate': 0.0002566626506024096, 'epoch': 17.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e47340e3ad84a78b5ba821a0ada3aa5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.019847767427563667, 'eval_wer': 0.39738308698812697, 'eval_runtime': 25.6348, 'eval_samples_per_second': 31.208, 'eval_steps_per_second': 3.901, 'epoch': 17.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0774, 'learning_rate': 0.00025630120481927706, 'epoch': 17.1}\n",
            "{'loss': 0.123, 'learning_rate': 0.00025593975903614454, 'epoch': 17.2}\n",
            "{'loss': 0.0965, 'learning_rate': 0.00025557831325301203, 'epoch': 17.3}\n",
            "{'loss': 0.0742, 'learning_rate': 0.00025521686746987946, 'epoch': 17.4}\n",
            "{'loss': 0.0631, 'learning_rate': 0.00025485542168674695, 'epoch': 17.5}\n",
            "{'loss': 0.0718, 'learning_rate': 0.00025449397590361443, 'epoch': 17.6}\n",
            "{'loss': 0.0667, 'learning_rate': 0.00025413253012048187, 'epoch': 17.7}\n",
            "{'loss': 0.0892, 'learning_rate': 0.0002537710843373494, 'epoch': 17.8}\n",
            "{'loss': 0.0803, 'learning_rate': 0.00025340963855421684, 'epoch': 17.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0935, 'learning_rate': 0.0002530481927710843, 'epoch': 18.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d6cdece6d34490ea5e22c471b40a042",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.01823374442756176, 'eval_wer': 0.4204022292221953, 'eval_runtime': 26.0711, 'eval_samples_per_second': 30.685, 'eval_steps_per_second': 3.836, 'epoch': 18.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0918, 'learning_rate': 0.0002526867469879518, 'epoch': 18.1}\n",
            "{'loss': 0.0919, 'learning_rate': 0.00025232530120481925, 'epoch': 18.2}\n",
            "{'loss': 0.0919, 'learning_rate': 0.00025196385542168673, 'epoch': 18.3}\n",
            "{'loss': 0.0947, 'learning_rate': 0.0002516024096385542, 'epoch': 18.4}\n",
            "{'loss': 0.0774, 'learning_rate': 0.00025124096385542165, 'epoch': 18.5}\n",
            "{'loss': 0.0738, 'learning_rate': 0.00025087951807228914, 'epoch': 18.6}\n",
            "{'loss': 0.0979, 'learning_rate': 0.0002505180722891566, 'epoch': 18.7}\n",
            "{'loss': 0.103, 'learning_rate': 0.00025015662650602406, 'epoch': 18.8}\n",
            "{'loss': 0.0887, 'learning_rate': 0.00024979518072289154, 'epoch': 18.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0868, 'learning_rate': 0.00024943373493975903, 'epoch': 19.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80b7d68af9a140d18b8d1e7e113227e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.01238958165049553, 'eval_wer': 0.38308698812696873, 'eval_runtime': 25.6486, 'eval_samples_per_second': 31.191, 'eval_steps_per_second': 3.899, 'epoch': 19.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0668, 'learning_rate': 0.00024907228915662646, 'epoch': 19.1}\n",
            "{'loss': 0.0887, 'learning_rate': 0.00024871084337349395, 'epoch': 19.2}\n",
            "{'loss': 0.0717, 'learning_rate': 0.00024834939759036144, 'epoch': 19.3}\n",
            "{'loss': 0.0628, 'learning_rate': 0.00024798795180722887, 'epoch': 19.4}\n",
            "{'loss': 0.0961, 'learning_rate': 0.00024762650602409635, 'epoch': 19.5}\n",
            "{'loss': 0.0758, 'learning_rate': 0.00024726506024096384, 'epoch': 19.6}\n",
            "{'loss': 0.085, 'learning_rate': 0.0002469036144578313, 'epoch': 19.7}\n",
            "{'loss': 0.0561, 'learning_rate': 0.00024654216867469876, 'epoch': 19.8}\n",
            "{'loss': 0.0688, 'learning_rate': 0.00024618072289156625, 'epoch': 19.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0722, 'learning_rate': 0.00024581927710843373, 'epoch': 20.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d35b0c44eedb4c7b9bee89738b65c0e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.017730452120304108, 'eval_wer': 0.3886600436152169, 'eval_runtime': 25.7729, 'eval_samples_per_second': 31.04, 'eval_steps_per_second': 3.88, 'epoch': 20.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0712, 'learning_rate': 0.00024545783132530116, 'epoch': 20.1}\n",
            "{'loss': 0.0745, 'learning_rate': 0.00024509638554216865, 'epoch': 20.2}\n",
            "{'loss': 0.0498, 'learning_rate': 0.00024473493975903614, 'epoch': 20.3}\n",
            "{'loss': 0.0658, 'learning_rate': 0.00024437349397590357, 'epoch': 20.4}\n",
            "{'loss': 0.0736, 'learning_rate': 0.00024401204819277106, 'epoch': 20.5}\n",
            "{'loss': 0.0794, 'learning_rate': 0.00024365060240963852, 'epoch': 20.6}\n",
            "{'loss': 0.0879, 'learning_rate': 0.00024328915662650598, 'epoch': 20.7}\n",
            "{'loss': 0.0859, 'learning_rate': 0.0002429277108433735, 'epoch': 20.8}\n",
            "{'loss': 0.0787, 'learning_rate': 0.00024256626506024095, 'epoch': 20.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0537, 'learning_rate': 0.0002422048192771084, 'epoch': 21.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3f955e28eca4c4882570b723a4065bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.014227272011339664, 'eval_wer': 0.39181003149987886, 'eval_runtime': 26.4214, 'eval_samples_per_second': 30.278, 'eval_steps_per_second': 3.785, 'epoch': 21.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0639, 'learning_rate': 0.0002418433734939759, 'epoch': 21.1}\n",
            "{'loss': 0.0708, 'learning_rate': 0.00024148192771084335, 'epoch': 21.2}\n",
            "{'loss': 0.0696, 'learning_rate': 0.00024112048192771081, 'epoch': 21.3}\n",
            "{'loss': 0.0595, 'learning_rate': 0.0002407590361445783, 'epoch': 21.4}\n",
            "{'loss': 0.0762, 'learning_rate': 0.00024039759036144576, 'epoch': 21.5}\n",
            "{'loss': 0.0535, 'learning_rate': 0.00024003614457831322, 'epoch': 21.6}\n",
            "{'loss': 0.0676, 'learning_rate': 0.0002396746987951807, 'epoch': 21.7}\n",
            "{'loss': 0.06, 'learning_rate': 0.00023931325301204817, 'epoch': 21.8}\n",
            "{'loss': 0.0414, 'learning_rate': 0.00023895180722891562, 'epoch': 21.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0754, 'learning_rate': 0.0002385903614457831, 'epoch': 22.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1699043f1b434548b0eb7706e1b83996",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.013142815791070461, 'eval_wer': 0.352798643082142, 'eval_runtime': 26.1617, 'eval_samples_per_second': 30.579, 'eval_steps_per_second': 3.822, 'epoch': 22.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0449, 'learning_rate': 0.00023822891566265057, 'epoch': 22.1}\n",
            "{'loss': 0.0542, 'learning_rate': 0.00023786746987951803, 'epoch': 22.2}\n",
            "{'loss': 0.0552, 'learning_rate': 0.00023750602409638552, 'epoch': 22.3}\n",
            "{'loss': 0.0622, 'learning_rate': 0.00023714457831325298, 'epoch': 22.4}\n",
            "{'loss': 0.0753, 'learning_rate': 0.00023678313253012044, 'epoch': 22.5}\n",
            "{'loss': 0.0629, 'learning_rate': 0.00023642168674698795, 'epoch': 22.6}\n",
            "{'loss': 0.0774, 'learning_rate': 0.0002360602409638554, 'epoch': 22.7}\n",
            "{'loss': 0.0598, 'learning_rate': 0.00023569879518072284, 'epoch': 22.8}\n",
            "{'loss': 0.0568, 'learning_rate': 0.00023533734939759036, 'epoch': 22.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0529, 'learning_rate': 0.00023497590361445781, 'epoch': 23.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8fd344214af4d3fab733d7829b3817c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.010320049710571766, 'eval_wer': 0.33486794281560456, 'eval_runtime': 26.1158, 'eval_samples_per_second': 30.633, 'eval_steps_per_second': 3.829, 'epoch': 23.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.041, 'learning_rate': 0.00023461445783132527, 'epoch': 23.1}\n",
            "{'loss': 0.055, 'learning_rate': 0.00023425301204819276, 'epoch': 23.2}\n",
            "{'loss': 0.0371, 'learning_rate': 0.00023389156626506022, 'epoch': 23.3}\n",
            "{'loss': 0.0619, 'learning_rate': 0.00023353012048192768, 'epoch': 23.4}\n",
            "{'loss': 0.053, 'learning_rate': 0.00023316867469879517, 'epoch': 23.5}\n",
            "{'loss': 0.05, 'learning_rate': 0.00023280722891566263, 'epoch': 23.6}\n",
            "{'loss': 0.0376, 'learning_rate': 0.0002324457831325301, 'epoch': 23.7}\n",
            "{'loss': 0.0766, 'learning_rate': 0.00023208433734939757, 'epoch': 23.8}\n",
            "{'loss': 0.0599, 'learning_rate': 0.00023172289156626503, 'epoch': 23.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0914, 'learning_rate': 0.00023136144578313252, 'epoch': 24.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5f0611a71984c4792b2f1a84384ffb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.011931517161428928, 'eval_wer': 0.3891446571359341, 'eval_runtime': 26.1451, 'eval_samples_per_second': 30.598, 'eval_steps_per_second': 3.825, 'epoch': 24.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0573, 'learning_rate': 0.00023099999999999998, 'epoch': 24.1}\n",
            "{'loss': 0.0652, 'learning_rate': 0.00023063855421686744, 'epoch': 24.2}\n",
            "{'loss': 0.0554, 'learning_rate': 0.00023027710843373492, 'epoch': 24.3}\n",
            "{'loss': 0.0588, 'learning_rate': 0.00022991566265060238, 'epoch': 24.4}\n",
            "{'loss': 0.0501, 'learning_rate': 0.00022955421686746984, 'epoch': 24.5}\n",
            "{'loss': 0.1347, 'learning_rate': 0.00022919277108433736, 'epoch': 24.6}\n",
            "{'loss': 0.0501, 'learning_rate': 0.00022883132530120482, 'epoch': 24.7}\n",
            "{'loss': 0.0466, 'learning_rate': 0.00022846987951807227, 'epoch': 24.8}\n",
            "{'loss': 0.0401, 'learning_rate': 0.00022810843373493976, 'epoch': 24.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0467, 'learning_rate': 0.00022774698795180722, 'epoch': 25.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2875f244fef04899a826e69e4d26c80d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.008380824699997902, 'eval_wer': 0.39350617882238914, 'eval_runtime': 25.6564, 'eval_samples_per_second': 31.181, 'eval_steps_per_second': 3.898, 'epoch': 25.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0518, 'learning_rate': 0.00022738554216867468, 'epoch': 25.1}\n",
            "{'loss': 0.0574, 'learning_rate': 0.00022702409638554217, 'epoch': 25.2}\n",
            "{'loss': 0.0832, 'learning_rate': 0.00022666265060240963, 'epoch': 25.3}\n",
            "{'loss': 0.0443, 'learning_rate': 0.00022630120481927709, 'epoch': 25.4}\n",
            "{'loss': 0.0586, 'learning_rate': 0.00022593975903614457, 'epoch': 25.5}\n",
            "{'loss': 0.0381, 'learning_rate': 0.00022557831325301203, 'epoch': 25.6}\n",
            "{'loss': 0.0661, 'learning_rate': 0.0002252168674698795, 'epoch': 25.7}\n",
            "{'loss': 0.0428, 'learning_rate': 0.00022485542168674698, 'epoch': 25.8}\n",
            "{'loss': 0.0614, 'learning_rate': 0.00022449397590361444, 'epoch': 25.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0715, 'learning_rate': 0.0002241325301204819, 'epoch': 26.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bfc82c74f8a45b0a8d2d431eb8ff9d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.010161702521145344, 'eval_wer': 0.3913254179791616, 'eval_runtime': 26.8044, 'eval_samples_per_second': 29.846, 'eval_steps_per_second': 3.731, 'epoch': 26.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0429, 'learning_rate': 0.00022377108433734938, 'epoch': 26.1}\n",
            "{'loss': 0.048, 'learning_rate': 0.00022340963855421684, 'epoch': 26.2}\n",
            "{'loss': 0.0354, 'learning_rate': 0.0002230481927710843, 'epoch': 26.3}\n",
            "{'loss': 0.043, 'learning_rate': 0.00022268674698795182, 'epoch': 26.4}\n",
            "{'loss': 0.0456, 'learning_rate': 0.00022232530120481928, 'epoch': 26.5}\n",
            "{'loss': 0.0476, 'learning_rate': 0.0002219638554216867, 'epoch': 26.6}\n",
            "{'loss': 0.0414, 'learning_rate': 0.00022160240963855422, 'epoch': 26.7}\n",
            "{'loss': 0.0582, 'learning_rate': 0.00022124096385542168, 'epoch': 26.8}\n",
            "{'loss': 0.0498, 'learning_rate': 0.00022087951807228914, 'epoch': 26.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0756, 'learning_rate': 0.00022051807228915663, 'epoch': 27.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa0f7e36d4ae46e0907ebc9ba0ee671f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.010732774622738361, 'eval_wer': 0.3796946934819481, 'eval_runtime': 25.6344, 'eval_samples_per_second': 31.208, 'eval_steps_per_second': 3.901, 'epoch': 27.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0598, 'learning_rate': 0.00022015662650602409, 'epoch': 27.1}\n",
            "{'loss': 0.0701, 'learning_rate': 0.00021979518072289155, 'epoch': 27.2}\n",
            "{'loss': 0.0554, 'learning_rate': 0.00021943373493975903, 'epoch': 27.3}\n",
            "{'loss': 0.0417, 'learning_rate': 0.0002190722891566265, 'epoch': 27.4}\n",
            "{'loss': 0.0763, 'learning_rate': 0.00021871084337349395, 'epoch': 27.5}\n",
            "{'loss': 0.069, 'learning_rate': 0.00021834939759036144, 'epoch': 27.6}\n",
            "{'loss': 0.0811, 'learning_rate': 0.0002179879518072289, 'epoch': 27.7}\n",
            "{'loss': 0.0848, 'learning_rate': 0.00021762650602409636, 'epoch': 27.8}\n",
            "{'loss': 0.0787, 'learning_rate': 0.00021726506024096384, 'epoch': 27.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0915, 'learning_rate': 0.0002169036144578313, 'epoch': 28.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f13d0cc5fa18402a9a7e94ab5006c4f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.010551259852945805, 'eval_wer': 0.3947177126241822, 'eval_runtime': 26.3216, 'eval_samples_per_second': 30.393, 'eval_steps_per_second': 3.799, 'epoch': 28.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0401, 'learning_rate': 0.00021654216867469876, 'epoch': 28.1}\n",
            "{'loss': 0.0552, 'learning_rate': 0.00021618072289156625, 'epoch': 28.2}\n",
            "{'loss': 0.0753, 'learning_rate': 0.0002158192771084337, 'epoch': 28.3}\n",
            "{'loss': 0.054, 'learning_rate': 0.00021545783132530117, 'epoch': 28.4}\n",
            "{'loss': 0.0492, 'learning_rate': 0.00021509638554216868, 'epoch': 28.5}\n",
            "{'loss': 0.0517, 'learning_rate': 0.00021473493975903614, 'epoch': 28.6}\n",
            "{'loss': 0.0451, 'learning_rate': 0.0002143734939759036, 'epoch': 28.7}\n",
            "{'loss': 0.0433, 'learning_rate': 0.0002140120481927711, 'epoch': 28.8}\n",
            "{'loss': 0.059, 'learning_rate': 0.00021365060240963855, 'epoch': 28.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0416, 'learning_rate': 0.000213289156626506, 'epoch': 29.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45d30ae591c64cee917f13c17b88d784",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.012106886133551598, 'eval_wer': 0.3896292706566513, 'eval_runtime': 26.1165, 'eval_samples_per_second': 30.632, 'eval_steps_per_second': 3.829, 'epoch': 29.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0499, 'learning_rate': 0.0002129277108433735, 'epoch': 29.1}\n",
            "{'loss': 0.0502, 'learning_rate': 0.00021256626506024095, 'epoch': 29.2}\n",
            "{'loss': 0.0566, 'learning_rate': 0.0002122048192771084, 'epoch': 29.3}\n",
            "{'loss': 0.0455, 'learning_rate': 0.0002118433734939759, 'epoch': 29.4}\n",
            "{'loss': 0.0684, 'learning_rate': 0.00021148192771084336, 'epoch': 29.5}\n",
            "{'loss': 0.0521, 'learning_rate': 0.00021112048192771082, 'epoch': 29.6}\n",
            "{'loss': 0.0571, 'learning_rate': 0.0002107590361445783, 'epoch': 29.7}\n",
            "{'loss': 0.05, 'learning_rate': 0.00021039759036144576, 'epoch': 29.8}\n",
            "{'loss': 0.0528, 'learning_rate': 0.00021003614457831322, 'epoch': 29.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0478, 'learning_rate': 0.0002096746987951807, 'epoch': 30.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d99571443c864899b53febf7252a1743",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.006294212304055691, 'eval_wer': 0.37630239883692757, 'eval_runtime': 26.6382, 'eval_samples_per_second': 30.032, 'eval_steps_per_second': 3.754, 'epoch': 30.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0504, 'learning_rate': 0.00020931325301204817, 'epoch': 30.1}\n",
            "{'loss': 0.05, 'learning_rate': 0.00020895180722891563, 'epoch': 30.2}\n",
            "{'loss': 0.0458, 'learning_rate': 0.00020859036144578311, 'epoch': 30.3}\n",
            "{'loss': 0.0409, 'learning_rate': 0.00020822891566265057, 'epoch': 30.4}\n",
            "{'loss': 0.0418, 'learning_rate': 0.00020786746987951803, 'epoch': 30.5}\n",
            "{'loss': 0.0382, 'learning_rate': 0.00020750602409638555, 'epoch': 30.6}\n",
            "{'loss': 0.037, 'learning_rate': 0.000207144578313253, 'epoch': 30.7}\n",
            "{'loss': 0.048, 'learning_rate': 0.00020678313253012047, 'epoch': 30.8}\n",
            "{'loss': 0.0325, 'learning_rate': 0.00020642168674698795, 'epoch': 30.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0421, 'learning_rate': 0.0002060602409638554, 'epoch': 31.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d565ceb761404e24ae53436e4c7d1b06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00601582694798708, 'eval_wer': 0.37436394475405865, 'eval_runtime': 26.4751, 'eval_samples_per_second': 30.217, 'eval_steps_per_second': 3.777, 'epoch': 31.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0383, 'learning_rate': 0.00020569879518072287, 'epoch': 31.1}\n",
            "{'loss': 0.0481, 'learning_rate': 0.00020533734939759036, 'epoch': 31.2}\n",
            "{'loss': 0.055, 'learning_rate': 0.00020497590361445782, 'epoch': 31.3}\n",
            "{'loss': 0.0582, 'learning_rate': 0.00020461445783132528, 'epoch': 31.4}\n",
            "{'loss': 0.0563, 'learning_rate': 0.00020425301204819276, 'epoch': 31.5}\n",
            "{'loss': 0.0371, 'learning_rate': 0.00020389156626506022, 'epoch': 31.6}\n",
            "{'loss': 0.0524, 'learning_rate': 0.00020353012048192768, 'epoch': 31.7}\n",
            "{'loss': 0.0388, 'learning_rate': 0.00020316867469879517, 'epoch': 31.8}\n",
            "{'loss': 0.0421, 'learning_rate': 0.00020280722891566263, 'epoch': 31.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0323, 'learning_rate': 0.0002024457831325301, 'epoch': 32.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a87277bfb744686baee4021edc750b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.005780806764960289, 'eval_wer': 0.3288102738066392, 'eval_runtime': 26.4122, 'eval_samples_per_second': 30.289, 'eval_steps_per_second': 3.786, 'epoch': 32.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0242, 'learning_rate': 0.00020208433734939757, 'epoch': 32.1}\n",
            "{'loss': 0.0421, 'learning_rate': 0.00020172289156626503, 'epoch': 32.2}\n",
            "{'loss': 0.05, 'learning_rate': 0.0002013614457831325, 'epoch': 32.3}\n",
            "{'loss': 0.0397, 'learning_rate': 0.000201, 'epoch': 32.4}\n",
            "{'loss': 0.04, 'learning_rate': 0.00020063855421686747, 'epoch': 32.5}\n",
            "{'loss': 0.0361, 'learning_rate': 0.0002002771084337349, 'epoch': 32.6}\n",
            "{'loss': 0.0417, 'learning_rate': 0.0001999156626506024, 'epoch': 32.7}\n",
            "{'loss': 0.0312, 'learning_rate': 0.00019955421686746987, 'epoch': 32.8}\n",
            "{'loss': 0.034, 'learning_rate': 0.00019919277108433733, 'epoch': 32.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0315, 'learning_rate': 0.00019883132530120482, 'epoch': 33.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b58f653c45b4486bacdd78dee407f5b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.005104288924485445, 'eval_wer': 0.32129876423552217, 'eval_runtime': 25.9011, 'eval_samples_per_second': 30.887, 'eval_steps_per_second': 3.861, 'epoch': 33.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0381, 'learning_rate': 0.00019846987951807228, 'epoch': 33.1}\n",
            "{'loss': 0.029, 'learning_rate': 0.00019810843373493974, 'epoch': 33.2}\n",
            "{'loss': 0.0521, 'learning_rate': 0.00019774698795180722, 'epoch': 33.3}\n",
            "{'loss': 0.0361, 'learning_rate': 0.00019738554216867468, 'epoch': 33.4}\n",
            "{'loss': 0.0461, 'learning_rate': 0.00019702409638554214, 'epoch': 33.5}\n",
            "{'loss': 0.0546, 'learning_rate': 0.00019666265060240963, 'epoch': 33.6}\n",
            "{'loss': 0.0496, 'learning_rate': 0.0001963012048192771, 'epoch': 33.7}\n",
            "{'loss': 0.0385, 'learning_rate': 0.00019593975903614455, 'epoch': 33.8}\n",
            "{'loss': 0.0386, 'learning_rate': 0.00019557831325301203, 'epoch': 33.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0346, 'learning_rate': 0.0001952168674698795, 'epoch': 34.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71e3cf028e7c4044a452bc3bf39a2d1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.008000246249139309, 'eval_wer': 0.3770293191180034, 'eval_runtime': 26.3437, 'eval_samples_per_second': 30.368, 'eval_steps_per_second': 3.796, 'epoch': 34.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0266, 'learning_rate': 0.00019485542168674695, 'epoch': 34.1}\n",
            "{'loss': 0.047, 'learning_rate': 0.00019449397590361444, 'epoch': 34.2}\n",
            "{'loss': 0.0463, 'learning_rate': 0.0001941325301204819, 'epoch': 34.3}\n",
            "{'loss': 0.0366, 'learning_rate': 0.00019377108433734936, 'epoch': 34.4}\n",
            "{'loss': 0.0321, 'learning_rate': 0.00019340963855421687, 'epoch': 34.5}\n",
            "{'loss': 0.0239, 'learning_rate': 0.00019304819277108433, 'epoch': 34.6}\n",
            "{'loss': 0.0295, 'learning_rate': 0.0001926867469879518, 'epoch': 34.7}\n",
            "{'loss': 0.0356, 'learning_rate': 0.00019232530120481928, 'epoch': 34.8}\n",
            "{'loss': 0.0351, 'learning_rate': 0.00019196385542168674, 'epoch': 34.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.034, 'learning_rate': 0.0001916024096385542, 'epoch': 35.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5885cef4fdcd4de5bd55b0c380fc28e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.005529549904167652, 'eval_wer': 0.37242549067118974, 'eval_runtime': 26.0331, 'eval_samples_per_second': 30.73, 'eval_steps_per_second': 3.841, 'epoch': 35.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0329, 'learning_rate': 0.00019124096385542168, 'epoch': 35.1}\n",
            "{'loss': 0.0309, 'learning_rate': 0.00019087951807228914, 'epoch': 35.2}\n",
            "{'loss': 0.0382, 'learning_rate': 0.0001905180722891566, 'epoch': 35.3}\n",
            "{'loss': 0.0305, 'learning_rate': 0.0001901566265060241, 'epoch': 35.4}\n",
            "{'loss': 0.0306, 'learning_rate': 0.00018979518072289155, 'epoch': 35.5}\n",
            "{'loss': 0.0282, 'learning_rate': 0.000189433734939759, 'epoch': 35.6}\n",
            "{'loss': 0.0386, 'learning_rate': 0.0001890722891566265, 'epoch': 35.7}\n",
            "{'loss': 0.0274, 'learning_rate': 0.00018871084337349395, 'epoch': 35.8}\n",
            "{'loss': 0.0376, 'learning_rate': 0.0001883493975903614, 'epoch': 35.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0317, 'learning_rate': 0.0001879879518072289, 'epoch': 36.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0193a71f1a0648b9a250ecc88010b756",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0050554098561406136, 'eval_wer': 0.34674097407317667, 'eval_runtime': 26.5134, 'eval_samples_per_second': 30.173, 'eval_steps_per_second': 3.772, 'epoch': 36.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0321, 'learning_rate': 0.00018762650602409636, 'epoch': 36.1}\n",
            "{'loss': 0.0315, 'learning_rate': 0.00018726506024096382, 'epoch': 36.2}\n",
            "{'loss': 0.0329, 'learning_rate': 0.00018690361445783133, 'epoch': 36.3}\n",
            "{'loss': 0.0299, 'learning_rate': 0.00018654216867469876, 'epoch': 36.4}\n",
            "{'loss': 0.031, 'learning_rate': 0.00018618072289156622, 'epoch': 36.5}\n",
            "{'loss': 0.0284, 'learning_rate': 0.00018581927710843374, 'epoch': 36.6}\n",
            "{'loss': 0.0335, 'learning_rate': 0.0001854578313253012, 'epoch': 36.7}\n",
            "{'loss': 0.0317, 'learning_rate': 0.00018509638554216866, 'epoch': 36.8}\n",
            "{'loss': 0.0301, 'learning_rate': 0.00018473493975903614, 'epoch': 36.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0596, 'learning_rate': 0.0001843734939759036, 'epoch': 37.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ab662a5d8854707b973b4def5676f5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.004546373151242733, 'eval_wer': 0.3823600678458929, 'eval_runtime': 26.1228, 'eval_samples_per_second': 30.625, 'eval_steps_per_second': 3.828, 'epoch': 37.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0304, 'learning_rate': 0.00018401204819277106, 'epoch': 37.1}\n",
            "{'loss': 0.028, 'learning_rate': 0.00018365060240963855, 'epoch': 37.2}\n",
            "{'loss': 0.0341, 'learning_rate': 0.000183289156626506, 'epoch': 37.3}\n",
            "{'loss': 0.0503, 'learning_rate': 0.00018292771084337347, 'epoch': 37.4}\n",
            "{'loss': 0.0239, 'learning_rate': 0.00018256626506024095, 'epoch': 37.5}\n",
            "{'loss': 0.037, 'learning_rate': 0.00018220481927710841, 'epoch': 37.6}\n",
            "{'loss': 0.0306, 'learning_rate': 0.00018184337349397587, 'epoch': 37.7}\n",
            "{'loss': 0.0316, 'learning_rate': 0.00018148192771084336, 'epoch': 37.8}\n",
            "{'loss': 0.0313, 'learning_rate': 0.00018112048192771082, 'epoch': 37.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0469, 'learning_rate': 0.00018075903614457828, 'epoch': 38.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c099608266d4f7aa5caedd06bd96373",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0067526535131037235, 'eval_wer': 0.404894596559244, 'eval_runtime': 26.0131, 'eval_samples_per_second': 30.754, 'eval_steps_per_second': 3.844, 'epoch': 38.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0303, 'learning_rate': 0.00018039759036144577, 'epoch': 38.1}\n",
            "{'loss': 0.0314, 'learning_rate': 0.00018003614457831322, 'epoch': 38.2}\n",
            "{'loss': 0.0396, 'learning_rate': 0.00017967469879518068, 'epoch': 38.3}\n",
            "{'loss': 0.0344, 'learning_rate': 0.0001793132530120482, 'epoch': 38.4}\n",
            "{'loss': 0.039, 'learning_rate': 0.00017895180722891566, 'epoch': 38.5}\n",
            "{'loss': 0.0328, 'learning_rate': 0.00017859036144578312, 'epoch': 38.6}\n",
            "{'loss': 0.0332, 'learning_rate': 0.0001782289156626506, 'epoch': 38.7}\n",
            "{'loss': 0.0451, 'learning_rate': 0.00017786746987951806, 'epoch': 38.8}\n",
            "{'loss': 0.0562, 'learning_rate': 0.00017750602409638552, 'epoch': 38.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0412, 'learning_rate': 0.000177144578313253, 'epoch': 39.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8875c4500e74e55be75abeb5e6f1718",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.006687963381409645, 'eval_wer': 0.41143687908892657, 'eval_runtime': 26.5428, 'eval_samples_per_second': 30.14, 'eval_steps_per_second': 3.768, 'epoch': 39.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-3900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0385, 'learning_rate': 0.00017678313253012047, 'epoch': 39.1}\n",
            "{'loss': 0.0378, 'learning_rate': 0.00017642168674698793, 'epoch': 39.2}\n",
            "{'loss': 0.0433, 'learning_rate': 0.00017606024096385541, 'epoch': 39.3}\n",
            "{'loss': 0.0439, 'learning_rate': 0.00017569879518072287, 'epoch': 39.4}\n",
            "{'loss': 0.0336, 'learning_rate': 0.00017533734939759033, 'epoch': 39.5}\n",
            "{'loss': 0.0326, 'learning_rate': 0.00017497590361445782, 'epoch': 39.6}\n",
            "{'loss': 0.0282, 'learning_rate': 0.00017461445783132528, 'epoch': 39.7}\n",
            "{'loss': 0.0291, 'learning_rate': 0.00017425301204819274, 'epoch': 39.8}\n",
            "{'loss': 0.029, 'learning_rate': 0.00017389156626506023, 'epoch': 39.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0308, 'learning_rate': 0.00017353012048192768, 'epoch': 40.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fec8571eda04259b8be1ecf6c349f7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0031552172731608152, 'eval_wer': 0.38793312333414104, 'eval_runtime': 26.0346, 'eval_samples_per_second': 30.728, 'eval_steps_per_second': 3.841, 'epoch': 40.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0295, 'learning_rate': 0.00017316867469879514, 'epoch': 40.1}\n",
            "{'loss': 0.0347, 'learning_rate': 0.00017280722891566263, 'epoch': 40.2}\n",
            "{'loss': 0.0326, 'learning_rate': 0.0001724457831325301, 'epoch': 40.3}\n",
            "{'loss': 0.0305, 'learning_rate': 0.00017208433734939755, 'epoch': 40.4}\n",
            "{'loss': 0.0285, 'learning_rate': 0.00017172289156626506, 'epoch': 40.5}\n",
            "{'loss': 0.0291, 'learning_rate': 0.00017136144578313252, 'epoch': 40.6}\n",
            "{'loss': 0.0259, 'learning_rate': 0.00017099999999999998, 'epoch': 40.7}\n",
            "{'loss': 0.0242, 'learning_rate': 0.00017063855421686747, 'epoch': 40.8}\n",
            "{'loss': 0.0297, 'learning_rate': 0.00017027710843373493, 'epoch': 40.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0426, 'learning_rate': 0.0001699156626506024, 'epoch': 41.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00d5c15777b748d48f240e6dc97ee1fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0038499063812196255, 'eval_wer': 0.4041676762781682, 'eval_runtime': 27.012, 'eval_samples_per_second': 29.616, 'eval_steps_per_second': 3.702, 'epoch': 41.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-3900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0216, 'learning_rate': 0.00016955421686746987, 'epoch': 41.1}\n",
            "{'loss': 0.0416, 'learning_rate': 0.00016919277108433733, 'epoch': 41.2}\n",
            "{'loss': 0.0272, 'learning_rate': 0.0001688313253012048, 'epoch': 41.3}\n",
            "{'loss': 0.034, 'learning_rate': 0.00016846987951807228, 'epoch': 41.4}\n",
            "{'loss': 0.0357, 'learning_rate': 0.00016810843373493974, 'epoch': 41.5}\n",
            "{'loss': 0.0271, 'learning_rate': 0.0001677469879518072, 'epoch': 41.6}\n",
            "{'loss': 0.0272, 'learning_rate': 0.00016738554216867469, 'epoch': 41.7}\n",
            "{'loss': 0.0281, 'learning_rate': 0.00016702409638554214, 'epoch': 41.8}\n",
            "{'loss': 0.0313, 'learning_rate': 0.0001666626506024096, 'epoch': 41.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0333, 'learning_rate': 0.0001663012048192771, 'epoch': 42.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "519edb4c565e4d7e9e45b603e8deabaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00388857489451766, 'eval_wer': 0.3881754300944996, 'eval_runtime': 26.0239, 'eval_samples_per_second': 30.741, 'eval_steps_per_second': 3.843, 'epoch': 42.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0236, 'learning_rate': 0.00016593975903614455, 'epoch': 42.1}\n",
            "{'loss': 0.0243, 'learning_rate': 0.000165578313253012, 'epoch': 42.2}\n",
            "{'loss': 0.0383, 'learning_rate': 0.00016521686746987952, 'epoch': 42.3}\n",
            "{'loss': 0.0231, 'learning_rate': 0.00016485542168674696, 'epoch': 42.4}\n",
            "{'loss': 0.0271, 'learning_rate': 0.00016449397590361442, 'epoch': 42.5}\n",
            "{'loss': 0.0518, 'learning_rate': 0.00016413253012048193, 'epoch': 42.6}\n",
            "{'loss': 0.025, 'learning_rate': 0.0001637710843373494, 'epoch': 42.7}\n",
            "{'loss': 0.0276, 'learning_rate': 0.00016340963855421685, 'epoch': 42.8}\n",
            "{'loss': 0.0381, 'learning_rate': 0.00016304819277108433, 'epoch': 42.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0312, 'learning_rate': 0.0001626867469879518, 'epoch': 43.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "603aa524a3fa45b0a089f1dd5046fdce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.002445411402732134, 'eval_wer': 0.37751393263872063, 'eval_runtime': 25.9108, 'eval_samples_per_second': 30.875, 'eval_steps_per_second': 3.859, 'epoch': 43.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0229, 'learning_rate': 0.00016232530120481925, 'epoch': 43.1}\n",
            "{'loss': 0.0261, 'learning_rate': 0.00016196385542168674, 'epoch': 43.2}\n",
            "{'loss': 0.0216, 'learning_rate': 0.0001616024096385542, 'epoch': 43.3}\n",
            "{'loss': 0.0269, 'learning_rate': 0.00016124096385542166, 'epoch': 43.4}\n",
            "{'loss': 0.0271, 'learning_rate': 0.00016087951807228915, 'epoch': 43.5}\n",
            "{'loss': 0.0235, 'learning_rate': 0.0001605180722891566, 'epoch': 43.6}\n",
            "{'loss': 0.0257, 'learning_rate': 0.00016015662650602406, 'epoch': 43.7}\n",
            "{'loss': 0.0264, 'learning_rate': 0.00015979518072289155, 'epoch': 43.8}\n",
            "{'loss': 0.0284, 'learning_rate': 0.000159433734939759, 'epoch': 43.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0233, 'learning_rate': 0.00015907228915662647, 'epoch': 44.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80fd879664c24f2ebb0cfe96ea65bcdc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.002629012567922473, 'eval_wer': 0.38890235037557547, 'eval_runtime': 26.7668, 'eval_samples_per_second': 29.888, 'eval_steps_per_second': 3.736, 'epoch': 44.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0344, 'learning_rate': 0.00015871084337349396, 'epoch': 44.1}\n",
            "{'loss': 0.0348, 'learning_rate': 0.00015834939759036142, 'epoch': 44.2}\n",
            "{'loss': 0.0257, 'learning_rate': 0.00015798795180722888, 'epoch': 44.3}\n",
            "{'loss': 0.0272, 'learning_rate': 0.0001576265060240964, 'epoch': 44.4}\n",
            "{'loss': 0.0449, 'learning_rate': 0.00015726506024096385, 'epoch': 44.5}\n",
            "{'loss': 0.0264, 'learning_rate': 0.00015690361445783133, 'epoch': 44.6}\n",
            "{'loss': 0.0409, 'learning_rate': 0.0001565421686746988, 'epoch': 44.7}\n",
            "{'loss': 0.0262, 'learning_rate': 0.00015618072289156625, 'epoch': 44.8}\n",
            "{'loss': 0.0421, 'learning_rate': 0.00015581927710843374, 'epoch': 44.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0498, 'learning_rate': 0.0001554578313253012, 'epoch': 45.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1362a619be65474ab309099c4a5745b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.002272204263135791, 'eval_wer': 0.40392536951780955, 'eval_runtime': 26.0346, 'eval_samples_per_second': 30.728, 'eval_steps_per_second': 3.841, 'epoch': 45.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.019, 'learning_rate': 0.00015509638554216866, 'epoch': 45.1}\n",
            "{'loss': 0.015, 'learning_rate': 0.00015473493975903615, 'epoch': 45.2}\n",
            "{'loss': 0.0437, 'learning_rate': 0.0001543734939759036, 'epoch': 45.3}\n",
            "{'loss': 0.0147, 'learning_rate': 0.00015401204819277106, 'epoch': 45.4}\n",
            "{'loss': 0.0221, 'learning_rate': 0.00015365060240963855, 'epoch': 45.5}\n",
            "{'loss': 0.0336, 'learning_rate': 0.000153289156626506, 'epoch': 45.6}\n",
            "{'loss': 0.0225, 'learning_rate': 0.00015296385542168674, 'epoch': 45.7}\n",
            "{'loss': 0.0232, 'learning_rate': 0.0001526024096385542, 'epoch': 45.8}\n",
            "{'loss': 0.0309, 'learning_rate': 0.00015224096385542168, 'epoch': 45.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0215, 'learning_rate': 0.00015187951807228914, 'epoch': 46.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8bcf24adbb1841bdade29c5b260d187a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.004568495787680149, 'eval_wer': 0.39350617882238914, 'eval_runtime': 26.7595, 'eval_samples_per_second': 29.896, 'eval_steps_per_second': 3.737, 'epoch': 46.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.027, 'learning_rate': 0.0001515180722891566, 'epoch': 46.1}\n",
            "{'loss': 0.0196, 'learning_rate': 0.0001511566265060241, 'epoch': 46.2}\n",
            "{'loss': 0.0187, 'learning_rate': 0.00015079518072289155, 'epoch': 46.3}\n",
            "{'loss': 0.0191, 'learning_rate': 0.000150433734939759, 'epoch': 46.4}\n",
            "{'loss': 0.0357, 'learning_rate': 0.0001500722891566265, 'epoch': 46.5}\n",
            "{'loss': 0.0249, 'learning_rate': 0.00014971084337349395, 'epoch': 46.6}\n",
            "{'loss': 0.032, 'learning_rate': 0.00014934939759036144, 'epoch': 46.7}\n",
            "{'loss': 0.0309, 'learning_rate': 0.0001489879518072289, 'epoch': 46.8}\n",
            "{'loss': 0.0205, 'learning_rate': 0.00014862650602409636, 'epoch': 46.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0197, 'learning_rate': 0.00014826506024096385, 'epoch': 47.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70d932d844da40839b0f07db29ff3636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00299663539044559, 'eval_wer': 0.4000484613520717, 'eval_runtime': 26.5372, 'eval_samples_per_second': 30.146, 'eval_steps_per_second': 3.768, 'epoch': 47.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0153, 'learning_rate': 0.0001479036144578313, 'epoch': 47.1}\n",
            "{'loss': 0.0319, 'learning_rate': 0.0001475421686746988, 'epoch': 47.2}\n",
            "{'loss': 0.0301, 'learning_rate': 0.00014718072289156625, 'epoch': 47.3}\n",
            "{'loss': 0.0141, 'learning_rate': 0.0001468192771084337, 'epoch': 47.4}\n",
            "{'loss': 0.0182, 'learning_rate': 0.0001464578313253012, 'epoch': 47.5}\n",
            "{'loss': 0.0178, 'learning_rate': 0.00014609638554216866, 'epoch': 47.6}\n",
            "{'loss': 0.0162, 'learning_rate': 0.00014573493975903614, 'epoch': 47.7}\n",
            "{'loss': 0.0119, 'learning_rate': 0.0001453734939759036, 'epoch': 47.8}\n",
            "{'loss': 0.0186, 'learning_rate': 0.00014501204819277106, 'epoch': 47.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0169, 'learning_rate': 0.00014465060240963855, 'epoch': 48.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eedfabb5dd954cb182d7c0d8b6b2ccf8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.005815005395561457, 'eval_wer': 0.41579840077538166, 'eval_runtime': 26.1621, 'eval_samples_per_second': 30.579, 'eval_steps_per_second': 3.822, 'epoch': 48.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0282, 'learning_rate': 0.000144289156626506, 'epoch': 48.1}\n",
            "{'loss': 0.0319, 'learning_rate': 0.0001439277108433735, 'epoch': 48.2}\n",
            "{'loss': 0.0291, 'learning_rate': 0.00014356626506024096, 'epoch': 48.3}\n",
            "{'loss': 0.0279, 'learning_rate': 0.00014320481927710841, 'epoch': 48.4}\n",
            "{'loss': 0.0322, 'learning_rate': 0.0001428433734939759, 'epoch': 48.5}\n",
            "{'loss': 0.0216, 'learning_rate': 0.00014248192771084336, 'epoch': 48.6}\n",
            "{'loss': 0.02, 'learning_rate': 0.00014212048192771082, 'epoch': 48.7}\n",
            "{'loss': 0.0185, 'learning_rate': 0.0001417590361445783, 'epoch': 48.8}\n",
            "{'loss': 0.0199, 'learning_rate': 0.00014139759036144577, 'epoch': 48.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0302, 'learning_rate': 0.00014103614457831323, 'epoch': 49.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d9dbeabdca54c6bbafb61e4d7bc65b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.002875412115827203, 'eval_wer': 0.41046765204749214, 'eval_runtime': 26.6378, 'eval_samples_per_second': 30.033, 'eval_steps_per_second': 3.754, 'epoch': 49.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-4900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0172, 'learning_rate': 0.0001406746987951807, 'epoch': 49.1}\n",
            "{'loss': 0.0224, 'learning_rate': 0.0001403132530120482, 'epoch': 49.2}\n",
            "{'loss': 0.022, 'learning_rate': 0.00013995180722891566, 'epoch': 49.3}\n",
            "{'loss': 0.0192, 'learning_rate': 0.00013959036144578312, 'epoch': 49.4}\n",
            "{'loss': 0.0171, 'learning_rate': 0.0001392289156626506, 'epoch': 49.5}\n",
            "{'loss': 0.0176, 'learning_rate': 0.00013886746987951806, 'epoch': 49.6}\n",
            "{'loss': 0.0367, 'learning_rate': 0.00013850602409638552, 'epoch': 49.7}\n",
            "{'loss': 0.0389, 'learning_rate': 0.000138144578313253, 'epoch': 49.8}\n",
            "{'loss': 0.018, 'learning_rate': 0.00013778313253012047, 'epoch': 49.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0294, 'learning_rate': 0.00013742168674698793, 'epoch': 50.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fa7b73250ab4d81aad368fbe7bc6ac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0035883812233805656, 'eval_wer': 0.4148291737339472, 'eval_runtime': 26.0292, 'eval_samples_per_second': 30.735, 'eval_steps_per_second': 3.842, 'epoch': 50.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0184, 'learning_rate': 0.00013706024096385542, 'epoch': 50.1}\n",
            "{'loss': 0.0193, 'learning_rate': 0.00013669879518072287, 'epoch': 50.2}\n",
            "{'loss': 0.0226, 'learning_rate': 0.00013633734939759036, 'epoch': 50.3}\n",
            "{'loss': 0.0202, 'learning_rate': 0.00013597590361445782, 'epoch': 50.4}\n",
            "{'loss': 0.0208, 'learning_rate': 0.00013561445783132528, 'epoch': 50.5}\n",
            "{'loss': 0.0198, 'learning_rate': 0.00013525301204819277, 'epoch': 50.6}\n",
            "{'loss': 0.0222, 'learning_rate': 0.00013489156626506023, 'epoch': 50.7}\n",
            "{'loss': 0.0251, 'learning_rate': 0.00013453012048192769, 'epoch': 50.8}\n",
            "{'loss': 0.0348, 'learning_rate': 0.00013416867469879517, 'epoch': 50.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0322, 'learning_rate': 0.00013380722891566266, 'epoch': 51.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dddec8a4d9de485faf6e4f9ea4cd377f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0034615451004356146, 'eval_wer': 0.41240610613036105, 'eval_runtime': 26.6649, 'eval_samples_per_second': 30.002, 'eval_steps_per_second': 3.75, 'epoch': 51.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-4900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0192, 'learning_rate': 0.0001334457831325301, 'epoch': 51.1}\n",
            "{'loss': 0.0338, 'learning_rate': 0.00013308433734939758, 'epoch': 51.2}\n",
            "{'loss': 0.0245, 'learning_rate': 0.00013272289156626506, 'epoch': 51.3}\n",
            "{'loss': 0.0207, 'learning_rate': 0.00013236144578313252, 'epoch': 51.4}\n",
            "{'loss': 0.0214, 'learning_rate': 0.00013199999999999998, 'epoch': 51.5}\n",
            "{'loss': 0.0196, 'learning_rate': 0.00013163855421686747, 'epoch': 51.6}\n",
            "{'loss': 0.0231, 'learning_rate': 0.00013127710843373493, 'epoch': 51.7}\n",
            "{'loss': 0.0347, 'learning_rate': 0.0001309156626506024, 'epoch': 51.8}\n",
            "{'loss': 0.0319, 'learning_rate': 0.00013055421686746988, 'epoch': 51.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.022, 'learning_rate': 0.00013019277108433733, 'epoch': 52.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bb6ea2b348e41e19741b1f5b262c3c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0031542195938527584, 'eval_wer': 0.42137145626362976, 'eval_runtime': 26.2484, 'eval_samples_per_second': 30.478, 'eval_steps_per_second': 3.81, 'epoch': 52.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0244, 'learning_rate': 0.00012983132530120482, 'epoch': 52.1}\n",
            "{'loss': 0.0339, 'learning_rate': 0.00012946987951807228, 'epoch': 52.2}\n",
            "{'loss': 0.022, 'learning_rate': 0.00012910843373493974, 'epoch': 52.3}\n",
            "{'loss': 0.0158, 'learning_rate': 0.00012874698795180723, 'epoch': 52.4}\n",
            "{'loss': 0.0176, 'learning_rate': 0.00012838554216867469, 'epoch': 52.5}\n",
            "{'loss': 0.0286, 'learning_rate': 0.00012802409638554215, 'epoch': 52.6}\n",
            "{'loss': 0.0228, 'learning_rate': 0.00012766265060240963, 'epoch': 52.7}\n",
            "{'loss': 0.0235, 'learning_rate': 0.0001273012048192771, 'epoch': 52.8}\n",
            "{'loss': 0.019, 'learning_rate': 0.00012693975903614455, 'epoch': 52.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0235, 'learning_rate': 0.00012657831325301204, 'epoch': 53.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72e3a2d89fae49b6b0c66f18ab036842",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0018601685296744108, 'eval_wer': 0.4252483644293676, 'eval_runtime': 25.7633, 'eval_samples_per_second': 31.052, 'eval_steps_per_second': 3.881, 'epoch': 53.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0264, 'learning_rate': 0.00012621686746987952, 'epoch': 53.1}\n",
            "{'loss': 0.0188, 'learning_rate': 0.00012585542168674698, 'epoch': 53.2}\n",
            "{'loss': 0.0475, 'learning_rate': 0.00012549397590361444, 'epoch': 53.3}\n",
            "{'loss': 0.023, 'learning_rate': 0.00012513253012048193, 'epoch': 53.4}\n",
            "{'loss': 0.0336, 'learning_rate': 0.0001247710843373494, 'epoch': 53.5}\n",
            "{'loss': 0.0207, 'learning_rate': 0.00012440963855421685, 'epoch': 53.6}\n",
            "{'loss': 0.0245, 'learning_rate': 0.00012404819277108434, 'epoch': 53.7}\n",
            "{'loss': 0.0328, 'learning_rate': 0.0001236867469879518, 'epoch': 53.8}\n",
            "{'loss': 0.0228, 'learning_rate': 0.00012332530120481925, 'epoch': 53.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0176, 'learning_rate': 0.00012296385542168674, 'epoch': 54.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03149f32dc42494287b348e643683a5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0020710427779704332, 'eval_wer': 0.4162830142960989, 'eval_runtime': 26.6421, 'eval_samples_per_second': 30.028, 'eval_steps_per_second': 3.753, 'epoch': 54.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.013, 'learning_rate': 0.0001226024096385542, 'epoch': 54.1}\n",
            "{'loss': 0.0213, 'learning_rate': 0.0001222409638554217, 'epoch': 54.2}\n",
            "{'loss': 0.0148, 'learning_rate': 0.00012187951807228915, 'epoch': 54.3}\n",
            "{'loss': 0.0142, 'learning_rate': 0.0001215180722891566, 'epoch': 54.4}\n",
            "{'loss': 0.0164, 'learning_rate': 0.00012115662650602408, 'epoch': 54.5}\n",
            "{'loss': 0.0158, 'learning_rate': 0.00012079518072289157, 'epoch': 54.6}\n",
            "{'loss': 0.0223, 'learning_rate': 0.00012043373493975902, 'epoch': 54.7}\n",
            "{'loss': 0.0207, 'learning_rate': 0.0001200722891566265, 'epoch': 54.8}\n",
            "{'loss': 0.0273, 'learning_rate': 0.00011971084337349397, 'epoch': 54.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0253, 'learning_rate': 0.00011934939759036143, 'epoch': 55.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b34411e016847598aa06f92f5a460a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0023658317513763905, 'eval_wer': 0.41192149260964384, 'eval_runtime': 25.7023, 'eval_samples_per_second': 31.126, 'eval_steps_per_second': 3.891, 'epoch': 55.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0157, 'learning_rate': 0.0001189879518072289, 'epoch': 55.1}\n",
            "{'loss': 0.0161, 'learning_rate': 0.00011862650602409638, 'epoch': 55.2}\n",
            "{'loss': 0.0168, 'learning_rate': 0.00011826506024096384, 'epoch': 55.3}\n",
            "{'loss': 0.0107, 'learning_rate': 0.00011790361445783131, 'epoch': 55.4}\n",
            "{'loss': 0.0282, 'learning_rate': 0.0001175421686746988, 'epoch': 55.5}\n",
            "{'loss': 0.0245, 'learning_rate': 0.00011718072289156624, 'epoch': 55.6}\n",
            "{'loss': 0.0219, 'learning_rate': 0.00011681927710843373, 'epoch': 55.7}\n",
            "{'loss': 0.0213, 'learning_rate': 0.0001164578313253012, 'epoch': 55.8}\n",
            "{'loss': 0.0151, 'learning_rate': 0.00011609638554216866, 'epoch': 55.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0213, 'learning_rate': 0.00011573493975903613, 'epoch': 56.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1df56781412840e09489dd2cf11252f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0033725248649716377, 'eval_wer': 0.4041676762781682, 'eval_runtime': 26.5926, 'eval_samples_per_second': 30.084, 'eval_steps_per_second': 3.76, 'epoch': 56.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0169, 'learning_rate': 0.0001153734939759036, 'epoch': 56.1}\n",
            "{'loss': 0.0138, 'learning_rate': 0.00011501204819277107, 'epoch': 56.2}\n",
            "{'loss': 0.0197, 'learning_rate': 0.00011465060240963854, 'epoch': 56.3}\n",
            "{'loss': 0.0132, 'learning_rate': 0.00011428915662650601, 'epoch': 56.4}\n",
            "{'loss': 0.0267, 'learning_rate': 0.00011392771084337347, 'epoch': 56.5}\n",
            "{'loss': 0.0105, 'learning_rate': 0.00011356626506024096, 'epoch': 56.6}\n",
            "{'loss': 0.0214, 'learning_rate': 0.00011320481927710843, 'epoch': 56.7}\n",
            "{'loss': 0.0198, 'learning_rate': 0.00011284337349397589, 'epoch': 56.8}\n",
            "{'loss': 0.0122, 'learning_rate': 0.00011248192771084336, 'epoch': 56.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0146, 'learning_rate': 0.00011212048192771084, 'epoch': 57.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b47f553decd4b60b9bd34ec32194227",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0032737082801759243, 'eval_wer': 0.4107099588078507, 'eval_runtime': 26.1896, 'eval_samples_per_second': 30.546, 'eval_steps_per_second': 3.818, 'epoch': 57.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0189, 'learning_rate': 0.0001117590361445783, 'epoch': 57.1}\n",
            "{'loss': 0.0269, 'learning_rate': 0.00011139759036144577, 'epoch': 57.2}\n",
            "{'loss': 0.0208, 'learning_rate': 0.00011103614457831324, 'epoch': 57.3}\n",
            "{'loss': 0.0201, 'learning_rate': 0.0001106746987951807, 'epoch': 57.4}\n",
            "{'loss': 0.0209, 'learning_rate': 0.00011031325301204817, 'epoch': 57.5}\n",
            "{'loss': 0.0165, 'learning_rate': 0.00010995180722891566, 'epoch': 57.6}\n",
            "{'loss': 0.0199, 'learning_rate': 0.00010959036144578312, 'epoch': 57.7}\n",
            "{'loss': 0.0278, 'learning_rate': 0.00010922891566265059, 'epoch': 57.8}\n",
            "{'loss': 0.0201, 'learning_rate': 0.00010886746987951807, 'epoch': 57.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0163, 'learning_rate': 0.00010850602409638554, 'epoch': 58.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5bcfbff74fb42c59a0f87887817cf60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.001524664810858667, 'eval_wer': 0.394233099103465, 'eval_runtime': 26.3207, 'eval_samples_per_second': 30.394, 'eval_steps_per_second': 3.799, 'epoch': 58.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0151, 'learning_rate': 0.000108144578313253, 'epoch': 58.1}\n",
            "{'loss': 0.0184, 'learning_rate': 0.00010778313253012047, 'epoch': 58.2}\n",
            "{'loss': 0.0292, 'learning_rate': 0.00010742168674698794, 'epoch': 58.3}\n",
            "{'loss': 0.017, 'learning_rate': 0.0001070602409638554, 'epoch': 58.4}\n",
            "{'loss': 0.0176, 'learning_rate': 0.00010669879518072289, 'epoch': 58.5}\n",
            "{'loss': 0.0187, 'learning_rate': 0.00010633734939759036, 'epoch': 58.6}\n",
            "{'loss': 0.0224, 'learning_rate': 0.00010597590361445782, 'epoch': 58.7}\n",
            "{'loss': 0.0164, 'learning_rate': 0.0001056144578313253, 'epoch': 58.8}\n",
            "{'loss': 0.0177, 'learning_rate': 0.00010525301204819277, 'epoch': 58.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0131, 'learning_rate': 0.00010489156626506023, 'epoch': 59.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86dd3a3867e74d038ac681e7167a2028",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0007316504488699138, 'eval_wer': 0.41579840077538166, 'eval_runtime': 26.7993, 'eval_samples_per_second': 29.852, 'eval_steps_per_second': 3.731, 'epoch': 59.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-5900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0148, 'learning_rate': 0.0001045301204819277, 'epoch': 59.1}\n",
            "{'loss': 0.0144, 'learning_rate': 0.00010416867469879517, 'epoch': 59.2}\n",
            "{'loss': 0.0133, 'learning_rate': 0.00010380722891566263, 'epoch': 59.3}\n",
            "{'loss': 0.0089, 'learning_rate': 0.00010344578313253011, 'epoch': 59.4}\n",
            "{'loss': 0.0173, 'learning_rate': 0.0001030843373493976, 'epoch': 59.5}\n",
            "{'loss': 0.0167, 'learning_rate': 0.00010272289156626505, 'epoch': 59.6}\n",
            "{'loss': 0.0071, 'learning_rate': 0.00010236144578313253, 'epoch': 59.7}\n",
            "{'loss': 0.0258, 'learning_rate': 0.000102, 'epoch': 59.8}\n",
            "{'loss': 0.0066, 'learning_rate': 0.00010163855421686746, 'epoch': 59.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0087, 'learning_rate': 0.00010127710843373493, 'epoch': 60.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91c19225ba704a54b6a952e6a23a0ba3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.000678347481880337, 'eval_wer': 0.4317906469590502, 'eval_runtime': 26.1669, 'eval_samples_per_second': 30.573, 'eval_steps_per_second': 3.822, 'epoch': 60.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0222, 'learning_rate': 0.0001009156626506024, 'epoch': 60.1}\n",
            "{'loss': 0.0128, 'learning_rate': 0.00010055421686746986, 'epoch': 60.2}\n",
            "{'loss': 0.0188, 'learning_rate': 0.00010019277108433734, 'epoch': 60.3}\n",
            "{'loss': 0.0154, 'learning_rate': 9.983132530120482e-05, 'epoch': 60.4}\n",
            "{'loss': 0.0117, 'learning_rate': 9.946987951807227e-05, 'epoch': 60.5}\n",
            "{'loss': 0.017, 'learning_rate': 9.910843373493976e-05, 'epoch': 60.6}\n",
            "{'loss': 0.0086, 'learning_rate': 9.874698795180723e-05, 'epoch': 60.7}\n",
            "{'loss': 0.012, 'learning_rate': 9.838554216867469e-05, 'epoch': 60.8}\n",
            "{'loss': 0.0085, 'learning_rate': 9.802409638554216e-05, 'epoch': 60.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0093, 'learning_rate': 9.766265060240963e-05, 'epoch': 61.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6266984646b24bf2bd8784a266ee4088",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0006771761109121144, 'eval_wer': 0.4247637509086504, 'eval_runtime': 26.1371, 'eval_samples_per_second': 30.608, 'eval_steps_per_second': 3.826, 'epoch': 61.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-5900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0103, 'learning_rate': 9.73012048192771e-05, 'epoch': 61.1}\n",
            "{'loss': 0.0177, 'learning_rate': 9.693975903614457e-05, 'epoch': 61.2}\n",
            "{'loss': 0.0162, 'learning_rate': 9.657831325301204e-05, 'epoch': 61.3}\n",
            "{'loss': 0.0171, 'learning_rate': 9.62168674698795e-05, 'epoch': 61.4}\n",
            "{'loss': 0.0185, 'learning_rate': 9.585542168674699e-05, 'epoch': 61.5}\n",
            "{'loss': 0.0098, 'learning_rate': 9.549397590361446e-05, 'epoch': 61.6}\n",
            "{'loss': 0.0196, 'learning_rate': 9.513253012048192e-05, 'epoch': 61.7}\n",
            "{'loss': 0.0253, 'learning_rate': 9.477108433734939e-05, 'epoch': 61.8}\n",
            "{'loss': 0.0148, 'learning_rate': 9.440963855421686e-05, 'epoch': 61.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0257, 'learning_rate': 9.404819277108432e-05, 'epoch': 62.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac2a4dcaef384e5dba498bdf0555acf9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0009892284870147705, 'eval_wer': 0.4281560455536709, 'eval_runtime': 26.2352, 'eval_samples_per_second': 30.493, 'eval_steps_per_second': 3.812, 'epoch': 62.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0154, 'learning_rate': 9.36867469879518e-05, 'epoch': 62.1}\n",
            "{'loss': 0.0119, 'learning_rate': 9.332530120481927e-05, 'epoch': 62.2}\n",
            "{'loss': 0.025, 'learning_rate': 9.296385542168673e-05, 'epoch': 62.3}\n",
            "{'loss': 0.022, 'learning_rate': 9.26024096385542e-05, 'epoch': 62.4}\n",
            "{'loss': 0.02, 'learning_rate': 9.224096385542169e-05, 'epoch': 62.5}\n",
            "{'loss': 0.0119, 'learning_rate': 9.187951807228915e-05, 'epoch': 62.6}\n",
            "{'loss': 0.0155, 'learning_rate': 9.151807228915662e-05, 'epoch': 62.7}\n",
            "{'loss': 0.007, 'learning_rate': 9.11566265060241e-05, 'epoch': 62.8}\n",
            "{'loss': 0.0163, 'learning_rate': 9.079518072289155e-05, 'epoch': 62.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0188, 'learning_rate': 9.043373493975903e-05, 'epoch': 63.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a6d1eeb1bc6460e9990755a90fb7d49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0006361621781252325, 'eval_wer': 0.3801793070026654, 'eval_runtime': 26.5543, 'eval_samples_per_second': 30.127, 'eval_steps_per_second': 3.766, 'epoch': 63.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0092, 'learning_rate': 9.00722891566265e-05, 'epoch': 63.1}\n",
            "{'loss': 0.013, 'learning_rate': 8.971084337349396e-05, 'epoch': 63.2}\n",
            "{'loss': 0.0132, 'learning_rate': 8.934939759036143e-05, 'epoch': 63.3}\n",
            "{'loss': 0.0111, 'learning_rate': 8.898795180722892e-05, 'epoch': 63.4}\n",
            "{'loss': 0.009, 'learning_rate': 8.862650602409638e-05, 'epoch': 63.5}\n",
            "{'loss': 0.0144, 'learning_rate': 8.826506024096385e-05, 'epoch': 63.6}\n",
            "{'loss': 0.0115, 'learning_rate': 8.790361445783132e-05, 'epoch': 63.7}\n",
            "{'loss': 0.0257, 'learning_rate': 8.754216867469878e-05, 'epoch': 63.8}\n",
            "{'loss': 0.0125, 'learning_rate': 8.718072289156626e-05, 'epoch': 63.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.017, 'learning_rate': 8.681927710843373e-05, 'epoch': 64.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79994e7354a042c0acb0ba765cb43d76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0015596040757372975, 'eval_wer': 0.42355221710685725, 'eval_runtime': 26.1599, 'eval_samples_per_second': 30.581, 'eval_steps_per_second': 3.823, 'epoch': 64.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.011, 'learning_rate': 8.645783132530119e-05, 'epoch': 64.1}\n",
            "{'loss': 0.0114, 'learning_rate': 8.609638554216866e-05, 'epoch': 64.2}\n",
            "{'loss': 0.014, 'learning_rate': 8.573493975903614e-05, 'epoch': 64.3}\n",
            "{'loss': 0.0115, 'learning_rate': 8.53734939759036e-05, 'epoch': 64.4}\n",
            "{'loss': 0.0139, 'learning_rate': 8.501204819277108e-05, 'epoch': 64.5}\n",
            "{'loss': 0.0124, 'learning_rate': 8.465060240963855e-05, 'epoch': 64.6}\n",
            "{'loss': 0.0134, 'learning_rate': 8.428915662650601e-05, 'epoch': 64.7}\n",
            "{'loss': 0.0134, 'learning_rate': 8.392771084337349e-05, 'epoch': 64.8}\n",
            "{'loss': 0.0189, 'learning_rate': 8.356626506024096e-05, 'epoch': 64.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0161, 'learning_rate': 8.320481927710842e-05, 'epoch': 65.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1830cb947d0487899d33090caf630a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0014344558585435152, 'eval_wer': 0.4516598013084565, 'eval_runtime': 25.8962, 'eval_samples_per_second': 30.893, 'eval_steps_per_second': 3.862, 'epoch': 65.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0119, 'learning_rate': 8.284337349397589e-05, 'epoch': 65.1}\n",
            "{'loss': 0.0165, 'learning_rate': 8.248192771084337e-05, 'epoch': 65.2}\n",
            "{'loss': 0.0213, 'learning_rate': 8.212048192771083e-05, 'epoch': 65.3}\n",
            "{'loss': 0.0128, 'learning_rate': 8.17590361445783e-05, 'epoch': 65.4}\n",
            "{'loss': 0.0152, 'learning_rate': 8.139759036144578e-05, 'epoch': 65.5}\n",
            "{'loss': 0.0131, 'learning_rate': 8.103614457831324e-05, 'epoch': 65.6}\n",
            "{'loss': 0.0087, 'learning_rate': 8.067469879518072e-05, 'epoch': 65.7}\n",
            "{'loss': 0.0098, 'learning_rate': 8.031325301204819e-05, 'epoch': 65.8}\n",
            "{'loss': 0.0149, 'learning_rate': 7.995180722891565e-05, 'epoch': 65.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0169, 'learning_rate': 7.959036144578312e-05, 'epoch': 66.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4895877a93c474395faaf8f09084553",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0008327290415763855, 'eval_wer': 0.4393021565301672, 'eval_runtime': 26.1683, 'eval_samples_per_second': 30.571, 'eval_steps_per_second': 3.821, 'epoch': 66.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0179, 'learning_rate': 7.92289156626506e-05, 'epoch': 66.1}\n",
            "{'loss': 0.0133, 'learning_rate': 7.886746987951806e-05, 'epoch': 66.2}\n",
            "{'loss': 0.014, 'learning_rate': 7.850602409638553e-05, 'epoch': 66.3}\n",
            "{'loss': 0.0105, 'learning_rate': 7.814457831325301e-05, 'epoch': 66.4}\n",
            "{'loss': 0.0144, 'learning_rate': 7.778313253012047e-05, 'epoch': 66.5}\n",
            "{'loss': 0.0111, 'learning_rate': 7.742168674698795e-05, 'epoch': 66.6}\n",
            "{'loss': 0.0138, 'learning_rate': 7.706024096385542e-05, 'epoch': 66.7}\n",
            "{'loss': 0.0185, 'learning_rate': 7.669879518072288e-05, 'epoch': 66.8}\n",
            "{'loss': 0.0142, 'learning_rate': 7.633734939759035e-05, 'epoch': 66.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0148, 'learning_rate': 7.597590361445783e-05, 'epoch': 67.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f5507487a714ac584f14af47e1a9b70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0005207034409977496, 'eval_wer': 0.4397867700508844, 'eval_runtime': 26.0048, 'eval_samples_per_second': 30.764, 'eval_steps_per_second': 3.845, 'epoch': 67.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0115, 'learning_rate': 7.561445783132529e-05, 'epoch': 67.1}\n",
            "{'loss': 0.0157, 'learning_rate': 7.525301204819276e-05, 'epoch': 67.2}\n",
            "{'loss': 0.0074, 'learning_rate': 7.489156626506023e-05, 'epoch': 67.3}\n",
            "{'loss': 0.0123, 'learning_rate': 7.45301204819277e-05, 'epoch': 67.4}\n",
            "{'loss': 0.0111, 'learning_rate': 7.416867469879518e-05, 'epoch': 67.5}\n",
            "{'loss': 0.0081, 'learning_rate': 7.380722891566265e-05, 'epoch': 67.6}\n",
            "{'loss': 0.0136, 'learning_rate': 7.344578313253011e-05, 'epoch': 67.7}\n",
            "{'loss': 0.0105, 'learning_rate': 7.308433734939758e-05, 'epoch': 67.8}\n",
            "{'loss': 0.0075, 'learning_rate': 7.272289156626506e-05, 'epoch': 67.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0217, 'learning_rate': 7.236144578313253e-05, 'epoch': 68.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddff03a4c7fa4863aefb42bf76f183dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0006389892078004777, 'eval_wer': 0.4477828931427187, 'eval_runtime': 26.2974, 'eval_samples_per_second': 30.421, 'eval_steps_per_second': 3.803, 'epoch': 68.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0109, 'learning_rate': 7.199999999999999e-05, 'epoch': 68.1}\n",
            "{'loss': 0.0076, 'learning_rate': 7.163855421686746e-05, 'epoch': 68.2}\n",
            "{'loss': 0.0213, 'learning_rate': 7.127710843373493e-05, 'epoch': 68.3}\n",
            "{'loss': 0.0119, 'learning_rate': 7.091566265060241e-05, 'epoch': 68.4}\n",
            "{'loss': 0.0136, 'learning_rate': 7.055421686746988e-05, 'epoch': 68.5}\n",
            "{'loss': 0.0065, 'learning_rate': 7.019277108433734e-05, 'epoch': 68.6}\n",
            "{'loss': 0.0153, 'learning_rate': 6.983132530120481e-05, 'epoch': 68.7}\n",
            "{'loss': 0.0107, 'learning_rate': 6.946987951807229e-05, 'epoch': 68.8}\n",
            "{'loss': 0.0187, 'learning_rate': 6.910843373493976e-05, 'epoch': 68.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0131, 'learning_rate': 6.874698795180722e-05, 'epoch': 69.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdfaf1a1b3c64b559dba38dbe4c2e8f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0008421235252171755, 'eval_wer': 0.4373637024472983, 'eval_runtime': 26.6756, 'eval_samples_per_second': 29.99, 'eval_steps_per_second': 3.749, 'epoch': 69.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-6900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0163, 'learning_rate': 6.838554216867469e-05, 'epoch': 69.1}\n",
            "{'loss': 0.0193, 'learning_rate': 6.802409638554216e-05, 'epoch': 69.2}\n",
            "{'loss': 0.0135, 'learning_rate': 6.766265060240962e-05, 'epoch': 69.3}\n",
            "{'loss': 0.0158, 'learning_rate': 6.730120481927711e-05, 'epoch': 69.4}\n",
            "{'loss': 0.0127, 'learning_rate': 6.693975903614457e-05, 'epoch': 69.5}\n",
            "{'loss': 0.0132, 'learning_rate': 6.657831325301204e-05, 'epoch': 69.6}\n",
            "{'loss': 0.0116, 'learning_rate': 6.621686746987952e-05, 'epoch': 69.7}\n",
            "{'loss': 0.0091, 'learning_rate': 6.585542168674698e-05, 'epoch': 69.8}\n",
            "{'loss': 0.0109, 'learning_rate': 6.549397590361445e-05, 'epoch': 69.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0106, 'learning_rate': 6.513253012048192e-05, 'epoch': 70.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a406d64e35844543a699a539960e3337",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0005796296172775328, 'eval_wer': 0.43857523624909134, 'eval_runtime': 26.2242, 'eval_samples_per_second': 30.506, 'eval_steps_per_second': 3.813, 'epoch': 70.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0086, 'learning_rate': 6.47710843373494e-05, 'epoch': 70.1}\n",
            "{'loss': 0.0101, 'learning_rate': 6.440963855421685e-05, 'epoch': 70.2}\n",
            "{'loss': 0.0111, 'learning_rate': 6.404819277108433e-05, 'epoch': 70.3}\n",
            "{'loss': 0.0138, 'learning_rate': 6.36867469879518e-05, 'epoch': 70.4}\n",
            "{'loss': 0.0063, 'learning_rate': 6.332530120481927e-05, 'epoch': 70.5}\n",
            "{'loss': 0.0158, 'learning_rate': 6.296385542168675e-05, 'epoch': 70.6}\n",
            "{'loss': 0.0097, 'learning_rate': 6.26024096385542e-05, 'epoch': 70.7}\n",
            "{'loss': 0.0059, 'learning_rate': 6.224096385542168e-05, 'epoch': 70.8}\n",
            "{'loss': 0.0106, 'learning_rate': 6.187951807228915e-05, 'epoch': 70.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0057, 'learning_rate': 6.151807228915662e-05, 'epoch': 71.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffcd879baaac415abd749076dacc4c71",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00043114929576404393, 'eval_wer': 0.43833292948873276, 'eval_runtime': 26.3227, 'eval_samples_per_second': 30.392, 'eval_steps_per_second': 3.799, 'epoch': 71.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-6900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0071, 'learning_rate': 6.11566265060241e-05, 'epoch': 71.1}\n",
            "{'loss': 0.0078, 'learning_rate': 6.0795180722891564e-05, 'epoch': 71.2}\n",
            "{'loss': 0.0106, 'learning_rate': 6.043373493975903e-05, 'epoch': 71.3}\n",
            "{'loss': 0.011, 'learning_rate': 6.00722891566265e-05, 'epoch': 71.4}\n",
            "{'loss': 0.0168, 'learning_rate': 5.971084337349397e-05, 'epoch': 71.5}\n",
            "{'loss': 0.0059, 'learning_rate': 5.934939759036144e-05, 'epoch': 71.6}\n",
            "{'loss': 0.005, 'learning_rate': 5.8987951807228915e-05, 'epoch': 71.7}\n",
            "{'loss': 0.0114, 'learning_rate': 5.862650602409638e-05, 'epoch': 71.8}\n",
            "{'loss': 0.0105, 'learning_rate': 5.826506024096385e-05, 'epoch': 71.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0126, 'learning_rate': 5.790361445783132e-05, 'epoch': 72.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "931236792f864ad2939fb57836af6b9f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0005003846599720418, 'eval_wer': 0.44099830385267746, 'eval_runtime': 25.9049, 'eval_samples_per_second': 30.882, 'eval_steps_per_second': 3.86, 'epoch': 72.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0086, 'learning_rate': 5.7542168674698794e-05, 'epoch': 72.1}\n",
            "{'loss': 0.0097, 'learning_rate': 5.718072289156626e-05, 'epoch': 72.2}\n",
            "{'loss': 0.0102, 'learning_rate': 5.681927710843373e-05, 'epoch': 72.3}\n",
            "{'loss': 0.0054, 'learning_rate': 5.64578313253012e-05, 'epoch': 72.4}\n",
            "{'loss': 0.0097, 'learning_rate': 5.6096385542168665e-05, 'epoch': 72.5}\n",
            "{'loss': 0.0058, 'learning_rate': 5.5734939759036145e-05, 'epoch': 72.6}\n",
            "{'loss': 0.0113, 'learning_rate': 5.537349397590361e-05, 'epoch': 72.7}\n",
            "{'loss': 0.0123, 'learning_rate': 5.501204819277108e-05, 'epoch': 72.8}\n",
            "{'loss': 0.0106, 'learning_rate': 5.465060240963855e-05, 'epoch': 72.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0117, 'learning_rate': 5.428915662650602e-05, 'epoch': 73.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f633aee49cf64df1b652ca3d282cc12d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0008489846950396895, 'eval_wer': 0.4417252241337533, 'eval_runtime': 26.2506, 'eval_samples_per_second': 30.475, 'eval_steps_per_second': 3.809, 'epoch': 73.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0135, 'learning_rate': 5.392771084337349e-05, 'epoch': 73.1}\n",
            "{'loss': 0.0061, 'learning_rate': 5.356626506024096e-05, 'epoch': 73.2}\n",
            "{'loss': 0.0085, 'learning_rate': 5.320481927710843e-05, 'epoch': 73.3}\n",
            "{'loss': 0.0088, 'learning_rate': 5.2843373493975895e-05, 'epoch': 73.4}\n",
            "{'loss': 0.0084, 'learning_rate': 5.248192771084337e-05, 'epoch': 73.5}\n",
            "{'loss': 0.009, 'learning_rate': 5.212048192771084e-05, 'epoch': 73.6}\n",
            "{'loss': 0.009, 'learning_rate': 5.175903614457831e-05, 'epoch': 73.7}\n",
            "{'loss': 0.0065, 'learning_rate': 5.139759036144578e-05, 'epoch': 73.8}\n",
            "{'loss': 0.0092, 'learning_rate': 5.103614457831325e-05, 'epoch': 73.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0062, 'learning_rate': 5.067469879518071e-05, 'epoch': 74.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e925683e7f594c53a5c11ba441b4f25d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0005724790971726179, 'eval_wer': 0.43494063484371215, 'eval_runtime': 26.0998, 'eval_samples_per_second': 30.652, 'eval_steps_per_second': 3.831, 'epoch': 74.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0054, 'learning_rate': 5.031325301204819e-05, 'epoch': 74.1}\n",
            "{'loss': 0.009, 'learning_rate': 4.995180722891566e-05, 'epoch': 74.2}\n",
            "{'loss': 0.0086, 'learning_rate': 4.9590361445783125e-05, 'epoch': 74.3}\n",
            "{'loss': 0.0133, 'learning_rate': 4.92289156626506e-05, 'epoch': 74.4}\n",
            "{'loss': 0.009, 'learning_rate': 4.8867469879518065e-05, 'epoch': 74.5}\n",
            "{'loss': 0.0077, 'learning_rate': 4.850602409638554e-05, 'epoch': 74.6}\n",
            "{'loss': 0.01, 'learning_rate': 4.814457831325301e-05, 'epoch': 74.7}\n",
            "{'loss': 0.0043, 'learning_rate': 4.778313253012048e-05, 'epoch': 74.8}\n",
            "{'loss': 0.0062, 'learning_rate': 4.742168674698794e-05, 'epoch': 74.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0104, 'learning_rate': 4.706024096385542e-05, 'epoch': 75.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b7a7aa52a174d58b038a453e912e5fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0006132724811322987, 'eval_wer': 0.4310637266779743, 'eval_runtime': 26.2703, 'eval_samples_per_second': 30.453, 'eval_steps_per_second': 3.807, 'epoch': 75.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0089, 'learning_rate': 4.669879518072289e-05, 'epoch': 75.1}\n",
            "{'loss': 0.0116, 'learning_rate': 4.6337349397590355e-05, 'epoch': 75.2}\n",
            "{'loss': 0.0062, 'learning_rate': 4.597590361445783e-05, 'epoch': 75.3}\n",
            "{'loss': 0.0103, 'learning_rate': 4.5614457831325295e-05, 'epoch': 75.4}\n",
            "{'loss': 0.0081, 'learning_rate': 4.525301204819276e-05, 'epoch': 75.5}\n",
            "{'loss': 0.0061, 'learning_rate': 4.489156626506024e-05, 'epoch': 75.6}\n",
            "{'loss': 0.0102, 'learning_rate': 4.453012048192771e-05, 'epoch': 75.7}\n",
            "{'loss': 0.0055, 'learning_rate': 4.416867469879517e-05, 'epoch': 75.8}\n",
            "{'loss': 0.0063, 'learning_rate': 4.3807228915662646e-05, 'epoch': 75.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0064, 'learning_rate': 4.344578313253011e-05, 'epoch': 76.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ce0d0e111f5433aba2eec6aa6f6562e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00048217541188932955, 'eval_wer': 0.41943300218076085, 'eval_runtime': 26.6721, 'eval_samples_per_second': 29.994, 'eval_steps_per_second': 3.749, 'epoch': 76.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0066, 'learning_rate': 4.308433734939759e-05, 'epoch': 76.1}\n",
            "{'loss': 0.0095, 'learning_rate': 4.272289156626506e-05, 'epoch': 76.2}\n",
            "{'loss': 0.0038, 'learning_rate': 4.2361445783132525e-05, 'epoch': 76.3}\n",
            "{'loss': 0.0104, 'learning_rate': 4.2e-05, 'epoch': 76.4}\n",
            "{'loss': 0.0064, 'learning_rate': 4.163855421686747e-05, 'epoch': 76.5}\n",
            "{'loss': 0.0057, 'learning_rate': 4.127710843373494e-05, 'epoch': 76.6}\n",
            "{'loss': 0.0083, 'learning_rate': 4.091566265060241e-05, 'epoch': 76.7}\n",
            "{'loss': 0.0092, 'learning_rate': 4.0554216867469876e-05, 'epoch': 76.8}\n",
            "{'loss': 0.009, 'learning_rate': 4.019277108433734e-05, 'epoch': 76.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0051, 'learning_rate': 3.983132530120482e-05, 'epoch': 77.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ec92a93f6c44f129985fbb53317e8a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0003311073814984411, 'eval_wer': 0.4141022534528713, 'eval_runtime': 26.922, 'eval_samples_per_second': 29.715, 'eval_steps_per_second': 3.714, 'epoch': 77.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0117, 'learning_rate': 3.946987951807229e-05, 'epoch': 77.1}\n",
            "{'loss': 0.0101, 'learning_rate': 3.9108433734939755e-05, 'epoch': 77.2}\n",
            "{'loss': 0.0101, 'learning_rate': 3.874698795180723e-05, 'epoch': 77.3}\n",
            "{'loss': 0.0111, 'learning_rate': 3.8385542168674694e-05, 'epoch': 77.4}\n",
            "{'loss': 0.0116, 'learning_rate': 3.802409638554216e-05, 'epoch': 77.5}\n",
            "{'loss': 0.0073, 'learning_rate': 3.766265060240964e-05, 'epoch': 77.6}\n",
            "{'loss': 0.0073, 'learning_rate': 3.7301204819277106e-05, 'epoch': 77.7}\n",
            "{'loss': 0.008, 'learning_rate': 3.693975903614458e-05, 'epoch': 77.8}\n",
            "{'loss': 0.007, 'learning_rate': 3.6578313253012045e-05, 'epoch': 77.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0215, 'learning_rate': 3.621686746987952e-05, 'epoch': 78.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c250715de38c4917a4fb6101ae52dd09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0002517133834771812, 'eval_wer': 0.41143687908892657, 'eval_runtime': 26.6223, 'eval_samples_per_second': 30.05, 'eval_steps_per_second': 3.756, 'epoch': 78.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0048, 'learning_rate': 3.5855421686746985e-05, 'epoch': 78.1}\n",
            "{'loss': 0.0057, 'learning_rate': 3.549397590361445e-05, 'epoch': 78.2}\n",
            "{'loss': 0.0089, 'learning_rate': 3.5132530120481924e-05, 'epoch': 78.3}\n",
            "{'loss': 0.0132, 'learning_rate': 3.47710843373494e-05, 'epoch': 78.4}\n",
            "{'loss': 0.0122, 'learning_rate': 3.440963855421686e-05, 'epoch': 78.5}\n",
            "{'loss': 0.0043, 'learning_rate': 3.4048192771084336e-05, 'epoch': 78.6}\n",
            "{'loss': 0.0034, 'learning_rate': 3.36867469879518e-05, 'epoch': 78.7}\n",
            "{'loss': 0.0032, 'learning_rate': 3.3325301204819275e-05, 'epoch': 78.8}\n",
            "{'loss': 0.0071, 'learning_rate': 3.296385542168674e-05, 'epoch': 78.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0046, 'learning_rate': 3.2602409638554215e-05, 'epoch': 79.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40a1b6f4965f4ec6bd90e9ba82ab58a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00023970559414010495, 'eval_wer': 0.4116791858492852, 'eval_runtime': 26.143, 'eval_samples_per_second': 30.601, 'eval_steps_per_second': 3.825, 'epoch': 79.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-7900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0244, 'learning_rate': 3.224096385542168e-05, 'epoch': 79.1}\n",
            "{'loss': 0.0085, 'learning_rate': 3.1879518072289154e-05, 'epoch': 79.2}\n",
            "{'loss': 0.0067, 'learning_rate': 3.151807228915663e-05, 'epoch': 79.3}\n",
            "{'loss': 0.0088, 'learning_rate': 3.115662650602409e-05, 'epoch': 79.4}\n",
            "{'loss': 0.0064, 'learning_rate': 3.0795180722891566e-05, 'epoch': 79.5}\n",
            "{'loss': 0.0088, 'learning_rate': 3.0433734939759036e-05, 'epoch': 79.6}\n",
            "{'loss': 0.0058, 'learning_rate': 3.0072289156626502e-05, 'epoch': 79.7}\n",
            "{'loss': 0.008, 'learning_rate': 2.9710843373493972e-05, 'epoch': 79.8}\n",
            "{'loss': 0.0046, 'learning_rate': 2.9349397590361445e-05, 'epoch': 79.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0112, 'learning_rate': 2.8987951807228914e-05, 'epoch': 80.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfbbf2816d374a69a2a33e459494d41d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00034291206975467503, 'eval_wer': 0.41579840077538166, 'eval_runtime': 26.3584, 'eval_samples_per_second': 30.351, 'eval_steps_per_second': 3.794, 'epoch': 80.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0064, 'learning_rate': 2.862650602409638e-05, 'epoch': 80.1}\n",
            "{'loss': 0.0068, 'learning_rate': 2.8265060240963854e-05, 'epoch': 80.2}\n",
            "{'loss': 0.0094, 'learning_rate': 2.7903614457831323e-05, 'epoch': 80.3}\n",
            "{'loss': 0.0102, 'learning_rate': 2.7542168674698793e-05, 'epoch': 80.4}\n",
            "{'loss': 0.0075, 'learning_rate': 2.7180722891566262e-05, 'epoch': 80.5}\n",
            "{'loss': 0.0054, 'learning_rate': 2.6819277108433735e-05, 'epoch': 80.6}\n",
            "{'loss': 0.0063, 'learning_rate': 2.6457831325301202e-05, 'epoch': 80.7}\n",
            "{'loss': 0.0091, 'learning_rate': 2.609638554216867e-05, 'epoch': 80.8}\n",
            "{'loss': 0.0059, 'learning_rate': 2.5734939759036144e-05, 'epoch': 80.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0074, 'learning_rate': 2.537349397590361e-05, 'epoch': 81.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0a2dc7cbd924c83adf358dba249e7fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00032181746792048216, 'eval_wer': 0.4141022534528713, 'eval_runtime': 26.5339, 'eval_samples_per_second': 30.15, 'eval_steps_per_second': 3.769, 'epoch': 81.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-7900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0064, 'learning_rate': 2.5012048192771084e-05, 'epoch': 81.1}\n",
            "{'loss': 0.0063, 'learning_rate': 2.4650602409638553e-05, 'epoch': 81.2}\n",
            "{'loss': 0.0018, 'learning_rate': 2.428915662650602e-05, 'epoch': 81.3}\n",
            "{'loss': 0.0054, 'learning_rate': 2.3927710843373493e-05, 'epoch': 81.4}\n",
            "{'loss': 0.016, 'learning_rate': 2.3566265060240962e-05, 'epoch': 81.5}\n",
            "{'loss': 0.0076, 'learning_rate': 2.3204819277108432e-05, 'epoch': 81.6}\n",
            "{'loss': 0.0058, 'learning_rate': 2.28433734939759e-05, 'epoch': 81.7}\n",
            "{'loss': 0.0102, 'learning_rate': 2.248192771084337e-05, 'epoch': 81.8}\n",
            "{'loss': 0.0061, 'learning_rate': 2.212048192771084e-05, 'epoch': 81.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0044, 'learning_rate': 2.175903614457831e-05, 'epoch': 82.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f979630d64dd4282979e342db3bf7559",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00030345062259584665, 'eval_wer': 0.4170099345771747, 'eval_runtime': 26.6365, 'eval_samples_per_second': 30.034, 'eval_steps_per_second': 3.754, 'epoch': 82.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-8000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0105, 'learning_rate': 2.1397590361445783e-05, 'epoch': 82.1}\n",
            "{'loss': 0.0104, 'learning_rate': 2.103614457831325e-05, 'epoch': 82.2}\n",
            "{'loss': 0.0048, 'learning_rate': 2.067469879518072e-05, 'epoch': 82.3}\n",
            "{'loss': 0.0118, 'learning_rate': 2.0313253012048192e-05, 'epoch': 82.4}\n",
            "{'loss': 0.0085, 'learning_rate': 1.995180722891566e-05, 'epoch': 82.5}\n",
            "{'loss': 0.0057, 'learning_rate': 1.959036144578313e-05, 'epoch': 82.6}\n",
            "{'loss': 0.0066, 'learning_rate': 1.92289156626506e-05, 'epoch': 82.7}\n",
            "{'loss': 0.0085, 'learning_rate': 1.8867469879518074e-05, 'epoch': 82.8}\n",
            "{'loss': 0.0066, 'learning_rate': 1.850602409638554e-05, 'epoch': 82.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0035, 'learning_rate': 1.814457831325301e-05, 'epoch': 83.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc98daff1ea14226acc1411f2ae09b12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00026358163449913263, 'eval_wer': 0.4150714804943058, 'eval_runtime': 26.5324, 'eval_samples_per_second': 30.152, 'eval_steps_per_second': 3.769, 'epoch': 83.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-8100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0074, 'learning_rate': 1.778313253012048e-05, 'epoch': 83.1}\n",
            "{'loss': 0.0059, 'learning_rate': 1.742168674698795e-05, 'epoch': 83.2}\n",
            "{'loss': 0.0057, 'learning_rate': 1.7060240963855422e-05, 'epoch': 83.3}\n",
            "{'loss': 0.0108, 'learning_rate': 1.669879518072289e-05, 'epoch': 83.4}\n",
            "{'loss': 0.0113, 'learning_rate': 1.633734939759036e-05, 'epoch': 83.5}\n",
            "{'loss': 0.0099, 'learning_rate': 1.597590361445783e-05, 'epoch': 83.6}\n",
            "{'loss': 0.006, 'learning_rate': 1.56144578313253e-05, 'epoch': 83.7}\n",
            "{'loss': 0.0092, 'learning_rate': 1.525301204819277e-05, 'epoch': 83.8}\n",
            "{'loss': 0.0088, 'learning_rate': 1.489156626506024e-05, 'epoch': 83.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0032, 'learning_rate': 1.453012048192771e-05, 'epoch': 84.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56f68fbe3173495c9bddd220eeca4d48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0002217352739535272, 'eval_wer': 0.42597528471044344, 'eval_runtime': 26.474, 'eval_samples_per_second': 30.218, 'eval_steps_per_second': 3.777, 'epoch': 84.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-8200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0053, 'learning_rate': 1.4168674698795181e-05, 'epoch': 84.1}\n",
            "{'loss': 0.0057, 'learning_rate': 1.3807228915662649e-05, 'epoch': 84.2}\n",
            "{'loss': 0.0053, 'learning_rate': 1.3445783132530118e-05, 'epoch': 84.3}\n",
            "{'loss': 0.0047, 'learning_rate': 1.308433734939759e-05, 'epoch': 84.4}\n",
            "{'loss': 0.0081, 'learning_rate': 1.272289156626506e-05, 'epoch': 84.5}\n",
            "{'loss': 0.0055, 'learning_rate': 1.236144578313253e-05, 'epoch': 84.6}\n",
            "{'loss': 0.0092, 'learning_rate': 1.1999999999999999e-05, 'epoch': 84.7}\n",
            "{'loss': 0.0057, 'learning_rate': 1.1638554216867468e-05, 'epoch': 84.8}\n",
            "{'loss': 0.0057, 'learning_rate': 1.127710843373494e-05, 'epoch': 84.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0087, 'learning_rate': 1.091566265060241e-05, 'epoch': 85.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a05ff4060fa546518129387c2acb6260",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00016839729505591094, 'eval_wer': 0.42452144414829174, 'eval_runtime': 26.104, 'eval_samples_per_second': 30.647, 'eval_steps_per_second': 3.831, 'epoch': 85.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-8300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0082, 'learning_rate': 1.0554216867469877e-05, 'epoch': 85.1}\n",
            "{'loss': 0.007, 'learning_rate': 1.0192771084337348e-05, 'epoch': 85.2}\n",
            "{'loss': 0.0044, 'learning_rate': 9.831325301204818e-06, 'epoch': 85.3}\n",
            "{'loss': 0.0083, 'learning_rate': 9.469879518072288e-06, 'epoch': 85.4}\n",
            "{'loss': 0.0068, 'learning_rate': 9.108433734939759e-06, 'epoch': 85.5}\n",
            "{'loss': 0.0089, 'learning_rate': 8.746987951807229e-06, 'epoch': 85.6}\n",
            "{'loss': 0.0058, 'learning_rate': 8.385542168674698e-06, 'epoch': 85.7}\n",
            "{'loss': 0.0076, 'learning_rate': 8.024096385542168e-06, 'epoch': 85.8}\n",
            "{'loss': 0.008, 'learning_rate': 7.662650602409638e-06, 'epoch': 85.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0057, 'learning_rate': 7.301204819277108e-06, 'epoch': 86.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bc2b5c914dd4545aa4423bed65f0c67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0001521749363746494, 'eval_wer': 0.4257329779500848, 'eval_runtime': 26.1882, 'eval_samples_per_second': 30.548, 'eval_steps_per_second': 3.819, 'epoch': 86.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-8400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0077, 'learning_rate': 6.939759036144578e-06, 'epoch': 86.1}\n",
            "{'loss': 0.0042, 'learning_rate': 6.578313253012048e-06, 'epoch': 86.2}\n",
            "{'loss': 0.0068, 'learning_rate': 6.216867469879517e-06, 'epoch': 86.3}\n",
            "{'loss': 0.0069, 'learning_rate': 5.855421686746987e-06, 'epoch': 86.4}\n",
            "{'loss': 0.0032, 'learning_rate': 5.493975903614458e-06, 'epoch': 86.5}\n",
            "{'loss': 0.0059, 'learning_rate': 5.1325301204819266e-06, 'epoch': 86.6}\n",
            "{'loss': 0.0075, 'learning_rate': 4.771084337349397e-06, 'epoch': 86.7}\n",
            "{'loss': 0.0068, 'learning_rate': 4.4096385542168675e-06, 'epoch': 86.8}\n",
            "{'loss': 0.0033, 'learning_rate': 4.048192771084337e-06, 'epoch': 86.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0053, 'learning_rate': 3.686746987951807e-06, 'epoch': 87.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ba28dac93b24fcaa34cb41d5abad655",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00015603793144691736, 'eval_wer': 0.4276714320329537, 'eval_runtime': 26.6274, 'eval_samples_per_second': 30.044, 'eval_steps_per_second': 3.756, 'epoch': 87.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-8500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.004, 'learning_rate': 3.325301204819277e-06, 'epoch': 87.1}\n",
            "{'loss': 0.0099, 'learning_rate': 2.9638554216867464e-06, 'epoch': 87.2}\n",
            "{'loss': 0.0062, 'learning_rate': 2.602409638554217e-06, 'epoch': 87.3}\n",
            "{'loss': 0.0053, 'learning_rate': 2.2409638554216865e-06, 'epoch': 87.4}\n",
            "{'loss': 0.0073, 'learning_rate': 1.8795180722891564e-06, 'epoch': 87.5}\n",
            "{'loss': 0.0027, 'learning_rate': 1.5180722891566262e-06, 'epoch': 87.6}\n",
            "{'loss': 0.0035, 'learning_rate': 1.1566265060240963e-06, 'epoch': 87.7}\n",
            "{'loss': 0.0039, 'learning_rate': 7.951807228915662e-07, 'epoch': 87.8}\n",
            "{'loss': 0.0052, 'learning_rate': 4.337349397590361e-07, 'epoch': 87.9}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 800\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0062, 'learning_rate': 7.228915662650602e-08, 'epoch': 88.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e34552ab13e4ce5b08743da5757954e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00014746309898328036, 'eval_wer': 0.4257329779500848, 'eval_runtime': 26.6356, 'eval_samples_per_second': 30.035, 'eval_steps_per_second': 3.754, 'epoch': 88.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-8800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-8600] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 13747.81, 'train_samples_per_second': 20.483, 'train_steps_per_second': 0.64, 'train_loss': 0.05770374050787227, 'epoch': 88.0}\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxpIR_X8w3Pq"
      },
      "source": [
        "CTC 손실을 사용하여 더 큰 데이터 세트에서 더 큰 모델을 미세 조정하려면 [여기서](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) 공식 음성 인식 예를 살펴봐야 한다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "00.XLSR - Wav2Vec2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('STT')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "19631f30805cf65d5465564d75f0fe7c05dee5c1f7be198222dbe754da644e52"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
