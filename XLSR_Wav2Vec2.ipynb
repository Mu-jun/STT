{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-f49502427ad3afb2\n",
            "Reusing dataset csv (C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-f49502427ad3afb2\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
          ]
        }
      ],
      "source": [
        "all_data = load_dataset('csv',data_files='./order_speech_ko1000.csv',split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G34Hj6BgnK3L"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a5818ef00234d50bdf0cf8f9a958980",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "remove_spectial_char_data = all_data.map(remove_special_characters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLCZvETJuFaO"
      },
      "source": [
        "## Create Wav2Vec2CTCTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YJUUl9lnryU-"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\",\n",
        "                                 unk_token=\"[UNK]\",\n",
        "                                 pad_token=\"[PAD]\",\n",
        "                                 word_delimiter_token=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCtrdRsuuDix"
      },
      "source": [
        "## Create XLSR-Wav2Vec2 Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O0vdrkhKr8D1"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1,\n",
        "                                             sampling_rate=16000,\n",
        "                                             padding_value=0.0,\n",
        "                                             do_normalize=True,\n",
        "                                             return_attention_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fY_Qm1dpsE78"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor,\n",
        "                              tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KTUahP1PsL3g"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9tMSA3xKsVma"
      },
      "outputs": [],
      "source": [
        "# processor.save_pretrained(\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## add audio array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "sr = 16000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def voice_sep(sig):\n",
        "    sig = np.array(sig).flatten()\n",
        "    S_full, phase = librosa.magphase(librosa.stft(sig))\n",
        "    S_filter = librosa.decompose.nn_filter(S_full,\n",
        "                                       aggregate=np.median,\n",
        "                                       metric='cosine',\n",
        "                                       width=int(librosa.time_to_frames(2, sr=sr)))\n",
        "    S_filter = np.minimum(S_full, S_filter)\n",
        "    margin_v = 2\n",
        "    power = 2\n",
        "    mask_v = librosa.util.softmask(S_full - S_filter,\n",
        "                               margin_v * S_filter,\n",
        "                               power=power)\n",
        "    S_foreground = mask_v * S_full\n",
        "    y_foreground = librosa.istft(S_foreground * phase)\n",
        "    return y_foreground\n",
        "\n",
        "def load_audio(batch):\n",
        "    batch['array'],_ = librosa.load('./dataset/audio/'+batch['filename'],sr=16000)\n",
        "    # batch['array'] = voice_sep(batch['array'])\n",
        "    \n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7643bf98948b46dd873ad7fb43476137",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "audio_data = remove_spectial_char_data.map(load_audio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(audio_data[0]['array'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data augmetation(RIR applied)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "rir_raw,_ = librosa.load('./outputs/rir_data.wav',sr)\n",
        "\n",
        "def rir_applied(batch):\n",
        "    rir = torch.from_numpy( rir_raw[int(sr * 2.44) : int(sr * 2.7)].reshape(1,-1) )\n",
        "    rir = rir / torch.norm(rir, p=2)\n",
        "    rir = torch.flip(rir,[1])\n",
        "\n",
        "    speech = torch.from_numpy(np.array(batch['array'],dtype=np.float32).reshape(1,-1))\n",
        "\n",
        "    speech_ = torch.nn.functional.pad(speech, (rir.shape[1] - 1, 0))\n",
        "    # print(speech.dtype)\n",
        "    # print(speech_.dtype)\n",
        "    # print(rir.dtype)\n",
        "    augmented = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n",
        "    batch['array'] = augmented.reshape(-1)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8fc9543846e4f068748c35bbe071b52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "rir_applied_audio_data = audio_data.map(rir_applied)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZiv4zqyt2cY"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "AscOxzgUsb50"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "\n",
        "    # batched output is \"un-batched\"\n",
        "    batch[\"input_values\"] = processor(batch[\"array\"], sampling_rate=16000).input_values[0]\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92caf84219434a4890df7aefc4b249d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "order_voice = rir_applied_audio_data.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=remove_spectial_char_data.column_names,\n",
        "    # num_proc=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['array', 'input_values', 'labels'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "order_voice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StCtCQxDtuY3"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUTqiRY-tpXZ"
      },
      "source": [
        "## Set-up Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "nTlewiOzsyqR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.cuda.HalfTensor]]]) -> Dict[str, torch.cuda.HalfTensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "WmfIPs18tGQb"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vJjfNy9-tJM6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric, Audio\n",
        "\n",
        "wer_metric = load_metric(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "03mNNUfNthSZ"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vgRoeQHvTqk"
      },
      "source": [
        "## Import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Iyv4qaclue6n"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_hid.bias', 'quantizer.weight_proj.weight', 'quantizer.codevectors', 'project_hid.weight', 'project_q.bias', 'project_q.weight', 'quantizer.weight_proj.bias']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\", \n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaYmsSutuv26"
      },
      "source": [
        "XLSR-Wav2Vec2의 첫 번째 구성 요소는 원시 음성 신호에서 음향적으로 의미가 있지만 문맥적으로 독립적인 기능을 추출하는 데 사용되는 CNN 계층 스택으로 구성됩니다.  \n",
        "모델의 이 부분은 사전 교육 중에 이미 충분히 훈련되었으며 논문에 명시된 바와 같이 더 이상 미세 조정할 필요가 없습니다. 따라서 특징 추출 부분의 모든 파라미터에 대해 require_grad를 False로 설정할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5VX8z6PpuhqG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1677: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "model.freeze_feature_extractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAfoHUk9vGxd"
      },
      "source": [
        "메모리를 절약하기 위해 그라데이션 체크포인팅을 활성화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "wZm6in_2vAIN"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-aJNQplvQgn"
      },
      "source": [
        "## TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "-LuuwoeMvOnV"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ko-demo\",\n",
        "  output_dir=\"./wav2vec2-large-xlsr-ko-demo\",\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=16,\n",
        "  gradient_accumulation_steps=2,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=80,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=3e-4,\n",
        "  warmup_steps=500,\n",
        "  save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6vhFC0BdvcOu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "ds_size = len(order_voice)\n",
        "train_size = int(ds_size*0.8)\n",
        "val_size = ds_size - train_size\n",
        "train_ds, val_ds = random_split(order_voice,[train_size,val_size])\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGNj1NSvsFk"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "a-rJ8fokvjJb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 800\n",
            "  Num Epochs = 80\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2000\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c7b0471bde6430fa5715b226c900bd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 52.0827, 'learning_rate': 4.2e-06, 'epoch': 0.4}\n",
            "{'loss': 50.2976, 'learning_rate': 1.02e-05, 'epoch': 0.8}\n",
            "{'loss': 50.924, 'learning_rate': 1.6199999999999997e-05, 'epoch': 1.2}\n",
            "{'loss': 49.8284, 'learning_rate': 2.2199999999999998e-05, 'epoch': 1.6}\n",
            "{'loss': 48.479, 'learning_rate': 2.7599999999999997e-05, 'epoch': 2.0}\n",
            "{'loss': 45.0288, 'learning_rate': 3.36e-05, 'epoch': 2.4}\n",
            "{'loss': 38.4044, 'learning_rate': 3.96e-05, 'epoch': 2.8}\n",
            "{'loss': 29.8678, 'learning_rate': 4.56e-05, 'epoch': 3.2}\n",
            "{'loss': 25.4932, 'learning_rate': 5.1599999999999994e-05, 'epoch': 3.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 22.0674, 'learning_rate': 5.76e-05, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6624fd5fee534d9780de108488788537",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 17.951250076293945, 'eval_wer': 1.0, 'eval_runtime': 10.9605, 'eval_samples_per_second': 18.247, 'eval_steps_per_second': 2.281, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 18.2406, 'learning_rate': 6.359999999999999e-05, 'epoch': 4.4}\n",
            "{'loss': 16.2973, 'learning_rate': 6.96e-05, 'epoch': 4.8}\n",
            "{'loss': 13.5595, 'learning_rate': 7.56e-05, 'epoch': 5.2}\n",
            "{'loss': 12.0247, 'learning_rate': 8.16e-05, 'epoch': 5.6}\n",
            "{'loss': 9.8252, 'learning_rate': 8.759999999999999e-05, 'epoch': 6.0}\n",
            "{'loss': 8.1916, 'learning_rate': 9.36e-05, 'epoch': 6.4}\n",
            "{'loss': 6.8414, 'learning_rate': 9.96e-05, 'epoch': 6.8}\n",
            "{'loss': 6.0061, 'learning_rate': 0.00010559999999999998, 'epoch': 7.2}\n",
            "{'loss': 5.2389, 'learning_rate': 0.00011159999999999999, 'epoch': 7.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.8191, 'learning_rate': 0.0001176, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2abc2dbb886c43f6a0d97c1b5a0ba1e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.6552910804748535, 'eval_wer': 1.0, 'eval_runtime': 10.3355, 'eval_samples_per_second': 19.351, 'eval_steps_per_second': 2.419, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.5806, 'learning_rate': 0.0001236, 'epoch': 8.4}\n",
            "{'loss': 4.5207, 'learning_rate': 0.00012959999999999998, 'epoch': 8.8}\n",
            "{'loss': 4.477, 'learning_rate': 0.0001356, 'epoch': 9.2}\n",
            "{'loss': 4.505, 'learning_rate': 0.00014159999999999997, 'epoch': 9.6}\n",
            "{'loss': 4.47, 'learning_rate': 0.00014759999999999998, 'epoch': 10.0}\n",
            "{'loss': 4.4681, 'learning_rate': 0.0001536, 'epoch': 10.4}\n",
            "{'loss': 4.4772, 'learning_rate': 0.0001596, 'epoch': 10.8}\n",
            "{'loss': 4.4626, 'learning_rate': 0.0001656, 'epoch': 11.2}\n",
            "{'loss': 4.4727, 'learning_rate': 0.00017159999999999997, 'epoch': 11.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.4439, 'learning_rate': 0.00017759999999999998, 'epoch': 12.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73e2c033090042aa823e46d73bb38b93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.5156989097595215, 'eval_wer': 1.0, 'eval_runtime': 10.5294, 'eval_samples_per_second': 18.995, 'eval_steps_per_second': 2.374, 'epoch': 12.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.4374, 'learning_rate': 0.0001836, 'epoch': 12.4}\n",
            "{'loss': 4.4601, 'learning_rate': 0.00018959999999999997, 'epoch': 12.8}\n",
            "{'loss': 4.4428, 'learning_rate': 0.00019559999999999998, 'epoch': 13.2}\n",
            "{'loss': 4.4522, 'learning_rate': 0.0002016, 'epoch': 13.6}\n",
            "{'loss': 4.3677, 'learning_rate': 0.00020759999999999998, 'epoch': 14.0}\n",
            "{'loss': 4.3599, 'learning_rate': 0.00021359999999999996, 'epoch': 14.4}\n",
            "{'loss': 4.3731, 'learning_rate': 0.00021959999999999997, 'epoch': 14.8}\n",
            "{'loss': 4.3606, 'learning_rate': 0.00022559999999999998, 'epoch': 15.2}\n",
            "{'loss': 4.3169, 'learning_rate': 0.0002316, 'epoch': 15.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.2379, 'learning_rate': 0.0002376, 'epoch': 16.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2acc90f308bc475aa0cfd834fb04e0fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.270021915435791, 'eval_wer': 1.0, 'eval_runtime': 10.7152, 'eval_samples_per_second': 18.665, 'eval_steps_per_second': 2.333, 'epoch': 16.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.2566, 'learning_rate': 0.00024359999999999999, 'epoch': 16.4}\n",
            "{'loss': 4.2392, 'learning_rate': 0.00024959999999999994, 'epoch': 16.8}\n",
            "{'loss': 4.1378, 'learning_rate': 0.0002556, 'epoch': 17.2}\n",
            "{'loss': 4.1459, 'learning_rate': 0.00026159999999999996, 'epoch': 17.6}\n",
            "{'loss': 4.1424, 'learning_rate': 0.0002676, 'epoch': 18.0}\n",
            "{'loss': 4.1561, 'learning_rate': 0.0002736, 'epoch': 18.4}\n",
            "{'loss': 4.0748, 'learning_rate': 0.00027959999999999997, 'epoch': 18.8}\n",
            "{'loss': 4.08, 'learning_rate': 0.00028559999999999995, 'epoch': 19.2}\n",
            "{'loss': 4.0741, 'learning_rate': 0.0002916, 'epoch': 19.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.0328, 'learning_rate': 0.00029759999999999997, 'epoch': 20.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1712325bfa749319a7f097d1df48a00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.111047267913818, 'eval_wer': 1.0, 'eval_runtime': 10.2961, 'eval_samples_per_second': 19.425, 'eval_steps_per_second': 2.428, 'epoch': 20.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.0249, 'learning_rate': 0.0002988, 'epoch': 20.4}\n",
            "{'loss': 4.0747, 'learning_rate': 0.00029679999999999995, 'epoch': 20.8}\n",
            "{'loss': 4.0031, 'learning_rate': 0.00029479999999999996, 'epoch': 21.2}\n",
            "{'loss': 3.9964, 'learning_rate': 0.00029279999999999996, 'epoch': 21.6}\n",
            "{'loss': 3.9571, 'learning_rate': 0.00029079999999999997, 'epoch': 22.0}\n",
            "{'loss': 3.9411, 'learning_rate': 0.00028879999999999997, 'epoch': 22.4}\n",
            "{'loss': 3.9449, 'learning_rate': 0.0002868, 'epoch': 22.8}\n",
            "{'loss': 3.9588, 'learning_rate': 0.0002848, 'epoch': 23.2}\n",
            "{'loss': 3.891, 'learning_rate': 0.0002828, 'epoch': 23.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.8769, 'learning_rate': 0.0002808, 'epoch': 24.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c9867cc23894fe3b3e19c5761b38ca5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.977762460708618, 'eval_wer': 0.9990049751243781, 'eval_runtime': 10.678, 'eval_samples_per_second': 18.73, 'eval_steps_per_second': 2.341, 'epoch': 24.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.869, 'learning_rate': 0.0002788, 'epoch': 24.4}\n",
            "{'loss': 3.8573, 'learning_rate': 0.00027679999999999995, 'epoch': 24.8}\n",
            "{'loss': 3.8345, 'learning_rate': 0.0002748, 'epoch': 25.2}\n",
            "{'loss': 3.8898, 'learning_rate': 0.00027279999999999996, 'epoch': 25.6}\n",
            "{'loss': 3.7843, 'learning_rate': 0.00027079999999999997, 'epoch': 26.0}\n",
            "{'loss': 3.8614, 'learning_rate': 0.0002688, 'epoch': 26.4}\n",
            "{'loss': 3.7694, 'learning_rate': 0.0002668, 'epoch': 26.8}\n",
            "{'loss': 3.7388, 'learning_rate': 0.0002648, 'epoch': 27.2}\n",
            "{'loss': 3.7843, 'learning_rate': 0.0002628, 'epoch': 27.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.7103, 'learning_rate': 0.00026079999999999994, 'epoch': 28.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4feb76e432bd476e88a824ac2c8af930",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.9193296432495117, 'eval_wer': 1.008955223880597, 'eval_runtime': 10.6308, 'eval_samples_per_second': 18.813, 'eval_steps_per_second': 2.352, 'epoch': 28.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.7462, 'learning_rate': 0.0002588, 'epoch': 28.4}\n",
            "{'loss': 3.6658, 'learning_rate': 0.00025679999999999995, 'epoch': 28.8}\n",
            "{'loss': 3.7034, 'learning_rate': 0.0002548, 'epoch': 29.2}\n",
            "{'loss': 3.6967, 'learning_rate': 0.00025279999999999996, 'epoch': 29.6}\n",
            "{'loss': 3.6402, 'learning_rate': 0.00025079999999999997, 'epoch': 30.0}\n",
            "{'loss': 3.6401, 'learning_rate': 0.0002488, 'epoch': 30.4}\n",
            "{'loss': 3.5968, 'learning_rate': 0.0002468, 'epoch': 30.8}\n",
            "{'loss': 3.6008, 'learning_rate': 0.0002448, 'epoch': 31.2}\n",
            "{'loss': 3.6182, 'learning_rate': 0.0002428, 'epoch': 31.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.5615, 'learning_rate': 0.00024079999999999997, 'epoch': 32.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fed14ec5af59440caad7dd2cfdefee14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.761359930038452, 'eval_wer': 1.0109452736318407, 'eval_runtime': 10.6356, 'eval_samples_per_second': 18.805, 'eval_steps_per_second': 2.351, 'epoch': 32.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.4932, 'learning_rate': 0.0002388, 'epoch': 32.4}\n",
            "{'loss': 3.6083, 'learning_rate': 0.00023679999999999998, 'epoch': 32.8}\n",
            "{'loss': 3.5314, 'learning_rate': 0.00023479999999999996, 'epoch': 33.2}\n",
            "{'loss': 3.52, 'learning_rate': 0.0002328, 'epoch': 33.6}\n",
            "{'loss': 3.3663, 'learning_rate': 0.00023079999999999997, 'epoch': 34.0}\n",
            "{'loss': 3.4544, 'learning_rate': 0.0002288, 'epoch': 34.4}\n",
            "{'loss': 3.4092, 'learning_rate': 0.00022679999999999998, 'epoch': 34.8}\n",
            "{'loss': 3.3894, 'learning_rate': 0.00022479999999999996, 'epoch': 35.2}\n",
            "{'loss': 3.3884, 'learning_rate': 0.0002228, 'epoch': 35.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.2714, 'learning_rate': 0.00022079999999999997, 'epoch': 36.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd53e20fb63947f3807a3e7ecfc203b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.5632684230804443, 'eval_wer': 0.9900497512437811, 'eval_runtime': 10.3903, 'eval_samples_per_second': 19.249, 'eval_steps_per_second': 2.406, 'epoch': 36.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.2619, 'learning_rate': 0.00021879999999999995, 'epoch': 36.4}\n",
            "{'loss': 3.2608, 'learning_rate': 0.00021679999999999998, 'epoch': 36.8}\n",
            "{'loss': 3.1889, 'learning_rate': 0.00021479999999999996, 'epoch': 37.2}\n",
            "{'loss': 3.1588, 'learning_rate': 0.0002128, 'epoch': 37.6}\n",
            "{'loss': 3.0963, 'learning_rate': 0.00021079999999999997, 'epoch': 38.0}\n",
            "{'loss': 3.1423, 'learning_rate': 0.00020879999999999998, 'epoch': 38.4}\n",
            "{'loss': 3.0915, 'learning_rate': 0.00020679999999999999, 'epoch': 38.8}\n",
            "{'loss': 2.9934, 'learning_rate': 0.00020479999999999996, 'epoch': 39.2}\n",
            "{'loss': 3.0307, 'learning_rate': 0.0002028, 'epoch': 39.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9816, 'learning_rate': 0.00020079999999999997, 'epoch': 40.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01d4c99ebc4a49c1abb0d2a7fe58750f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.365421772003174, 'eval_wer': 0.9691542288557214, 'eval_runtime': 10.345, 'eval_samples_per_second': 19.333, 'eval_steps_per_second': 2.417, 'epoch': 40.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9834, 'learning_rate': 0.00019879999999999998, 'epoch': 40.4}\n",
            "{'loss': 2.9021, 'learning_rate': 0.00019679999999999999, 'epoch': 40.8}\n",
            "{'loss': 2.8279, 'learning_rate': 0.0001948, 'epoch': 41.2}\n",
            "{'loss': 2.8688, 'learning_rate': 0.0001928, 'epoch': 41.6}\n",
            "{'loss': 2.7975, 'learning_rate': 0.00019079999999999998, 'epoch': 42.0}\n",
            "{'loss': 2.7619, 'learning_rate': 0.00018879999999999998, 'epoch': 42.4}\n",
            "{'loss': 2.6786, 'learning_rate': 0.0001868, 'epoch': 42.8}\n",
            "{'loss': 2.6336, 'learning_rate': 0.0001848, 'epoch': 43.2}\n",
            "{'loss': 2.619, 'learning_rate': 0.00018279999999999997, 'epoch': 43.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.577, 'learning_rate': 0.00018079999999999998, 'epoch': 44.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a73da0aa86544d096a41e26493b5b49",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.0966265201568604, 'eval_wer': 0.9532338308457712, 'eval_runtime': 10.7304, 'eval_samples_per_second': 18.639, 'eval_steps_per_second': 2.33, 'epoch': 44.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.5293, 'learning_rate': 0.00017879999999999998, 'epoch': 44.4}\n",
            "{'loss': 2.5273, 'learning_rate': 0.0001768, 'epoch': 44.8}\n",
            "{'loss': 2.4542, 'learning_rate': 0.0001748, 'epoch': 45.2}\n",
            "{'loss': 2.3773, 'learning_rate': 0.00017279999999999997, 'epoch': 45.6}\n",
            "{'loss': 2.3045, 'learning_rate': 0.0001708, 'epoch': 46.0}\n",
            "{'loss': 2.2994, 'learning_rate': 0.00016879999999999998, 'epoch': 46.4}\n",
            "{'loss': 2.2216, 'learning_rate': 0.0001668, 'epoch': 46.8}\n",
            "{'loss': 2.1192, 'learning_rate': 0.0001648, 'epoch': 47.2}\n",
            "{'loss': 2.144, 'learning_rate': 0.00016279999999999997, 'epoch': 47.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.0261, 'learning_rate': 0.0001608, 'epoch': 48.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "600ed8179eb947178e945b789cc7952c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.8286943435668945, 'eval_wer': 0.9791044776119403, 'eval_runtime': 10.5864, 'eval_samples_per_second': 18.892, 'eval_steps_per_second': 2.362, 'epoch': 48.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.0098, 'learning_rate': 0.00015879999999999998, 'epoch': 48.4}\n",
            "{'loss': 1.969, 'learning_rate': 0.00015679999999999996, 'epoch': 48.8}\n",
            "{'loss': 1.9436, 'learning_rate': 0.0001548, 'epoch': 49.2}\n",
            "{'loss': 1.7886, 'learning_rate': 0.00015279999999999997, 'epoch': 49.6}\n",
            "{'loss': 1.7848, 'learning_rate': 0.0001508, 'epoch': 50.0}\n",
            "{'loss': 1.7386, 'learning_rate': 0.00014879999999999998, 'epoch': 50.4}\n",
            "{'loss': 1.7142, 'learning_rate': 0.0001468, 'epoch': 50.8}\n",
            "{'loss': 1.5966, 'learning_rate': 0.0001448, 'epoch': 51.2}\n",
            "{'loss': 1.592, 'learning_rate': 0.00014279999999999997, 'epoch': 51.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.5506, 'learning_rate': 0.00014079999999999998, 'epoch': 52.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a51f5c33c2141b696b2268fd25ef808",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.6418256759643555, 'eval_wer': 1.0338308457711443, 'eval_runtime': 10.6161, 'eval_samples_per_second': 18.839, 'eval_steps_per_second': 2.355, 'epoch': 52.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.5176, 'learning_rate': 0.00013879999999999999, 'epoch': 52.4}\n",
            "{'loss': 1.4956, 'learning_rate': 0.0001368, 'epoch': 52.8}\n",
            "{'loss': 1.4001, 'learning_rate': 0.00013479999999999997, 'epoch': 53.2}\n",
            "{'loss': 1.4119, 'learning_rate': 0.00013279999999999998, 'epoch': 53.6}\n",
            "{'loss': 1.2732, 'learning_rate': 0.00013079999999999998, 'epoch': 54.0}\n",
            "{'loss': 1.3515, 'learning_rate': 0.0001288, 'epoch': 54.4}\n",
            "{'loss': 1.2873, 'learning_rate': 0.0001268, 'epoch': 54.8}\n",
            "{'loss': 1.1798, 'learning_rate': 0.00012479999999999997, 'epoch': 55.2}\n",
            "{'loss': 1.1732, 'learning_rate': 0.00012279999999999998, 'epoch': 55.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.168, 'learning_rate': 0.0001208, 'epoch': 56.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35bb8828816e4cd0a4cb0e70f45f19a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.5494866371154785, 'eval_wer': 1.0577114427860697, 'eval_runtime': 10.4751, 'eval_samples_per_second': 19.093, 'eval_steps_per_second': 2.387, 'epoch': 56.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1336, 'learning_rate': 0.0001188, 'epoch': 56.4}\n",
            "{'loss': 1.0799, 'learning_rate': 0.00011679999999999998, 'epoch': 56.8}\n",
            "{'loss': 1.0451, 'learning_rate': 0.00011479999999999999, 'epoch': 57.2}\n",
            "{'loss': 1.0625, 'learning_rate': 0.00011279999999999999, 'epoch': 57.6}\n",
            "{'loss': 0.9502, 'learning_rate': 0.0001108, 'epoch': 58.0}\n",
            "{'loss': 0.9817, 'learning_rate': 0.0001088, 'epoch': 58.4}\n",
            "{'loss': 0.9708, 'learning_rate': 0.00010679999999999998, 'epoch': 58.8}\n",
            "{'loss': 1.0184, 'learning_rate': 0.00010479999999999999, 'epoch': 59.2}\n",
            "{'loss': 0.8823, 'learning_rate': 0.00010279999999999999, 'epoch': 59.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8203, 'learning_rate': 0.0001008, 'epoch': 60.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be4078564eb74463ac2d518a81a52971",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.416396379470825, 'eval_wer': 1.0507462686567164, 'eval_runtime': 10.5801, 'eval_samples_per_second': 18.903, 'eval_steps_per_second': 2.363, 'epoch': 60.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9095, 'learning_rate': 9.879999999999999e-05, 'epoch': 60.4}\n",
            "{'loss': 0.8716, 'learning_rate': 9.68e-05, 'epoch': 60.8}\n",
            "{'loss': 0.7679, 'learning_rate': 9.479999999999999e-05, 'epoch': 61.2}\n",
            "{'loss': 0.8037, 'learning_rate': 9.279999999999999e-05, 'epoch': 61.6}\n",
            "{'loss': 0.8072, 'learning_rate': 9.079999999999998e-05, 'epoch': 62.0}\n",
            "{'loss': 0.8308, 'learning_rate': 8.879999999999999e-05, 'epoch': 62.4}\n",
            "{'loss': 0.7556, 'learning_rate': 8.68e-05, 'epoch': 62.8}\n",
            "{'loss': 0.6817, 'learning_rate': 8.48e-05, 'epoch': 63.2}\n",
            "{'loss': 0.6683, 'learning_rate': 8.28e-05, 'epoch': 63.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6819, 'learning_rate': 8.079999999999999e-05, 'epoch': 64.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e79f88abce664e8280ad9bef8f8324ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.3830456733703613, 'eval_wer': 1.0288557213930347, 'eval_runtime': 10.4245, 'eval_samples_per_second': 19.186, 'eval_steps_per_second': 2.398, 'epoch': 64.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7647, 'learning_rate': 7.879999999999999e-05, 'epoch': 64.4}\n",
            "{'loss': 0.7094, 'learning_rate': 7.68e-05, 'epoch': 64.8}\n",
            "{'loss': 0.6143, 'learning_rate': 7.479999999999999e-05, 'epoch': 65.2}\n",
            "{'loss': 0.6772, 'learning_rate': 7.28e-05, 'epoch': 65.6}\n",
            "{'loss': 0.6087, 'learning_rate': 7.079999999999999e-05, 'epoch': 66.0}\n",
            "{'loss': 0.5963, 'learning_rate': 6.879999999999999e-05, 'epoch': 66.4}\n",
            "{'loss': 0.6322, 'learning_rate': 6.68e-05, 'epoch': 66.8}\n",
            "{'loss': 0.5854, 'learning_rate': 6.479999999999999e-05, 'epoch': 67.2}\n",
            "{'loss': 0.6532, 'learning_rate': 6.28e-05, 'epoch': 67.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5473, 'learning_rate': 6.0799999999999994e-05, 'epoch': 68.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f12d0cb6ddff41adbaf9254289f21676",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.412040948867798, 'eval_wer': 1.045771144278607, 'eval_runtime': 10.4709, 'eval_samples_per_second': 19.1, 'eval_steps_per_second': 2.388, 'epoch': 68.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6703, 'learning_rate': 5.88e-05, 'epoch': 68.4}\n",
            "{'loss': 0.6043, 'learning_rate': 5.679999999999999e-05, 'epoch': 68.8}\n",
            "{'loss': 0.5682, 'learning_rate': 5.48e-05, 'epoch': 69.2}\n",
            "{'loss': 0.5973, 'learning_rate': 5.279999999999999e-05, 'epoch': 69.6}\n",
            "{'loss': 0.5064, 'learning_rate': 5.0799999999999995e-05, 'epoch': 70.0}\n",
            "{'loss': 0.5542, 'learning_rate': 4.8799999999999994e-05, 'epoch': 70.4}\n",
            "{'loss': 0.495, 'learning_rate': 4.68e-05, 'epoch': 70.8}\n",
            "{'loss': 0.533, 'learning_rate': 4.48e-05, 'epoch': 71.2}\n",
            "{'loss': 0.5357, 'learning_rate': 4.28e-05, 'epoch': 71.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5329, 'learning_rate': 4.08e-05, 'epoch': 72.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5516102c35464c55b4b1cc28846b9468",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.4236600399017334, 'eval_wer': 1.0517412935323383, 'eval_runtime': 10.6996, 'eval_samples_per_second': 18.692, 'eval_steps_per_second': 2.337, 'epoch': 72.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5942, 'learning_rate': 3.8799999999999994e-05, 'epoch': 72.4}\n",
            "{'loss': 0.473, 'learning_rate': 3.679999999999999e-05, 'epoch': 72.8}\n",
            "{'loss': 0.4587, 'learning_rate': 3.48e-05, 'epoch': 73.2}\n",
            "{'loss': 0.4666, 'learning_rate': 3.28e-05, 'epoch': 73.6}\n",
            "{'loss': 0.4906, 'learning_rate': 3.0799999999999996e-05, 'epoch': 74.0}\n",
            "{'loss': 0.4951, 'learning_rate': 2.88e-05, 'epoch': 74.4}\n",
            "{'loss': 0.4879, 'learning_rate': 2.6799999999999998e-05, 'epoch': 74.8}\n",
            "{'loss': 0.4544, 'learning_rate': 2.4799999999999996e-05, 'epoch': 75.2}\n",
            "{'loss': 0.4775, 'learning_rate': 2.28e-05, 'epoch': 75.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4523, 'learning_rate': 2.0799999999999997e-05, 'epoch': 76.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "287f6319d9e04e9f93506f66ff7b8839",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.4080045223236084, 'eval_wer': 1.037810945273632, 'eval_runtime': 10.3367, 'eval_samples_per_second': 19.349, 'eval_steps_per_second': 2.419, 'epoch': 76.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4593, 'learning_rate': 1.8799999999999996e-05, 'epoch': 76.4}\n",
            "{'loss': 0.4669, 'learning_rate': 1.68e-05, 'epoch': 76.8}\n",
            "{'loss': 0.4613, 'learning_rate': 1.4799999999999999e-05, 'epoch': 77.2}\n",
            "{'loss': 0.446, 'learning_rate': 1.2799999999999998e-05, 'epoch': 77.6}\n",
            "{'loss': 0.4333, 'learning_rate': 1.0799999999999998e-05, 'epoch': 78.0}\n",
            "{'loss': 0.4896, 'learning_rate': 8.799999999999999e-06, 'epoch': 78.4}\n",
            "{'loss': 0.4578, 'learning_rate': 6.8e-06, 'epoch': 78.8}\n",
            "{'loss': 0.4424, 'learning_rate': 4.8e-06, 'epoch': 79.2}\n",
            "{'loss': 0.4888, 'learning_rate': 2.8e-06, 'epoch': 79.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4335, 'learning_rate': 7.999999999999999e-07, 'epoch': 80.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "363a561af63b4f618a11f0e0ae2efee4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.384470224380493, 'eval_wer': 1.0407960199004975, 'eval_runtime': 11.1086, 'eval_samples_per_second': 18.004, 'eval_steps_per_second': 2.251, 'epoch': 80.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\preprocessor_config.json\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 1587657280 vs 1587657168",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1004\\4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m         )\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1627\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1800\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1801\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m             \u001b[1;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1903\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSCHEDULER_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:300] . unexpected pos 1587657280 vs 1587657168"
          ]
        }
      ],
      "source": [
        "hist = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxpIR_X8w3Pq"
      },
      "source": [
        "CTC 손실을 사용하여 더 큰 데이터 세트에서 더 큰 모델을 미세 조정하려면 [여기서](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) 공식 음성 인식 예를 살펴봐야 한다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "00.XLSR - Wav2Vec2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "99a2429cd063090ad39acbbb390da3f5879d741ee9cb02f1744db7376a74b1ea"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
