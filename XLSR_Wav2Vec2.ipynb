{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLCZvETJuFaO"
      },
      "source": [
        "## Create Wav2Vec2CTCTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-722dfaf1004ac490\n",
            "Reusing dataset csv (C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-722dfaf1004ac490\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
          ]
        }
      ],
      "source": [
        "all_data = load_dataset('csv',data_files='./order_speech_ko1000.csv',split='train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G34Hj6BgnK3L"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower() + \" \"\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-722dfaf1004ac490\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-cc8bd026da9643a4.arrow\n"
          ]
        }
      ],
      "source": [
        "remove_spectial_char_data = all_data.map(remove_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>script1_g_0044-6001-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>보고 있는 영상 정지시켜 줘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>script1_g_0044-6002-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>다음 주까지 날씨가 어때</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>script1_g_0044-6003-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>나 대신 점등해 줘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>script1_g_0044-6004-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>이번 주 대체로 흐린지 궁금해</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>script1_g_0044-6005-01-01-KSM-F-05-A.wav</td>\n",
              "      <td>지금 당장 취침 등 꺼 줘</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        src               text\n",
              "0  script1_g_0044-6001-01-01-KSM-F-05-A.wav   보고 있는 영상 정지시켜 줘 \n",
              "1  script1_g_0044-6002-01-01-KSM-F-05-A.wav     다음 주까지 날씨가 어때 \n",
              "2  script1_g_0044-6003-01-01-KSM-F-05-A.wav        나 대신 점등해 줘 \n",
              "3  script1_g_0044-6004-01-01-KSM-F-05-A.wav  이번 주 대체로 흐린지 궁금해 \n",
              "4  script1_g_0044-6005-01-01-KSM-F-05-A.wav    지금 당장 취침 등 꺼 줘 "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(remove_spectial_char_data)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZsimiwGSnWDY"
      },
      "outputs": [],
      "source": [
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"text\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6e5a09463ef465998c908821fbc8149",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "char_vocab = remove_spectial_char_data.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=remove_spectial_char_data.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['vocab', 'all_text'],\n",
              "    num_rows: 1\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_list = list(set(char_vocab[\"vocab\"][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'바': 0,\n",
              " '캐': 1,\n",
              " '라': 2,\n",
              " '오': 3,\n",
              " '냄': 4,\n",
              " '녹': 5,\n",
              " '큰': 6,\n",
              " '히': 7,\n",
              " '먹': 8,\n",
              " '급': 9,\n",
              " '무': 10,\n",
              " '량': 11,\n",
              " '없': 12,\n",
              " '쓰': 13,\n",
              " ' ': 14,\n",
              " '중': 15,\n",
              " '챙': 16,\n",
              " '깜': 17,\n",
              " '울': 18,\n",
              " '딘': 19,\n",
              " '퍼': 20,\n",
              " '연': 21,\n",
              " '성': 22,\n",
              " '균': 23,\n",
              " '즌': 24,\n",
              " '춤': 25,\n",
              " '더': 26,\n",
              " '걸': 27,\n",
              " '종': 28,\n",
              " '린': 29,\n",
              " '잠': 30,\n",
              " '십': 31,\n",
              " '주': 32,\n",
              " '빛': 33,\n",
              " '흥': 34,\n",
              " '떤': 35,\n",
              " '밤': 36,\n",
              " '찾': 37,\n",
              " '방': 38,\n",
              " '람': 39,\n",
              " '천': 40,\n",
              " '슨': 41,\n",
              " '아': 42,\n",
              " '크': 43,\n",
              " '위': 44,\n",
              " '봅': 45,\n",
              " '핫': 46,\n",
              " '내': 47,\n",
              " '마': 48,\n",
              " '토': 49,\n",
              " '몇': 50,\n",
              " '생': 51,\n",
              " '만': 52,\n",
              " '벽': 53,\n",
              " '넣': 54,\n",
              " '콘': 55,\n",
              " '한': 56,\n",
              " '원': 57,\n",
              " '터': 58,\n",
              " '헤': 59,\n",
              " '포': 60,\n",
              " '엔': 61,\n",
              " '률': 62,\n",
              " '닫': 63,\n",
              " '같': 64,\n",
              " '떻': 65,\n",
              " '많': 66,\n",
              " '하': 67,\n",
              " '치': 68,\n",
              " '싫': 69,\n",
              " '뀌': 70,\n",
              " '필': 71,\n",
              " '력': 72,\n",
              " '광': 73,\n",
              " '열': 74,\n",
              " '려': 75,\n",
              " '도': 76,\n",
              " '씨': 77,\n",
              " '을': 78,\n",
              " '롤': 79,\n",
              " '산': 80,\n",
              " '듣': 81,\n",
              " '태': 82,\n",
              " '져': 83,\n",
              " '름': 84,\n",
              " '총': 85,\n",
              " '막': 86,\n",
              " '풀': 87,\n",
              " '맑': 88,\n",
              " '전': 89,\n",
              " '났': 90,\n",
              " '쉬': 91,\n",
              " '퇴': 92,\n",
              " '팔': 93,\n",
              " '황': 94,\n",
              " '떼': 95,\n",
              " '플': 96,\n",
              " '끝': 97,\n",
              " '최': 98,\n",
              " '텐': 99,\n",
              " '프': 100,\n",
              " '교': 101,\n",
              " '얘': 102,\n",
              " '잘': 103,\n",
              " '돌': 104,\n",
              " '키': 105,\n",
              " '별': 106,\n",
              " '달': 107,\n",
              " '몸': 108,\n",
              " '서': 109,\n",
              " '즈': 110,\n",
              " '영': 111,\n",
              " '멍': 112,\n",
              " '권': 113,\n",
              " '역': 114,\n",
              " '줄': 115,\n",
              " '트': 116,\n",
              " '있': 117,\n",
              " '답': 118,\n",
              " '홉': 119,\n",
              " '복': 120,\n",
              " '겨': 121,\n",
              " '선': 122,\n",
              " '놓': 123,\n",
              " '순': 124,\n",
              " '널': 125,\n",
              " '심': 126,\n",
              " '관': 127,\n",
              " '덜': 128,\n",
              " '애': 129,\n",
              " '학': 130,\n",
              " '슬': 131,\n",
              " '며': 132,\n",
              " '우': 133,\n",
              " '피': 134,\n",
              " '칼': 135,\n",
              " '런': 136,\n",
              " '본': 137,\n",
              " '대': 138,\n",
              " '소': 139,\n",
              " '련': 140,\n",
              " '즘': 141,\n",
              " '발': 142,\n",
              " '줬': 143,\n",
              " '월': 144,\n",
              " '혀': 145,\n",
              " '문': 146,\n",
              " '잔': 147,\n",
              " '과': 148,\n",
              " '환': 149,\n",
              " '경': 150,\n",
              " '취': 151,\n",
              " '새': 152,\n",
              " '품': 153,\n",
              " '수': 154,\n",
              " '뭔': 155,\n",
              " '합': 156,\n",
              " '칠': 157,\n",
              " '목': 158,\n",
              " '꿀': 159,\n",
              " '음': 160,\n",
              " '약': 161,\n",
              " '회': 162,\n",
              " '브': 163,\n",
              " '괄': 164,\n",
              " '료': 165,\n",
              " '삭': 166,\n",
              " '짧': 167,\n",
              " '느': 168,\n",
              " '픽': 169,\n",
              " '펴': 170,\n",
              " '혼': 171,\n",
              " '탐': 172,\n",
              " '낌': 173,\n",
              " '체': 174,\n",
              " '미': 175,\n",
              " '독': 176,\n",
              " '못': 177,\n",
              " '빠': 178,\n",
              " '둡': 179,\n",
              " '뤄': 180,\n",
              " '건': 181,\n",
              " '감': 182,\n",
              " '꼭': 183,\n",
              " '떨': 184,\n",
              " '렴': 185,\n",
              " '게': 186,\n",
              " '디': 187,\n",
              " '때': 188,\n",
              " '야': 189,\n",
              " '춰': 190,\n",
              " '냐': 191,\n",
              " '눈': 192,\n",
              " '빵': 193,\n",
              " '거': 194,\n",
              " '놔': 195,\n",
              " '존': 196,\n",
              " '는': 197,\n",
              " '룸': 198,\n",
              " '봤': 199,\n",
              " '루': 200,\n",
              " '둠': 201,\n",
              " '여': 202,\n",
              " '준': 203,\n",
              " '어': 204,\n",
              " '맛': 205,\n",
              " '튼': 206,\n",
              " '맙': 207,\n",
              " '갈': 208,\n",
              " '딱': 209,\n",
              " '흘': 210,\n",
              " '셔': 211,\n",
              " '부': 212,\n",
              " '떠': 213,\n",
              " '곳': 214,\n",
              " '책': 215,\n",
              " '녁': 216,\n",
              " '쌀': 217,\n",
              " '그': 218,\n",
              " '출': 219,\n",
              " '팀': 220,\n",
              " '운': 221,\n",
              " '엇': 222,\n",
              " '반': 223,\n",
              " '굽': 224,\n",
              " '근': 225,\n",
              " '괜': 226,\n",
              " '갔': 227,\n",
              " '층': 228,\n",
              " '간': 229,\n",
              " '센': 230,\n",
              " '모': 231,\n",
              " '각': 232,\n",
              " '분': 233,\n",
              " '얼': 234,\n",
              " '류': 235,\n",
              " '팝': 236,\n",
              " '르': 237,\n",
              " '동': 238,\n",
              " '했': 239,\n",
              " '세': 240,\n",
              " '테': 241,\n",
              " '줘': 242,\n",
              " '껴': 243,\n",
              " '규': 244,\n",
              " '흐': 245,\n",
              " '화': 246,\n",
              " '측': 247,\n",
              " '풍': 248,\n",
              " '뜨': 249,\n",
              " '활': 250,\n",
              " '와': 251,\n",
              " '습': 252,\n",
              " '슈': 253,\n",
              " '말': 254,\n",
              " '밝': 255,\n",
              " '낭': 256,\n",
              " '힐': 257,\n",
              " '깐': 258,\n",
              " '었': 259,\n",
              " '청': 260,\n",
              " '넷': 261,\n",
              " '쪽': 262,\n",
              " '뉴': 263,\n",
              " '스': 264,\n",
              " '결': 265,\n",
              " '졌': 266,\n",
              " '레': 267,\n",
              " '석': 268,\n",
              " '해': 269,\n",
              " '드': 270,\n",
              " '랑': 271,\n",
              " '찮': 272,\n",
              " '배': 273,\n",
              " '탕': 274,\n",
              " '메': 275,\n",
              " '용': 276,\n",
              " '왔': 277,\n",
              " '데': 278,\n",
              " '진': 279,\n",
              " '든': 280,\n",
              " '쳐': 281,\n",
              " '휴': 282,\n",
              " '밖': 283,\n",
              " '핵': 284,\n",
              " '개': 285,\n",
              " '처': 286,\n",
              " '뽑': 287,\n",
              " '점': 288,\n",
              " '항': 289,\n",
              " '던': 290,\n",
              " '물': 291,\n",
              " '손': 292,\n",
              " '농': 293,\n",
              " '까': 294,\n",
              " '예': 295,\n",
              " '엘': 296,\n",
              " '특': 297,\n",
              " '번': 298,\n",
              " '면': 299,\n",
              " '다': 300,\n",
              " '악': 301,\n",
              " '뒤': 302,\n",
              " '언': 303,\n",
              " '탁': 304,\n",
              " '호': 305,\n",
              " '늦': 306,\n",
              " '앞': 307,\n",
              " '를': 308,\n",
              " '니': 309,\n",
              " '티': 310,\n",
              " '베': 311,\n",
              " '된': 312,\n",
              " '큼': 313,\n",
              " '햇': 314,\n",
              " '융': 315,\n",
              " '받': 316,\n",
              " '증': 317,\n",
              " '송': 318,\n",
              " '속': 319,\n",
              " '귀': 320,\n",
              " '장': 321,\n",
              " '씻': 322,\n",
              " '머': 323,\n",
              " '걷': 324,\n",
              " '효': 325,\n",
              " '리': 326,\n",
              " '랜': 327,\n",
              " '될': 328,\n",
              " '작': 329,\n",
              " '컨': 330,\n",
              " '색': 331,\n",
              " '이': 332,\n",
              " '았': 333,\n",
              " '착': 334,\n",
              " '망': 335,\n",
              " '빙': 336,\n",
              " '등': 337,\n",
              " '곡': 338,\n",
              " '꿔': 339,\n",
              " '것': 340,\n",
              " '온': 341,\n",
              " '입': 342,\n",
              " '닥': 343,\n",
              " '능': 344,\n",
              " '택': 345,\n",
              " '되': 346,\n",
              " '됐': 347,\n",
              " '탑': 348,\n",
              " '끓': 349,\n",
              " '금': 350,\n",
              " '닝': 351,\n",
              " '업': 352,\n",
              " '쯤': 353,\n",
              " '인': 354,\n",
              " '편': 355,\n",
              " '멈': 356,\n",
              " '승': 357,\n",
              " '알': 358,\n",
              " '구': 359,\n",
              " '율': 360,\n",
              " '궁': 361,\n",
              " '할': 362,\n",
              " '재': 363,\n",
              " '비': 364,\n",
              " '조': 365,\n",
              " '변': 366,\n",
              " '너': 367,\n",
              " '남': 368,\n",
              " '침': 369,\n",
              " '매': 370,\n",
              " '뜻': 371,\n",
              " '뀐': 372,\n",
              " '투': 373,\n",
              " '외': 374,\n",
              " '뱅': 375,\n",
              " '맥': 376,\n",
              " '따': 377,\n",
              " '텔': 378,\n",
              " '샷': 379,\n",
              " '록': 380,\n",
              " '의': 381,\n",
              " '좀': 382,\n",
              " '지': 383,\n",
              " '림': 384,\n",
              " '코': 385,\n",
              " '끼': 386,\n",
              " '륙': 387,\n",
              " '떴': 388,\n",
              " '창': 389,\n",
              " '읊': 390,\n",
              " '워': 391,\n",
              " '액': 392,\n",
              " '펼': 393,\n",
              " '나': 394,\n",
              " '설': 395,\n",
              " '기': 396,\n",
              " '불': 397,\n",
              " '살': 398,\n",
              " '또': 399,\n",
              " '곱': 400,\n",
              " '국': 401,\n",
              " '절': 402,\n",
              " '쓸': 403,\n",
              " '깨': 404,\n",
              " '강': 405,\n",
              " '민': 406,\n",
              " '폭': 407,\n",
              " '길': 408,\n",
              " '축': 409,\n",
              " '짜': 410,\n",
              " '덤': 411,\n",
              " '러': 412,\n",
              " '직': 413,\n",
              " '난': 414,\n",
              " '표': 415,\n",
              " '즉': 416,\n",
              " '꺼': 417,\n",
              " '로': 418,\n",
              " '시': 419,\n",
              " '늘': 420,\n",
              " '겠': 421,\n",
              " '네': 422,\n",
              " '먼': 423,\n",
              " '빨': 424,\n",
              " '통': 425,\n",
              " '둬': 426,\n",
              " '곧': 427,\n",
              " '래': 428,\n",
              " '안': 429,\n",
              " '제': 430,\n",
              " '락': 431,\n",
              " '블': 432,\n",
              " '으': 433,\n",
              " '정': 434,\n",
              " '에': 435,\n",
              " '잡': 436,\n",
              " '식': 437,\n",
              " '두': 438,\n",
              " '공': 439,\n",
              " '올': 440,\n",
              " '랐': 441,\n",
              " '켤': 442,\n",
              " '팅': 443,\n",
              " '럼': 444,\n",
              " '즐': 445,\n",
              " '횐': 446,\n",
              " '집': 447,\n",
              " '행': 448,\n",
              " '검': 449,\n",
              " '넘': 450,\n",
              " '평': 451,\n",
              " '가': 452,\n",
              " '않': 453,\n",
              " '요': 454,\n",
              " '당': 455,\n",
              " '쇼': 456,\n",
              " '혹': 457,\n",
              " '노': 458,\n",
              " '릴': 459,\n",
              " '봐': 460,\n",
              " '엠': 461,\n",
              " '들': 462,\n",
              " '형': 463,\n",
              " '적': 464,\n",
              " '싶': 465,\n",
              " '짝': 466,\n",
              " '낮': 467,\n",
              " '자': 468,\n",
              " '현': 469,\n",
              " '높': 470,\n",
              " '차': 471,\n",
              " '날': 472,\n",
              " '사': 473,\n",
              " '픈': 474,\n",
              " '저': 475,\n",
              " '룹': 476,\n",
              " '였': 477,\n",
              " '좋': 478,\n",
              " '은': 479,\n",
              " '케': 480,\n",
              " '깥': 481,\n",
              " '계': 482,\n",
              " '법': 483,\n",
              " '추': 484,\n",
              " '임': 485,\n",
              " '실': 486,\n",
              " '커': 487,\n",
              " '상': 488,\n",
              " '란': 489,\n",
              " '략': 490,\n",
              " '삼': 491,\n",
              " '꾸': 492,\n",
              " '턴': 493,\n",
              " '확': 494,\n",
              " '놨': 495,\n",
              " '유': 496,\n",
              " '졸': 497,\n",
              " '볼': 498,\n",
              " '읽': 499,\n",
              " '켜': 500,\n",
              " '뭐': 501,\n",
              " '채': 502,\n",
              " '핑': 503,\n",
              " '보': 504,\n",
              " '일': 505,\n",
              " '뜰': 506,\n",
              " '톱': 507,\n",
              " '틀': 508,\n",
              " '파': 509,\n",
              " '염': 510,\n",
              " '신': 511,\n",
              " '후': 512,\n",
              " '른': 513,\n",
              " '북': 514,\n",
              " '단': 515,\n",
              " '명': 516,\n",
              " '고': 517,\n",
              " '돼': 518,\n",
              " '쉼': 519,\n",
              " '웠': 520,\n",
              " '타': 521,\n",
              " '틴': 522,\n",
              " '초': 523,\n",
              " '카': 524,\n",
              " '맞': 525,\n",
              " '셋': 526,\n",
              " '츠': 527,\n",
              " '엌': 528}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "vocab_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_hi_HbR_rCKb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "531"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "del vocab_dict[\" \"]\n",
        "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "len(vocab_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rMWugphlrsZ1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('vocab.json', 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YJUUl9lnryU-"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2CTCTokenizer\n",
        "\n",
        "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\",\n",
        "                                 unk_token=\"[UNK]\",\n",
        "                                 pad_token=\"[PAD]\",\n",
        "                                 word_delimiter_token=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCtrdRsuuDix"
      },
      "source": [
        "## Create XLSR-Wav2Vec2 Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O0vdrkhKr8D1"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1,\n",
        "                                             sampling_rate=16000,\n",
        "                                             padding_value=0.0,\n",
        "                                             do_normalize=True,\n",
        "                                             return_attention_mask=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fY_Qm1dpsE78"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor(feature_extractor=feature_extractor,\n",
        "                              tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KTUahP1PsL3g"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9tMSA3xKsVma"
      },
      "outputs": [],
      "source": [
        "# processor.save_pretrained(\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## add audio array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def voice_sep(sig):\n",
        "    sig = np.array(sig).flatten()\n",
        "    S_full, phase = librosa.magphase(librosa.stft(sig))\n",
        "    S_filter = librosa.decompose.nn_filter(S_full,\n",
        "                                       aggregate=np.median,\n",
        "                                       metric='cosine',\n",
        "                                       width=int(librosa.time_to_frames(2, sr=sr)))\n",
        "    S_filter = np.minimum(S_full, S_filter)\n",
        "    margin_v = 2\n",
        "    power = 2\n",
        "    mask_v = librosa.util.softmask(S_full - S_filter,\n",
        "                               margin_v * S_filter,\n",
        "                               power=power)\n",
        "    S_foreground = mask_v * S_full\n",
        "    y_foreground = librosa.istft(S_foreground * phase)\n",
        "    return y_foreground\n",
        "\n",
        "def load_audio(batch):\n",
        "    batch['array'],_ = librosa.load('./dataset/audio/'+batch['src'],sr=16000)\n",
        "    batch['array'] = voice_sep(batch['array'])\n",
        "    \n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at C:\\Users\\AI_server\\.cache\\huggingface\\datasets\\csv\\default-722dfaf1004ac490\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519\\cache-baae49471d76c12f.arrow\n"
          ]
        }
      ],
      "source": [
        "audio_data = remove_spectial_char_data.map(load_audio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZiv4zqyt2cY"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AscOxzgUsb50"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "\n",
        "    # batched output is \"un-batched\"\n",
        "    batch[\"input_values\"] = processor(batch[\"array\"], sampling_rate=16000).input_values[0]\n",
        "    \n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['src', 'text', 'array'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "audio_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a093fe1ec134e4fb1e43075f16828fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?ex/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "order_voice = audio_data.map(\n",
        "    prepare_dataset,\n",
        "    remove_columns=remove_spectial_char_data.column_names,\n",
        "    # num_proc=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['array', 'input_values', 'labels'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "order_voice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StCtCQxDtuY3"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUTqiRY-tpXZ"
      },
      "source": [
        "## Set-up Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "nTlewiOzsyqR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.cuda.HalfTensor]]]) -> Dict[str, torch.cuda.HalfTensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WmfIPs18tGQb"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vJjfNy9-tJM6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric, Audio\n",
        "\n",
        "wer_metric = load_metric(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "03mNNUfNthSZ"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_logits = pred.predictions\n",
        "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "\n",
        "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids)\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vgRoeQHvTqk"
      },
      "source": [
        "## Import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Iyv4qaclue6n"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/wav2vec2-large-xlsr-53 were not used when initializing Wav2Vec2ForCTC: ['project_hid.bias', 'project_q.weight', 'quantizer.codevectors', 'quantizer.weight_proj.weight', 'quantizer.weight_proj.bias', 'project_hid.weight', 'project_q.bias']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-large-xlsr-53\", \n",
        "    attention_dropout=0.1,\n",
        "    hidden_dropout=0.1,\n",
        "    feat_proj_dropout=0.0,\n",
        "    mask_time_prob=0.05,\n",
        "    layerdrop=0.1,\n",
        "    ctc_loss_reduction=\"mean\", \n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaYmsSutuv26"
      },
      "source": [
        "XLSR-Wav2Vec2의 첫 번째 구성 요소는 원시 음성 신호에서 음향적으로 의미가 있지만 문맥적으로 독립적인 기능을 추출하는 데 사용되는 CNN 계층 스택으로 구성됩니다.  \n",
        "모델의 이 부분은 사전 교육 중에 이미 충분히 훈련되었으며 논문에 명시된 바와 같이 더 이상 미세 조정할 필요가 없습니다. 따라서 특징 추출 부분의 모든 파라미터에 대해 require_grad를 False로 설정할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5VX8z6PpuhqG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1677: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "model.freeze_feature_extractor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAfoHUk9vGxd"
      },
      "source": [
        "메모리를 절약하기 위해 그라데이션 체크포인팅을 활성화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wZm6in_2vAIN"
      },
      "outputs": [],
      "source": [
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-aJNQplvQgn"
      },
      "source": [
        "## TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-LuuwoeMvOnV"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  # output_dir=\"/content/gdrive/MyDrive/wav2vec2-large-xlsr-ko-demo\",\n",
        "  output_dir=\"./wav2vec2-large-xlsr-ko-demo\",\n",
        "  group_by_length=True,\n",
        "  per_device_train_batch_size=16,\n",
        "  gradient_accumulation_steps=2,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=100,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=3e-4,\n",
        "  warmup_steps=500,\n",
        "  save_total_limit=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6vhFC0BdvcOu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "ds_size = len(order_voice)\n",
        "train_size = int(ds_size*0.8)\n",
        "val_size = ds_size - train_size\n",
        "train_ds, val_ds = random_split(order_voice,[train_size,val_size])\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHGNj1NSvsFk"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "a-rJ8fokvjJb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\AI_server\\Anaconda3\\envs\\STT\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 800\n",
            "  Num Epochs = 100\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 2500\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "388f89c8eb4046ebbd2c8a7fd3e78578",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 51.3664, 'learning_rate': 4.8e-06, 'epoch': 0.4}\n",
            "{'loss': 52.9128, 'learning_rate': 1.0799999999999998e-05, 'epoch': 0.8}\n",
            "{'loss': 50.8141, 'learning_rate': 1.68e-05, 'epoch': 1.2}\n",
            "{'loss': 51.2312, 'learning_rate': 2.28e-05, 'epoch': 1.6}\n",
            "{'loss': 49.2999, 'learning_rate': 2.8199999999999998e-05, 'epoch': 2.0}\n",
            "{'loss': 44.1223, 'learning_rate': 3.42e-05, 'epoch': 2.4}\n",
            "{'loss': 37.6529, 'learning_rate': 3.96e-05, 'epoch': 2.8}\n",
            "{'loss': 29.5956, 'learning_rate': 4.56e-05, 'epoch': 3.2}\n",
            "{'loss': 26.2828, 'learning_rate': 5.1599999999999994e-05, 'epoch': 3.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 21.1898, 'learning_rate': 5.76e-05, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5b555a6c57043088cc96e8fd783c92b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 17.147859573364258, 'eval_wer': 1.0, 'eval_runtime': 17.1911, 'eval_samples_per_second': 11.634, 'eval_steps_per_second': 1.454, 'epoch': 4.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-100\\preprocessor_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 18.4054, 'learning_rate': 6.359999999999999e-05, 'epoch': 4.4}\n",
            "{'loss': 16.4396, 'learning_rate': 6.96e-05, 'epoch': 4.8}\n",
            "{'loss': 13.7893, 'learning_rate': 7.56e-05, 'epoch': 5.2}\n",
            "{'loss': 12.0708, 'learning_rate': 8.16e-05, 'epoch': 5.6}\n",
            "{'loss': 10.2045, 'learning_rate': 8.759999999999999e-05, 'epoch': 6.0}\n",
            "{'loss': 8.8138, 'learning_rate': 9.36e-05, 'epoch': 6.4}\n",
            "{'loss': 7.2258, 'learning_rate': 9.96e-05, 'epoch': 6.8}\n",
            "{'loss': 6.1952, 'learning_rate': 0.00010559999999999998, 'epoch': 7.2}\n",
            "{'loss': 5.4578, 'learning_rate': 0.00011159999999999999, 'epoch': 7.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.9816, 'learning_rate': 0.0001176, 'epoch': 8.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af47619decaf4b24bedbefee23dff862",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.713186740875244, 'eval_wer': 1.0, 'eval_runtime': 10.0235, 'eval_samples_per_second': 19.953, 'eval_steps_per_second': 2.494, 'epoch': 8.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-200\\preprocessor_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.6713, 'learning_rate': 0.0001236, 'epoch': 8.4}\n",
            "{'loss': 4.5442, 'learning_rate': 0.00012959999999999998, 'epoch': 8.8}\n",
            "{'loss': 4.5117, 'learning_rate': 0.0001356, 'epoch': 9.2}\n",
            "{'loss': 4.4809, 'learning_rate': 0.00014159999999999997, 'epoch': 9.6}\n",
            "{'loss': 4.476, 'learning_rate': 0.00014759999999999998, 'epoch': 10.0}\n",
            "{'loss': 4.4873, 'learning_rate': 0.0001536, 'epoch': 10.4}\n",
            "{'loss': 4.4631, 'learning_rate': 0.0001596, 'epoch': 10.8}\n",
            "{'loss': 4.4383, 'learning_rate': 0.0001656, 'epoch': 11.2}\n",
            "{'loss': 4.4939, 'learning_rate': 0.00017159999999999997, 'epoch': 11.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.4607, 'learning_rate': 0.00017759999999999998, 'epoch': 12.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12dcf24d70e243e89011eda351c5f089",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.479443550109863, 'eval_wer': 1.0, 'eval_runtime': 9.4923, 'eval_samples_per_second': 21.07, 'eval_steps_per_second': 2.634, 'epoch': 12.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.4744, 'learning_rate': 0.0001836, 'epoch': 12.4}\n",
            "{'loss': 4.4812, 'learning_rate': 0.00018959999999999997, 'epoch': 12.8}\n",
            "{'loss': 4.4642, 'learning_rate': 0.00019559999999999998, 'epoch': 13.2}\n",
            "{'loss': 4.4444, 'learning_rate': 0.0002016, 'epoch': 13.6}\n",
            "{'loss': 4.4789, 'learning_rate': 0.00020759999999999998, 'epoch': 14.0}\n",
            "{'loss': 4.4359, 'learning_rate': 0.00021359999999999996, 'epoch': 14.4}\n",
            "{'loss': 4.4588, 'learning_rate': 0.00021959999999999997, 'epoch': 14.8}\n",
            "{'loss': 4.4032, 'learning_rate': 0.00022559999999999998, 'epoch': 15.2}\n",
            "{'loss': 4.4066, 'learning_rate': 0.0002316, 'epoch': 15.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.352, 'learning_rate': 0.0002376, 'epoch': 16.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b06149c9d4614ea8af1988e491e5a113",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.363787651062012, 'eval_wer': 1.0, 'eval_runtime': 9.1931, 'eval_samples_per_second': 21.756, 'eval_steps_per_second': 2.719, 'epoch': 16.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.3391, 'learning_rate': 0.00024359999999999999, 'epoch': 16.4}\n",
            "{'loss': 4.2788, 'learning_rate': 0.00024959999999999994, 'epoch': 16.8}\n",
            "{'loss': 4.2254, 'learning_rate': 0.0002556, 'epoch': 17.2}\n",
            "{'loss': 4.2948, 'learning_rate': 0.00026159999999999996, 'epoch': 17.6}\n",
            "{'loss': 4.1979, 'learning_rate': 0.0002676, 'epoch': 18.0}\n",
            "{'loss': 4.1784, 'learning_rate': 0.0002736, 'epoch': 18.4}\n",
            "{'loss': 4.1527, 'learning_rate': 0.00027959999999999997, 'epoch': 18.8}\n",
            "{'loss': 4.2344, 'learning_rate': 0.00028559999999999995, 'epoch': 19.2}\n",
            "{'loss': 4.1441, 'learning_rate': 0.0002916, 'epoch': 19.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.1295, 'learning_rate': 0.00029759999999999997, 'epoch': 20.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79b2ff28743f4a829f105082df43dadf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.149694919586182, 'eval_wer': 0.9990300678952473, 'eval_runtime': 10.0026, 'eval_samples_per_second': 19.995, 'eval_steps_per_second': 2.499, 'epoch': 20.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.1023, 'learning_rate': 0.00029909999999999995, 'epoch': 20.4}\n",
            "{'loss': 4.0923, 'learning_rate': 0.00029759999999999997, 'epoch': 20.8}\n",
            "{'loss': 4.0971, 'learning_rate': 0.0002961, 'epoch': 21.2}\n",
            "{'loss': 4.0602, 'learning_rate': 0.00029459999999999995, 'epoch': 21.6}\n",
            "{'loss': 4.0481, 'learning_rate': 0.00029309999999999997, 'epoch': 22.0}\n",
            "{'loss': 4.0543, 'learning_rate': 0.0002916, 'epoch': 22.4}\n",
            "{'loss': 4.0219, 'learning_rate': 0.00029009999999999995, 'epoch': 22.8}\n",
            "{'loss': 3.9574, 'learning_rate': 0.00028859999999999997, 'epoch': 23.2}\n",
            "{'loss': 3.94, 'learning_rate': 0.0002871, 'epoch': 23.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9873, 'learning_rate': 0.00028559999999999995, 'epoch': 24.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d1ac578b0bb496b8332f77ab4a1c801",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 4.055669784545898, 'eval_wer': 0.9990300678952473, 'eval_runtime': 11.6247, 'eval_samples_per_second': 17.205, 'eval_steps_per_second': 2.151, 'epoch': 24.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9671, 'learning_rate': 0.00028409999999999997, 'epoch': 24.4}\n",
            "{'loss': 3.9591, 'learning_rate': 0.0002826, 'epoch': 24.8}\n",
            "{'loss': 3.8838, 'learning_rate': 0.0002811, 'epoch': 25.2}\n",
            "{'loss': 3.9082, 'learning_rate': 0.00027959999999999997, 'epoch': 25.6}\n",
            "{'loss': 3.8914, 'learning_rate': 0.0002781, 'epoch': 26.0}\n",
            "{'loss': 3.8651, 'learning_rate': 0.0002766, 'epoch': 26.4}\n",
            "{'loss': 3.8654, 'learning_rate': 0.00027509999999999996, 'epoch': 26.8}\n",
            "{'loss': 3.8347, 'learning_rate': 0.0002736, 'epoch': 27.2}\n",
            "{'loss': 3.8475, 'learning_rate': 0.0002721, 'epoch': 27.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.781, 'learning_rate': 0.00027059999999999996, 'epoch': 28.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6aada84baf414538bafa30d591afccdc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.864508628845215, 'eval_wer': 0.9990300678952473, 'eval_runtime': 13.56, 'eval_samples_per_second': 14.749, 'eval_steps_per_second': 1.844, 'epoch': 28.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.7682, 'learning_rate': 0.0002691, 'epoch': 28.4}\n",
            "{'loss': 3.7549, 'learning_rate': 0.0002676, 'epoch': 28.8}\n",
            "{'loss': 3.7184, 'learning_rate': 0.00026609999999999996, 'epoch': 29.2}\n",
            "{'loss': 3.7812, 'learning_rate': 0.0002646, 'epoch': 29.6}\n",
            "{'loss': 3.7738, 'learning_rate': 0.0002631, 'epoch': 30.0}\n",
            "{'loss': 3.7098, 'learning_rate': 0.00026159999999999996, 'epoch': 30.4}\n",
            "{'loss': 3.6547, 'learning_rate': 0.0002601, 'epoch': 30.8}\n",
            "{'loss': 3.6451, 'learning_rate': 0.0002586, 'epoch': 31.2}\n",
            "{'loss': 3.6038, 'learning_rate': 0.00025709999999999996, 'epoch': 31.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.5256, 'learning_rate': 0.0002556, 'epoch': 32.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4251e247e0314a2bbe7ad8f02a599951",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.677790641784668, 'eval_wer': 0.9932104752667313, 'eval_runtime': 9.6149, 'eval_samples_per_second': 20.801, 'eval_steps_per_second': 2.6, 'epoch': 32.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.6118, 'learning_rate': 0.0002541, 'epoch': 32.4}\n",
            "{'loss': 3.4485, 'learning_rate': 0.00025259999999999996, 'epoch': 32.8}\n",
            "{'loss': 3.4347, 'learning_rate': 0.0002511, 'epoch': 33.2}\n",
            "{'loss': 3.4231, 'learning_rate': 0.00024959999999999994, 'epoch': 33.6}\n",
            "{'loss': 3.3405, 'learning_rate': 0.00024809999999999996, 'epoch': 34.0}\n",
            "{'loss': 3.3611, 'learning_rate': 0.0002466, 'epoch': 34.4}\n",
            "{'loss': 3.2896, 'learning_rate': 0.00024509999999999994, 'epoch': 34.8}\n",
            "{'loss': 3.163, 'learning_rate': 0.00024359999999999999, 'epoch': 35.2}\n",
            "{'loss': 3.1321, 'learning_rate': 0.0002421, 'epoch': 35.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0573, 'learning_rate': 0.0002406, 'epoch': 36.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0821f7cd42a94f1c89c850f52c7a7199",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 3.1737325191497803, 'eval_wer': 0.9214354995150339, 'eval_runtime': 10.2349, 'eval_samples_per_second': 19.541, 'eval_steps_per_second': 2.443, 'epoch': 36.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1057, 'learning_rate': 0.00023909999999999998, 'epoch': 36.4}\n",
            "{'loss': 3.0095, 'learning_rate': 0.0002376, 'epoch': 36.8}\n",
            "{'loss': 2.8538, 'learning_rate': 0.0002361, 'epoch': 37.2}\n",
            "{'loss': 2.8309, 'learning_rate': 0.00023459999999999998, 'epoch': 37.6}\n",
            "{'loss': 2.7059, 'learning_rate': 0.00023309999999999997, 'epoch': 38.0}\n",
            "{'loss': 2.641, 'learning_rate': 0.0002316, 'epoch': 38.4}\n",
            "{'loss': 2.521, 'learning_rate': 0.00023009999999999998, 'epoch': 38.8}\n",
            "{'loss': 2.4025, 'learning_rate': 0.00022859999999999997, 'epoch': 39.2}\n",
            "{'loss': 2.2575, 'learning_rate': 0.0002271, 'epoch': 39.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.0987, 'learning_rate': 0.00022559999999999998, 'epoch': 40.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64624f96af2b43bdb134af2f77258471",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 2.2300209999084473, 'eval_wer': 0.903976721629486, 'eval_runtime': 9.1501, 'eval_samples_per_second': 21.858, 'eval_steps_per_second': 2.732, 'epoch': 40.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.0499, 'learning_rate': 0.00022409999999999997, 'epoch': 40.4}\n",
            "{'loss': 1.9429, 'learning_rate': 0.0002226, 'epoch': 40.8}\n",
            "{'loss': 1.8746, 'learning_rate': 0.00022109999999999998, 'epoch': 41.2}\n",
            "{'loss': 1.776, 'learning_rate': 0.00021959999999999997, 'epoch': 41.6}\n",
            "{'loss': 1.6675, 'learning_rate': 0.00021809999999999996, 'epoch': 42.0}\n",
            "{'loss': 1.5764, 'learning_rate': 0.00021659999999999998, 'epoch': 42.4}\n",
            "{'loss': 1.4698, 'learning_rate': 0.00021509999999999997, 'epoch': 42.8}\n",
            "{'loss': 1.4085, 'learning_rate': 0.00021359999999999996, 'epoch': 43.2}\n",
            "{'loss': 1.4134, 'learning_rate': 0.00021209999999999998, 'epoch': 43.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.2708, 'learning_rate': 0.00021059999999999997, 'epoch': 44.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc28b075b192470483e3e061fde2aecc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.5400234460830688, 'eval_wer': 0.8690591658583899, 'eval_runtime': 9.4969, 'eval_samples_per_second': 21.06, 'eval_steps_per_second': 2.632, 'epoch': 44.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.1929, 'learning_rate': 0.00020909999999999996, 'epoch': 44.4}\n",
            "{'loss': 1.1342, 'learning_rate': 0.00020759999999999998, 'epoch': 44.8}\n",
            "{'loss': 1.0933, 'learning_rate': 0.0002061, 'epoch': 45.2}\n",
            "{'loss': 1.0274, 'learning_rate': 0.00020459999999999999, 'epoch': 45.6}\n",
            "{'loss': 1.0287, 'learning_rate': 0.0002031, 'epoch': 46.0}\n",
            "{'loss': 0.9306, 'learning_rate': 0.0002016, 'epoch': 46.4}\n",
            "{'loss': 0.8974, 'learning_rate': 0.00020009999999999998, 'epoch': 46.8}\n",
            "{'loss': 0.8453, 'learning_rate': 0.0001986, 'epoch': 47.2}\n",
            "{'loss': 0.8077, 'learning_rate': 0.0001971, 'epoch': 47.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7882, 'learning_rate': 0.00019559999999999998, 'epoch': 48.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bb16c9179cb4ee09f990b5962bbfe29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.2667744159698486, 'eval_wer': 0.7381183317167799, 'eval_runtime': 15.7727, 'eval_samples_per_second': 12.68, 'eval_steps_per_second': 1.585, 'epoch': 48.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.7187, 'learning_rate': 0.0001941, 'epoch': 48.4}\n",
            "{'loss': 0.7488, 'learning_rate': 0.0001926, 'epoch': 48.8}\n",
            "{'loss': 0.7702, 'learning_rate': 0.00019109999999999998, 'epoch': 49.2}\n",
            "{'loss': 0.6095, 'learning_rate': 0.00018959999999999997, 'epoch': 49.6}\n",
            "{'loss': 0.6396, 'learning_rate': 0.0001881, 'epoch': 50.0}\n",
            "{'loss': 0.6028, 'learning_rate': 0.00018659999999999998, 'epoch': 50.4}\n",
            "{'loss': 0.6026, 'learning_rate': 0.00018509999999999997, 'epoch': 50.8}\n",
            "{'loss': 0.5802, 'learning_rate': 0.0001836, 'epoch': 51.2}\n",
            "{'loss': 0.5952, 'learning_rate': 0.00018209999999999998, 'epoch': 51.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5771, 'learning_rate': 0.00018059999999999997, 'epoch': 52.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "514330a1b0e44f5e970834a1ef1c0698",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.128394603729248, 'eval_wer': 0.8370514064015518, 'eval_runtime': 9.8204, 'eval_samples_per_second': 20.366, 'eval_steps_per_second': 2.546, 'epoch': 52.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5306, 'learning_rate': 0.0001791, 'epoch': 52.4}\n",
            "{'loss': 0.5245, 'learning_rate': 0.00017759999999999998, 'epoch': 52.8}\n",
            "{'loss': 0.4365, 'learning_rate': 0.00017609999999999997, 'epoch': 53.2}\n",
            "{'loss': 0.5639, 'learning_rate': 0.00017459999999999996, 'epoch': 53.6}\n",
            "{'loss': 0.4393, 'learning_rate': 0.00017309999999999998, 'epoch': 54.0}\n",
            "{'loss': 0.4472, 'learning_rate': 0.00017159999999999997, 'epoch': 54.4}\n",
            "{'loss': 0.4236, 'learning_rate': 0.00017009999999999996, 'epoch': 54.8}\n",
            "{'loss': 0.4152, 'learning_rate': 0.0001686, 'epoch': 55.2}\n",
            "{'loss': 0.3665, 'learning_rate': 0.0001671, 'epoch': 55.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3796, 'learning_rate': 0.0001656, 'epoch': 56.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ea31e26ba664a079b66cf59951325cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.0671281814575195, 'eval_wer': 0.7546071774975752, 'eval_runtime': 9.7118, 'eval_samples_per_second': 20.594, 'eval_steps_per_second': 2.574, 'epoch': 56.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3424, 'learning_rate': 0.0001641, 'epoch': 56.4}\n",
            "{'loss': 0.3642, 'learning_rate': 0.0001626, 'epoch': 56.8}\n",
            "{'loss': 0.33, 'learning_rate': 0.00016109999999999999, 'epoch': 57.2}\n",
            "{'loss': 0.296, 'learning_rate': 0.0001596, 'epoch': 57.6}\n",
            "{'loss': 0.3305, 'learning_rate': 0.0001581, 'epoch': 58.0}\n",
            "{'loss': 0.3222, 'learning_rate': 0.00015659999999999998, 'epoch': 58.4}\n",
            "{'loss': 0.3135, 'learning_rate': 0.0001551, 'epoch': 58.8}\n",
            "{'loss': 0.3422, 'learning_rate': 0.0001536, 'epoch': 59.2}\n",
            "{'loss': 0.2849, 'learning_rate': 0.00015209999999999998, 'epoch': 59.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3005, 'learning_rate': 0.00015059999999999997, 'epoch': 60.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09d32af471f5409c95c118c8b6b5507c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.988898754119873, 'eval_wer': 0.6935014548981572, 'eval_runtime': 9.4251, 'eval_samples_per_second': 21.22, 'eval_steps_per_second': 2.652, 'epoch': 60.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1300] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2685, 'learning_rate': 0.0001491, 'epoch': 60.4}\n",
            "{'loss': 0.2543, 'learning_rate': 0.00014759999999999998, 'epoch': 60.8}\n",
            "{'loss': 0.2742, 'learning_rate': 0.00014609999999999997, 'epoch': 61.2}\n",
            "{'loss': 0.282, 'learning_rate': 0.0001446, 'epoch': 61.6}\n",
            "{'loss': 0.2608, 'learning_rate': 0.00014309999999999998, 'epoch': 62.0}\n",
            "{'loss': 0.2424, 'learning_rate': 0.00014159999999999997, 'epoch': 62.4}\n",
            "{'loss': 0.2404, 'learning_rate': 0.0001401, 'epoch': 62.8}\n",
            "{'loss': 0.1985, 'learning_rate': 0.0001386, 'epoch': 63.2}\n",
            "{'loss': 0.2146, 'learning_rate': 0.0001371, 'epoch': 63.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.268, 'learning_rate': 0.0001356, 'epoch': 64.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5c53c829f3c4d3990865e6f59fce9ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 1.012782335281372, 'eval_wer': 0.7798254122211445, 'eval_runtime': 16.9461, 'eval_samples_per_second': 11.802, 'eval_steps_per_second': 1.475, 'epoch': 64.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1600\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1400] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2354, 'learning_rate': 0.00013409999999999998, 'epoch': 64.4}\n",
            "{'loss': 0.2092, 'learning_rate': 0.0001326, 'epoch': 64.8}\n",
            "{'loss': 0.219, 'learning_rate': 0.0001311, 'epoch': 65.2}\n",
            "{'loss': 0.1818, 'learning_rate': 0.00012959999999999998, 'epoch': 65.6}\n",
            "{'loss': 0.1875, 'learning_rate': 0.0001281, 'epoch': 66.0}\n",
            "{'loss': 0.2344, 'learning_rate': 0.0001266, 'epoch': 66.4}\n",
            "{'loss': 0.2018, 'learning_rate': 0.00012509999999999998, 'epoch': 66.8}\n",
            "{'loss': 0.1891, 'learning_rate': 0.0001236, 'epoch': 67.2}\n",
            "{'loss': 0.169, 'learning_rate': 0.00012209999999999999, 'epoch': 67.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1892, 'learning_rate': 0.00012059999999999999, 'epoch': 68.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3107ae3f43b4945a96b0da628eefb10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9917768836021423, 'eval_wer': 0.7652764306498545, 'eval_runtime': 18.5, 'eval_samples_per_second': 10.811, 'eval_steps_per_second': 1.351, 'epoch': 68.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1700\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1500] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2166, 'learning_rate': 0.0001191, 'epoch': 68.4}\n",
            "{'loss': 0.195, 'learning_rate': 0.0001176, 'epoch': 68.8}\n",
            "{'loss': 0.193, 'learning_rate': 0.00011609999999999999, 'epoch': 69.2}\n",
            "{'loss': 0.1907, 'learning_rate': 0.0001146, 'epoch': 69.6}\n",
            "{'loss': 0.1518, 'learning_rate': 0.00011309999999999998, 'epoch': 70.0}\n",
            "{'loss': 0.1771, 'learning_rate': 0.00011159999999999999, 'epoch': 70.4}\n",
            "{'loss': 0.1613, 'learning_rate': 0.00011009999999999999, 'epoch': 70.8}\n",
            "{'loss': 0.172, 'learning_rate': 0.00010859999999999998, 'epoch': 71.2}\n",
            "{'loss': 0.1794, 'learning_rate': 0.00010709999999999999, 'epoch': 71.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1569, 'learning_rate': 0.00010559999999999998, 'epoch': 72.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95fc87bcd4a24c84bfa9d991165cbe88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9897462725639343, 'eval_wer': 0.7419980601357905, 'eval_runtime': 9.5734, 'eval_samples_per_second': 20.891, 'eval_steps_per_second': 2.611, 'epoch': 72.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1800\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1600] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1542, 'learning_rate': 0.00010409999999999998, 'epoch': 72.4}\n",
            "{'loss': 0.1483, 'learning_rate': 0.0001026, 'epoch': 72.8}\n",
            "{'loss': 0.1669, 'learning_rate': 0.0001011, 'epoch': 73.2}\n",
            "{'loss': 0.1462, 'learning_rate': 9.96e-05, 'epoch': 73.6}\n",
            "{'loss': 0.1376, 'learning_rate': 9.81e-05, 'epoch': 74.0}\n",
            "{'loss': 0.163, 'learning_rate': 9.659999999999999e-05, 'epoch': 74.4}\n",
            "{'loss': 0.1898, 'learning_rate': 9.51e-05, 'epoch': 74.8}\n",
            "{'loss': 0.1392, 'learning_rate': 9.36e-05, 'epoch': 75.2}\n",
            "{'loss': 0.1745, 'learning_rate': 9.209999999999999e-05, 'epoch': 75.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1225, 'learning_rate': 9.059999999999999e-05, 'epoch': 76.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c0c7c6b436e47df95a72e00eca2726c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9779219627380371, 'eval_wer': 0.7352085354025218, 'eval_runtime': 16.8533, 'eval_samples_per_second': 11.867, 'eval_steps_per_second': 1.483, 'epoch': 76.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-1900\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1700] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1359, 'learning_rate': 8.909999999999998e-05, 'epoch': 76.4}\n",
            "{'loss': 0.1278, 'learning_rate': 8.759999999999999e-05, 'epoch': 76.8}\n",
            "{'loss': 0.126, 'learning_rate': 8.609999999999999e-05, 'epoch': 77.2}\n",
            "{'loss': 0.1484, 'learning_rate': 8.459999999999998e-05, 'epoch': 77.6}\n",
            "{'loss': 0.1348, 'learning_rate': 8.31e-05, 'epoch': 78.0}\n",
            "{'loss': 0.1362, 'learning_rate': 8.16e-05, 'epoch': 78.4}\n",
            "{'loss': 0.1282, 'learning_rate': 8.01e-05, 'epoch': 78.8}\n",
            "{'loss': 0.1195, 'learning_rate': 7.86e-05, 'epoch': 79.2}\n",
            "{'loss': 0.1296, 'learning_rate': 7.709999999999999e-05, 'epoch': 79.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1407, 'learning_rate': 7.56e-05, 'epoch': 80.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99f1c6226d404abaa076e022aaef45b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.958179771900177, 'eval_wer': 0.7390882638215325, 'eval_runtime': 16.1488, 'eval_samples_per_second': 12.385, 'eval_steps_per_second': 1.548, 'epoch': 80.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2000\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1800] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1129, 'learning_rate': 7.41e-05, 'epoch': 80.4}\n",
            "{'loss': 0.1251, 'learning_rate': 7.259999999999999e-05, 'epoch': 80.8}\n",
            "{'loss': 0.1087, 'learning_rate': 7.11e-05, 'epoch': 81.2}\n",
            "{'loss': 0.11, 'learning_rate': 6.96e-05, 'epoch': 81.6}\n",
            "{'loss': 0.1351, 'learning_rate': 6.81e-05, 'epoch': 82.0}\n",
            "{'loss': 0.1429, 'learning_rate': 6.659999999999999e-05, 'epoch': 82.4}\n",
            "{'loss': 0.1083, 'learning_rate': 6.51e-05, 'epoch': 82.8}\n",
            "{'loss': 0.1004, 'learning_rate': 6.359999999999999e-05, 'epoch': 83.2}\n",
            "{'loss': 0.1157, 'learning_rate': 6.209999999999999e-05, 'epoch': 83.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1035, 'learning_rate': 6.0599999999999996e-05, 'epoch': 84.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "003e360cfcfa4a078f3eba21539ee6c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2100\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2100\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9425376653671265, 'eval_wer': 0.7594568380213385, 'eval_runtime': 14.2016, 'eval_samples_per_second': 14.083, 'eval_steps_per_second': 1.76, 'epoch': 84.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2100\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2100\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-1900] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.1158, 'learning_rate': 5.91e-05, 'epoch': 84.4}\n",
            "{'loss': 0.1026, 'learning_rate': 5.76e-05, 'epoch': 84.8}\n",
            "{'loss': 0.0951, 'learning_rate': 5.6099999999999995e-05, 'epoch': 85.2}\n",
            "{'loss': 0.1173, 'learning_rate': 5.459999999999999e-05, 'epoch': 85.6}\n",
            "{'loss': 0.0997, 'learning_rate': 5.309999999999999e-05, 'epoch': 86.0}\n",
            "{'loss': 0.1027, 'learning_rate': 5.1599999999999994e-05, 'epoch': 86.4}\n",
            "{'loss': 0.117, 'learning_rate': 5.01e-05, 'epoch': 86.8}\n",
            "{'loss': 0.0921, 'learning_rate': 4.8599999999999995e-05, 'epoch': 87.2}\n",
            "{'loss': 0.1225, 'learning_rate': 4.709999999999999e-05, 'epoch': 87.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0849, 'learning_rate': 4.56e-05, 'epoch': 88.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a69363c545ea4e2c82c3756487534a14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2200\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2200\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9539394974708557, 'eval_wer': 0.7555771096023278, 'eval_runtime': 10.3953, 'eval_samples_per_second': 19.239, 'eval_steps_per_second': 2.405, 'epoch': 88.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2200\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2200\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2000] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.095, 'learning_rate': 4.4099999999999995e-05, 'epoch': 88.4}\n",
            "{'loss': 0.1037, 'learning_rate': 4.259999999999999e-05, 'epoch': 88.8}\n",
            "{'loss': 0.0922, 'learning_rate': 4.11e-05, 'epoch': 89.2}\n",
            "{'loss': 0.1082, 'learning_rate': 3.96e-05, 'epoch': 89.6}\n",
            "{'loss': 0.1068, 'learning_rate': 3.81e-05, 'epoch': 90.0}\n",
            "{'loss': 0.0975, 'learning_rate': 3.6599999999999995e-05, 'epoch': 90.4}\n",
            "{'loss': 0.1025, 'learning_rate': 3.51e-05, 'epoch': 90.8}\n",
            "{'loss': 0.11, 'learning_rate': 3.36e-05, 'epoch': 91.2}\n",
            "{'loss': 0.0832, 'learning_rate': 3.2099999999999994e-05, 'epoch': 91.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0837, 'learning_rate': 3.06e-05, 'epoch': 92.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c626f8ce46c488d84c7f567892a963d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2300\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2300\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9398958086967468, 'eval_wer': 0.7516973811833172, 'eval_runtime': 10.688, 'eval_samples_per_second': 18.713, 'eval_steps_per_second': 2.339, 'epoch': 92.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2300\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2300\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2100] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.087, 'learning_rate': 2.91e-05, 'epoch': 92.4}\n",
            "{'loss': 0.0994, 'learning_rate': 2.7599999999999997e-05, 'epoch': 92.8}\n",
            "{'loss': 0.0969, 'learning_rate': 2.6099999999999997e-05, 'epoch': 93.2}\n",
            "{'loss': 0.0927, 'learning_rate': 2.4599999999999998e-05, 'epoch': 93.6}\n",
            "{'loss': 0.0746, 'learning_rate': 2.31e-05, 'epoch': 94.0}\n",
            "{'loss': 0.0833, 'learning_rate': 2.1599999999999996e-05, 'epoch': 94.4}\n",
            "{'loss': 0.0943, 'learning_rate': 2.01e-05, 'epoch': 94.8}\n",
            "{'loss': 0.0954, 'learning_rate': 1.8599999999999998e-05, 'epoch': 95.2}\n",
            "{'loss': 0.0868, 'learning_rate': 1.71e-05, 'epoch': 95.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0944, 'learning_rate': 1.5599999999999996e-05, 'epoch': 96.0}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66a66620ed8e40b5860f8e3a68bfc4b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2400\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2400\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9262589812278748, 'eval_wer': 0.7303588748787585, 'eval_runtime': 12.9433, 'eval_samples_per_second': 15.452, 'eval_steps_per_second': 1.931, 'epoch': 96.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2400\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2400\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2200] due to args.save_total_limit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0902, 'learning_rate': 1.4099999999999999e-05, 'epoch': 96.4}\n",
            "{'loss': 0.1018, 'learning_rate': 1.26e-05, 'epoch': 96.8}\n",
            "{'loss': 0.0866, 'learning_rate': 1.1099999999999999e-05, 'epoch': 97.2}\n",
            "{'loss': 0.0913, 'learning_rate': 9.6e-06, 'epoch': 97.6}\n",
            "{'loss': 0.0944, 'learning_rate': 8.099999999999999e-06, 'epoch': 98.0}\n",
            "{'loss': 0.0991, 'learning_rate': 6.599999999999999e-06, 'epoch': 98.4}\n",
            "{'loss': 0.0815, 'learning_rate': 5.1e-06, 'epoch': 98.8}\n",
            "{'loss': 0.0929, 'learning_rate': 3.6e-06, 'epoch': 99.2}\n",
            "{'loss': 0.1005, 'learning_rate': 2.1e-06, 'epoch': 99.6}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0856, 'learning_rate': 6e-07, 'epoch': 100.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: array. If array are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba0231ec932f4e0a85dfe4d7efb72766",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2500\n",
            "Configuration saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2500\\config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.9267768859863281, 'eval_wer': 0.7187196896217265, 'eval_runtime': 11.61, 'eval_samples_per_second': 17.227, 'eval_steps_per_second': 2.153, 'epoch': 100.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model weights saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2500\\pytorch_model.bin\n",
            "Feature extractor saved in ./wav2vec2-large-xlsr-ko-demo\\checkpoint-2500\\preprocessor_config.json\n",
            "Deleting older checkpoint [wav2vec2-large-xlsr-ko-demo\\checkpoint-2300] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 7706.641, 'train_samples_per_second': 10.381, 'train_steps_per_second': 0.324, 'train_loss': 3.5201705753326418, 'epoch': 100.0}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=3.5201705753326418, metrics={'train_runtime': 7706.641, 'train_samples_per_second': 10.381, 'train_steps_per_second': 0.324, 'train_loss': 3.5201705753326418, 'epoch': 100.0})"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxpIR_X8w3Pq"
      },
      "source": [
        "CTC 손실을 사용하여 더 큰 데이터 세트에서 더 큰 모델을 미세 조정하려면 [여기서](https://github.com/huggingface/transformers/tree/master/examples/pytorch/speech-recognition#connectionist-temporal-classification-without-language-model-ctc-wo-lm) 공식 음성 인식 예를 살펴봐야 한다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "00.XLSR - Wav2Vec2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('STT')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "19631f30805cf65d5465564d75f0fe7c05dee5c1f7be198222dbe754da644e52"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
